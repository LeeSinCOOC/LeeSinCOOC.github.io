<!DOCTYPE html>





<html class="theme-next mist use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/bitbug_favicon1.ico?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/bitbug_favicon.ico?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":true,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="为什么需要做词嵌入实际应用中 ， 每一步只输入一个字母显然是不太合适的 ， 更加高效的方法是每一步输入一个单词，如果还继续使用独热表示 ，那么每一步输入的向量维数会非常大 ，独热表示实际上完全平等看待了单词表中的所有单词 ，忽略了单词之间的联系 词嵌入的原理如何学习到映射 关系？一般有两种方法： 一种方法是基于“计数” 的 ，大型语料库中 ， 计算一个词语和另一个词语同时出现的概率，将经常同时出现">
<meta name="keywords" content="python,tensorflow,pytorch,人工智能">
<meta property="og:type" content="article">
<meta property="og:title" content="14.21个项目玩转深度学习之词的向量表示">
<meta property="og:url" content="http://leesin.cc/tensorflow/14.21个项目玩转深度学习之词的向量表示.html">
<meta property="og:site_name" content="Chen Jian&#39;s Blog">
<meta property="og:description" content="为什么需要做词嵌入实际应用中 ， 每一步只输入一个字母显然是不太合适的 ， 更加高效的方法是每一步输入一个单词，如果还继续使用独热表示 ，那么每一步输入的向量维数会非常大 ，独热表示实际上完全平等看待了单词表中的所有单词 ，忽略了单词之间的联系 词嵌入的原理如何学习到映射 关系？一般有两种方法： 一种方法是基于“计数” 的 ，大型语料库中 ， 计算一个词语和另一个词语同时出现的概率，将经常同时出现">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-11-02T13:46:48.997Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="14.21个项目玩转深度学习之词的向量表示">
<meta name="twitter:description" content="为什么需要做词嵌入实际应用中 ， 每一步只输入一个字母显然是不太合适的 ， 更加高效的方法是每一步输入一个单词，如果还继续使用独热表示 ，那么每一步输入的向量维数会非常大 ，独热表示实际上完全平等看待了单词表中的所有单词 ，忽略了单词之间的联系 词嵌入的原理如何学习到映射 关系？一般有两种方法： 一种方法是基于“计数” 的 ，大型语料库中 ， 计算一个词语和另一个词语同时出现的概率，将经常同时出现">
  <link rel="canonical" href="http://leesin.cc/tensorflow/14.21个项目玩转深度学习之词的向量表示">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>14.21个项目玩转深度学习之词的向量表示 | Chen Jian's Blog</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-right">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen Jian's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-reading">
      
    

    <a href="/reading/" rel="section"><i class="menu-item-icon fa fa-fw fa-book"></i> <br>笔记</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-miscellaneous">
      
    

    <a href="/miscellaneous/" rel="section"><i class="menu-item-icon fa fa-fw fa-link"></i> <br>常用网站</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

    

</nav>
  <div class="site-search">
    
  <div class="popup search-popup">
  <div class="search-header">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <div class="search-input-wrapper">
      <input autocomplete="off" autocorrect="off" autocapitalize="none"
             placeholder="搜索..." spellcheck="false"
             type="text" id="search-input">
    </div>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>
  <div id="search-result"></div>
</div>


  </div>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://leesin.cc/tensorflow/14.21个项目玩转深度学习之词的向量表示.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="陈 建">
      <meta itemprop="description" content="当时明月在，曾照彩云归">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen Jian's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">14.21个项目玩转深度学习之词的向量表示

          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-09-04 21:31:28" itemprop="dateCreated datePublished" datetime="2019-09-04T21:31:28+08:00">2019-09-04</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-02 21:46:48" itemprop="dateModified" datetime="2019-11-02T21:46:48+08:00">2019-11-02</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/TensorFlow教程/" itemprop="url" rel="index"><span itemprop="name">TensorFlow教程</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="为什么需要做词嵌入"><a href="#为什么需要做词嵌入" class="headerlink" title="为什么需要做词嵌入"></a>为什么需要做词嵌入</h2><p>实际应用中 ， 每一步只输入一个字母显然是不太合适的 ， 更加高效的方法是每一步输入一个单词，如果还继续使用独热表示 ，那么每一步输入的向量维数会非常大 ，独热表示实际上完全平等看待了单词表中的所有单词 ，忽略了单词之间的联系 </p><h2 id="词嵌入的原理"><a href="#词嵌入的原理" class="headerlink" title="词嵌入的原理"></a>词嵌入的原理</h2><p>如何学习到映射 关系？一般有两种方法：</p><ul>
<li><p>一种方法是基于“计数” 的 ，大型语料库中 ， 计算一个词语和另一个词语同时出现的概率，将经常同时出现的词映射到向量空间的中相近位置 </p>
</li>
<li><p>一种方法是基于“预测” 的，从一个词或几个词出发 ， 预测它们可能的相邻词，在预测过程中自然而然地学习到了词嵌入的映射 </p>
</li>
</ul><a id="more"></a>


<p>两种基于预测的方法，分别叫 <code>CBOW</code> 和 <code>Skip-Gram</code> </p>
<h2 id="CBOW-实现词嵌入的原理"><a href="#CBOW-实现词嵌入的原理" class="headerlink" title="CBOW 实现词嵌入的原理"></a>CBOW 实现词嵌入的原理</h2><p>CBOW 的全称为 Continuous Bag of Words ，即连续词袋模型 ，它的核心思想是利用某个词语的上下文预测这个词语 </p>
<p>通过优化二分类损失函数来训练模型后，最后得到的模型中的隐含层可以看作是 word2vec 中的“vec”向量 。 对于一个单词，先将独热表示输入模型，隐含层的值是对应的词嵌入表示 。 另外，在 TensorFlow 中，这里使用的损失被称为 NCE 损失，对应的函数为 tf.nn.nce loss 。 </p>
<h2 id="Skip-Gram-实现词嵌入的原理"><a href="#Skip-Gram-实现词嵌入的原理" class="headerlink" title="Skip-Gram 实现词嵌入的原理"></a>Skip-Gram 实现词嵌入的原理</h2><p>Skip-Gram 方法和 CBOW 方法正好相反：使用“出现的词”来预测 “上下文文中词 </p>
<p>在损失的选择上，和 CBOW 一样，不使用 V 类分类的 Softmax 交叉摘损失， 而是取出一些“噪声词”，训练一个两类分类器（即同样使用 NCE损失）。 </p>
<h2 id="在-TensorFlow-中实现词嵌入"><a href="#在-TensorFlow-中实现词嵌入" class="headerlink" title="在 TensorFlow 中实现词嵌入"></a>在 TensorFlow 中实现词嵌入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment"># ==============================================================================</span></span><br><span class="line"><span class="string">"""Basic word2vec example."""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入一些需要的库</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange  <span class="comment"># pylint: disable=redefined-builtin</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一步: 在下面这个地址下载语料库</span></span><br><span class="line">url = <span class="string">'http://mattmahoney.net/dc/'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download</span><span class="params">(filename, expected_bytes)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  这个函数的功能是：</span></span><br><span class="line"><span class="string">      如果filename不存在，就在上面的地址下载它。</span></span><br><span class="line"><span class="string">      如果filename存在，就跳过下载。</span></span><br><span class="line"><span class="string">      最终会检查文字的字节数是否和expected_bytes相同。</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(filename):</span><br><span class="line">    print(<span class="string">'start downloading...'</span>)</span><br><span class="line">    filename, _ = urllib.request.urlretrieve(url + filename, filename)</span><br><span class="line">  statinfo = os.stat(filename)</span><br><span class="line">  <span class="keyword">if</span> statinfo.st_size == expected_bytes:</span><br><span class="line">    print(<span class="string">'Found and verified'</span>, filename)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    print(statinfo.st_size)</span><br><span class="line">    <span class="keyword">raise</span> Exception(</span><br><span class="line">        <span class="string">'Failed to verify '</span> + filename + <span class="string">'. Can you get to it with a browser?'</span>)</span><br><span class="line">  <span class="keyword">return</span> filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载语料库text8.zip并验证下载</span></span><br><span class="line">filename = maybe_download(<span class="string">'text8.zip'</span>, <span class="number">31344016</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将语料库解压，并转换成一个word的list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  这个函数的功能是：</span></span><br><span class="line"><span class="string">      将下载好的zip文件解压并读取为word的list</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">with</span> zipfile.ZipFile(filename) <span class="keyword">as</span> f:</span><br><span class="line">    data = tf.compat.as_str(f.read(f.namelist()[<span class="number">0</span>])).split()</span><br><span class="line">  <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">vocabulary = read_data(filename)</span><br><span class="line">print(<span class="string">'Data size'</span>, len(vocabulary)) <span class="comment"># 总长度为1700万左右</span></span><br><span class="line"><span class="comment"># 输出前100个词。</span></span><br><span class="line">print(vocabulary[<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步: 制作一个词表，将不常见的词变成一个UNK标识符</span></span><br><span class="line"><span class="comment"># 词表的大小为5万（即我们只考虑最常出现的5万个词）</span></span><br><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words, n_words)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  函数功能：将原始的单词表示变成index</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  count = [[<span class="string">'UNK'</span>, <span class="number">-1</span>]]</span><br><span class="line">  count.extend(collections.Counter(words).most_common(n_words - <span class="number">1</span>))</span><br><span class="line">  dictionary = dict()</span><br><span class="line">  <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">    dictionary[word] = len(dictionary)</span><br><span class="line">  data = list()</span><br><span class="line">  unk_count = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">      index = dictionary[word]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      index = <span class="number">0</span>  <span class="comment"># UNK的index为0</span></span><br><span class="line">      unk_count += <span class="number">1</span></span><br><span class="line">    data.append(index)</span><br><span class="line">  count[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">  <span class="keyword">return</span> data, count, dictionary, reversed_dictionary</span><br><span class="line"></span><br><span class="line">data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,</span><br><span class="line">                                                            vocabulary_size)</span><br><span class="line"><span class="keyword">del</span> vocabulary  <span class="comment"># 删除已节省内存</span></span><br><span class="line"><span class="comment"># 输出最常出现的5个单词</span></span><br><span class="line">print(<span class="string">'Most common words (+UNK)'</span>, count[:<span class="number">5</span>])</span><br><span class="line"><span class="comment"># 输出转换后的数据库data，和原来的单词（前10个）</span></span><br><span class="line">print(<span class="string">'Sample data'</span>, data[:<span class="number">10</span>], [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> data[:<span class="number">10</span>]])</span><br><span class="line"><span class="comment"># 我们下面就使用data来制作训练集</span></span><br><span class="line">data_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三步：定义一个函数，用于生成skip-gram模型用的batch</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">  <span class="comment"># data_index相当于一个指针，初始为0</span></span><br><span class="line">  <span class="comment"># 每次生成一个batch，data_index就会相应地往后推</span></span><br><span class="line">  <span class="keyword">global</span> data_index</span><br><span class="line">  <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">  <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">  batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">  labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">  span = <span class="number">2</span> * skip_window + <span class="number">1</span>  <span class="comment"># [ skip_window target skip_window ]</span></span><br><span class="line">  buffer = collections.deque(maxlen=span)</span><br><span class="line">  <span class="comment"># data_index是当前数据开始的位置</span></span><br><span class="line">  <span class="comment"># 产生batch后就往后推1位（产生batch）</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // num_skips):</span><br><span class="line">    <span class="comment"># 利用buffer生成batch</span></span><br><span class="line">    <span class="comment"># buffer是一个长度为 2 * skip_window + 1长度的word list</span></span><br><span class="line">    <span class="comment"># 一个buffer生成num_skips个数的样本</span></span><br><span class="line"><span class="comment">#     print([reverse_dictionary[i] for i in buffer])</span></span><br><span class="line">    target = skip_window  <span class="comment"># target label at the center of the buffer</span></span><br><span class="line"><span class="comment">#     targets_to_avoid保证样本不重复</span></span><br><span class="line">    targets_to_avoid = [skip_window]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">      <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">        target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">      targets_to_avoid.append(target)</span><br><span class="line">      batch[i * num_skips + j] = buffer[skip_window]</span><br><span class="line">      labels[i * num_skips + j, <span class="number">0</span>] = buffer[target]</span><br><span class="line">    buffer.append(data[data_index])</span><br><span class="line">    <span class="comment"># 每利用buffer生成num_skips个样本，data_index就向后推进一位</span></span><br><span class="line">    data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">  data_index = (data_index + len(data) - span) % len(data)</span><br><span class="line">  <span class="keyword">return</span> batch, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认情况下skip_window=1, num_skips=2</span></span><br><span class="line"><span class="comment"># 此时就是从连续的3(3 = skip_window*2 + 1)个词中生成2(num_skips)个样本。</span></span><br><span class="line"><span class="comment"># 如连续的三个词['used', 'against', 'early']</span></span><br><span class="line"><span class="comment"># 生成两个样本：against -&gt; used, against -&gt; early</span></span><br><span class="line">batch, labels = generate_batch(batch_size=<span class="number">8</span>, num_skips=<span class="number">2</span>, skip_window=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">  print(batch[i], reverse_dictionary[batch[i]],</span><br><span class="line">        <span class="string">'-&gt;'</span>, labels[i, <span class="number">0</span>], reverse_dictionary[labels[i, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步: 建立模型.</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span>  <span class="comment"># 词嵌入空间是128维的。即word2vec中的vec是一个128维的向量</span></span><br><span class="line">skip_window = <span class="number">1</span>       <span class="comment"># skip_window参数和之前保持一致</span></span><br><span class="line">num_skips = <span class="number">2</span>         <span class="comment"># num_skips参数和之前保持一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练过程中，会对模型进行验证 </span></span><br><span class="line"><span class="comment"># 验证的方法就是找出和某个词最近的词。</span></span><br><span class="line"><span class="comment"># 只对前valid_window的词进行验证，因为这些词最常出现</span></span><br><span class="line">valid_size = <span class="number">16</span>     <span class="comment"># 每次验证16个词</span></span><br><span class="line">valid_window = <span class="number">100</span>  <span class="comment"># 这16个词是在前100个最常见的词中选出来的</span></span><br><span class="line">valid_examples = np.random.choice(valid_window, valid_size, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造损失时选取的噪声词的数量</span></span><br><span class="line">num_sampled = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 输入的batch</span></span><br><span class="line">  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">  train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">  <span class="comment"># 用于验证的词</span></span><br><span class="line">  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 下面采用的某些函数还没有gpu实现，所以我们只在cpu上定义模型</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    <span class="comment"># 定义1个embeddings变量，相当于一行存储一个词的embedding</span></span><br><span class="line">    embeddings = tf.Variable(</span><br><span class="line">        tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">    <span class="comment"># 利用embedding_lookup可以轻松得到一个batch内的所有的词嵌入</span></span><br><span class="line">    embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建两个变量用于NCE Loss（即选取噪声词的二分类损失）</span></span><br><span class="line">    nce_weights = tf.Variable(</span><br><span class="line">        tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                            stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># tf.nn.nce_loss会自动选取噪声词，并且形成损失。</span></span><br><span class="line">  <span class="comment"># 随机选取num_sampled个噪声词</span></span><br><span class="line">  loss = tf.reduce_mean(</span><br><span class="line">      tf.nn.nce_loss(weights=nce_weights,</span><br><span class="line">                     biases=nce_biases,</span><br><span class="line">                     labels=train_labels,</span><br><span class="line">                     inputs=embed,</span><br><span class="line">                     num_sampled=num_sampled,</span><br><span class="line">                     num_classes=vocabulary_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 得到loss后，我们就可以构造优化器了</span></span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 计算词和词的相似度（用于验证）</span></span><br><span class="line">  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), <span class="number">1</span>, keep_dims=<span class="literal">True</span>))</span><br><span class="line">  normalized_embeddings = embeddings / norm</span><br><span class="line">  <span class="comment"># 找出和验证词的embedding并计算它们和所有单词的相似度</span></span><br><span class="line">  valid_embeddings = tf.nn.embedding_lookup(</span><br><span class="line">      normalized_embeddings, valid_dataset)</span><br><span class="line">  similarity = tf.matmul(</span><br><span class="line">      valid_embeddings, normalized_embeddings, transpose_b=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 变量初始化步骤</span></span><br><span class="line">  init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五步：开始训练</span></span><br><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> session:</span><br><span class="line">  <span class="comment"># 初始化变量</span></span><br><span class="line">  init.run()</span><br><span class="line">  print(<span class="string">'Initialized'</span>)</span><br><span class="line"></span><br><span class="line">  average_loss = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> step <span class="keyword">in</span> xrange(num_steps):</span><br><span class="line">    batch_inputs, batch_labels = generate_batch(</span><br><span class="line">        batch_size, num_skips, skip_window)</span><br><span class="line">    feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化一步</span></span><br><span class="line">    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">    average_loss += loss_val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        average_loss /= <span class="number">2000</span></span><br><span class="line">      <span class="comment"># 2000个batch的平均损失</span></span><br><span class="line">      print(<span class="string">'Average loss at step '</span>, step, <span class="string">': '</span>, average_loss)</span><br><span class="line">      average_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每1万步，我们进行一次验证</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">      <span class="comment"># sim是验证词与所有词之间的相似度</span></span><br><span class="line">      sim = similarity.eval()</span><br><span class="line">      <span class="comment"># 一共有valid_size个验证词</span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> xrange(valid_size):</span><br><span class="line">        valid_word = reverse_dictionary[valid_examples[i]]</span><br><span class="line">        top_k = <span class="number">8</span>  <span class="comment"># 输出最相邻的8个词语</span></span><br><span class="line">        nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k + <span class="number">1</span>]</span><br><span class="line">        log_str = <span class="string">'Nearest to %s:'</span> % valid_word</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> xrange(top_k):</span><br><span class="line">          close_word = reverse_dictionary[nearest[k]]</span><br><span class="line">          log_str = <span class="string">'%s %s,'</span> % (log_str, close_word)</span><br><span class="line">        print(log_str)</span><br><span class="line">  <span class="comment"># final_embeddings是我们最后得到的embedding向量</span></span><br><span class="line">  <span class="comment"># 它的形状是[vocabulary_size, embedding_size]</span></span><br><span class="line">  <span class="comment"># 每一行就代表着对应index词的词嵌入表示</span></span><br><span class="line">  final_embeddings = normalized_embeddings.eval()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: 可视化</span></span><br><span class="line"><span class="comment"># 可视化的图片会保存为“tsne.png”</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels, filename=<span class="string">'tsne.png'</span>)</span>:</span></span><br><span class="line">  <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels), <span class="string">'More labels than embeddings'</span></span><br><span class="line">  plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))  <span class="comment"># in inches</span></span><br><span class="line">  <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">    x, y = low_dim_embs[i, :]</span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    plt.annotate(label,</span><br><span class="line">                 xy=(x, y),</span><br><span class="line">                 xytext=(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">                 textcoords=<span class="string">'offset points'</span>,</span><br><span class="line">                 ha=<span class="string">'right'</span>,</span><br><span class="line">                 va=<span class="string">'bottom'</span>)</span><br><span class="line"></span><br><span class="line">  plt.savefig(filename)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  <span class="comment"># pylint: disable=g-import-not-at-top</span></span><br><span class="line">  <span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">  <span class="keyword">import</span> matplotlib</span><br><span class="line">  matplotlib.use(<span class="string">'agg'</span>)</span><br><span class="line">  <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">  <span class="comment"># 因为我们的embedding的大小为128维，没有办法直接可视化</span></span><br><span class="line">  <span class="comment"># 所以我们用t-SNE方法进行降维</span></span><br><span class="line">  tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">'pca'</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">  <span class="comment"># 只画出500个词的位置</span></span><br><span class="line">  plot_only = <span class="number">500</span></span><br><span class="line">  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])</span><br><span class="line">  labels = [reverse_dictionary[i] <span class="keyword">for</span> i <span class="keyword">in</span> xrange(plot_only)]</span><br><span class="line">  plot_with_labels(low_dim_embs, labels)</span><br><span class="line"></span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">  print(<span class="string">'Please install sklearn, matplotlib, and scipy to show embeddings.'</span>)</span><br></pre></td></tr></table></figure>


    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>您的支持是对我最大的鼓励</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/weixin.jpg" alt="陈 建 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="陈 建 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>陈 建</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://leesin.cc/tensorflow/14.21个项目玩转深度学习之词的向量表示.html" title="14.21个项目玩转深度学习之词的向量表示">http://leesin.cc/tensorflow/14.21个项目玩转深度学习之词的向量表示.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/tensorflow/13.21个项目玩转深度学习之序列分类问题详解.html" rel="next" title="13.21个项目玩转深度学习之序列分类问题详解">
                  <i class="fa fa-chevron-left"></i> 13.21个项目玩转深度学习之序列分类问题详解
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/tensorflow/15.21个项目玩转深度学习之在Tensorflow中进行时间序列预测.html" rel="prev" title="15.21个项目玩转深度学习之在Tensorflow中进行时间序列预测">
                  15.21个项目玩转深度学习之在Tensorflow中进行时间序列预测 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="陈 建">
  <p class="site-author-name" itemprop="name">陈 建</p>
  <div class="site-description motion-element" itemprop="description">当时明月在，曾照彩云归</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">103</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/LeeSinCOOC" title="GitHub &rarr; https://github.com/LeeSinCOOC" rel="noopener" target="_blank"><i class="fa fa-fw fa-GitHub"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:690246265@qq.com" title="E-Mail &rarr; mailto:690246265@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-E-Mail"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-block">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.baidu.com" title="https://www.baidu.com" rel="noopener" target="_blank">baidu</a>
        </li>
      
    </ul>
  </div>


        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么需要做词嵌入"><span class="nav-number">1.</span> <span class="nav-text">为什么需要做词嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词嵌入的原理"><span class="nav-number">2.</span> <span class="nav-text">词嵌入的原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CBOW-实现词嵌入的原理"><span class="nav-number">3.</span> <span class="nav-text">CBOW 实现词嵌入的原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Skip-Gram-实现词嵌入的原理"><span class="nav-number">4.</span> <span class="nav-text">Skip-Gram 实现词嵌入的原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在-TensorFlow-中实现词嵌入"><span class="nav-number">5.</span> <span class="nav-text">在 TensorFlow 中实现词嵌入</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">陈 建</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.3.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>

<script src="/js/schemes/muse.js?v=7.3.0"></script>



<script src="/js/next-boot.js?v=7.3.0"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>















  <script src="/js/local-search.js?v=7.3.0"></script>














  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script><script src="/js/post-details.js?v=7.3.0"></script>


</body>
</html>
