<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[18.漂亮的结尾]]></title>
    <url>%2Fmoviepy%2F18.%E6%BC%82%E4%BA%AE%E7%9A%84%E7%BB%93%E5%B0%BE.html</url>
    <content type="text"><![CDATA[漂亮的结尾 1234567891011121314151617181920212223from moviepy.editor import *from moviepy.video.tools.drawing import circle # 原有视频clip = VideoFileClip("./videos/1.mp4", audio=False).add_mask() w,h = clip.size # 这里的mask是一个半径按照 r(t) = 800-200*t 根据时间变化消失的圆 clip.mask.get_frame = lambda t: circle(screensize=(clip.w,clip.h), center=(clip.w/2,clip.h/4), radius=max(0,int(200-200*t)), col1=1, col2=0, blur=4) # 搞一个TextClip来放The Endthe_end = TextClip("The End", font="Amiri-bold", color="white", fontsize=70).set_duration(clip.duration) final = CompositeVideoClip([the_end.set_pos('center'),clip], size =clip.size) final.write_gif("./images/magic_theEnd.gif",fps=10)]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[17.追踪人脸]]></title>
    <url>%2Fmoviepy%2F17.%E8%BF%BD%E8%B8%AA%E4%BA%BA%E8%84%B8.html</url>
    <content type="text"><![CDATA[追踪人脸 下面的代码依然跑不通 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import pickle from moviepy.editor import *from moviepy.video.tools.tracking import manual_tracking, to_fxfy # 加载clip，截取一个卓别林电影的6‘51-7’01之间的片段clip = VideoFileClip("../../videos/chaplin.mp4").subclip((6,51.7),(7,01.3)) # 手动跟踪标记头部 # 下面的三行代码，手动跟踪，然后把结果保存进文件，应该在一次运行之后就完成量跟踪标记# 注意：我们保存的格式是一个(ti,xi,yi)list，不是函数fx和fy #txy, (fx,fy) = manual_tracking(clip, fps=6)#with open("../../chaplin_txy.dat",'w+') as f:# pickle.dump(txy) # 已经完成手动跟踪人脸并标记的情况下# fx(t),fy(t)的形式加载跟踪标记的数据 with open("../../chaplin_txy.dat",'r') as f: fx,fy = to_fxfy( pickle.load(f) ) # 在clip中，模糊卓别林的头部 clip_blurred = clip.fx( vfx.headblur, fx, fy, 25) # 生成文本，灰色背景 txt = TextClip("Hey you ! \n You're blurry!", color='grey70', size = clip.size, bg_color='grey20', font = "Century-Schoolbook-Italic", fontsize=40) # 把卓别林的vedio clip和TextClip连接起来，添加audio clip final = concatenate_videoclips([clip_blurred,txt.set_duration(3)]).\ set_audio(clip.audio) # 将比特率修改为3000k是为了画面不至于太丑 final.write_videofile('../../blurredChaplin.avi', bitrate="3000k")]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[16.Logo阴影]]></title>
    <url>%2Fmoviepy%2F16.Logo%E9%98%B4%E5%BD%B1.html</url>
    <content type="text"><![CDATA[Logo阴影 !!! 这个例子没代码 这教程。。。。无语了]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[15.使用画笔特效定格视频中的某一帧]]></title>
    <url>%2Fmoviepy%2F15.%E4%BD%BF%E7%94%A8%E7%94%BB%E7%AC%94%E7%89%B9%E6%95%88%E5%AE%9A%E6%A0%BC%E8%A7%86%E9%A2%91%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%80%E5%B8%A7.html</url>
    <content type="text"><![CDATA[使用画笔特效定格视频中的某一帧 !!!!我擦，这段也跑不通，这教程对新手这么不友善？AttributeError: ‘VideoFileClip’ object has no attribute ‘coreader’ 这样的处理手法，会让一帧画面看起来像画一样： 用Sobel算法算出图像中的边缘，我们就获得了像手绘的黑白画面 图片矩阵相乘，获得比较亮的画面，再叠加第一步获得的轮廓 最终的clip包含三级：处理之前的部分，处理的部分，处理过后的部分。处理的部分，是按照以下三步走来获得的。 定格一帧画面，制作成手绘风格，作为一个clip 添加一个写着[Audrey]的text clip到第一步的clip 把的以上得到的clip覆盖在原始的clip上，让后让它用渐入和渐出的效果显示和移除显示 下面是代码1234567891011121314151617181920212223242526272829303132333435363738394041""" requires scikit-image installed (for vfx.painting) """ from moviepy.editor import *# WE TAKE THE SUBCLIPS WHICH ARE 2 SECONDS BEFORE &amp; AFTER THE FREEZEcharade = VideoFileClip("../../videos/charade.mp4")tfreeze = cvsecs(19.21) # Time of the freeze, 19'21# when using several subclips of a same clip, it can be faster# to create 'coreaders' of the clip (=other entrance points).clip_before = charade.coreader().subclip(tfreeze -2,tfreeze)clip_after = charade.coreader().subclip(tfreeze ,tfreeze +2)# THE FRAME TO FREEZEim_freeze = charade.to_ImageClip(tfreeze)painting = (charade.fx( vfx.painting, saturation = 1.6,black = 0.006) .to_ImageClip(tfreeze)) txt = TextClip('Audrey',font='Amiri-regular',fontsize=35)painting_txt = (CompositeVideoClip([painting,txt.set_pos((10,180))]) .add_mask() .set_duration(3) .crossfadein( 0.5) .crossfadeout( 0.5))# FADEIN/FADEOUT EFFECT ON THE PAINTED IMAGEpainting_fading = CompositeVideoClip([im_freeze,painting_txt])# FINAL CLIP AND RENDERINGfinal_clip = concatenate_videoclips([ clip_before, painting_fading.set_duration(3), clip_after])final_clip.write_videofile('../../audrey.avi',fps=charade.fps, codec = "mpeg4", audio_bitrate="3000k")]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[14.特效_部分隐藏字幕]]></title>
    <url>%2Fmoviepy%2F14.%E7%89%B9%E6%95%88_%E9%83%A8%E5%88%86%E9%9A%90%E8%97%8F%E5%AD%97%E5%B9%95.html</url>
    <content type="text"><![CDATA[部分隐藏字幕奇怪的很，作者这个代码跑不通，不加duration 要报错，加了duration 又没字，尬 12345678910111213141516171819202122232425from moviepy.editor import *from moviepy.video.tools.credits import credits1 # 加载山背景的clip，截取，变慢，画面变暗clip = (VideoFileClip('./MY_VIDEOS/my_concatenate.mp4', audio=False) .subclip(0,7) .speedx(0.4) .fx(vfx.colorx, 0.7)) # 保存第一帧画面，一会使用GIMP处理，增加一个mask#clip.save_frame('./images/magic_Mask.png') # 加载mountain maskmask = ImageClip('./images/magic_Mask.png',ismask=True).set_duration(3)# 用一个文本文件内容生成字幕m_credits = credits1('./worlds/China_zhou.txt',3*clip.w/4)scrolling_credits = m_credits.set_pos(lambda t:('center',-10*t)).set_duration(3) # 让字幕以10像素每秒的速度滚动起来final = CompositeVideoClip([clip, scrolling_credits, clip.set_mask(mask)])final.write_gif('magic_worlds_show.gif',fps=10)]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[13.特效_星战序幕]]></title>
    <url>%2Fmoviepy%2F13.%E7%89%B9%E6%95%88_%E6%98%9F%E6%88%98%E5%BA%8F%E5%B9%95.html</url>
    <content type="text"><![CDATA[星战序幕星战效果手册效果 注意： 这里可能存在numpy版本过高的问题，解决方案有两种 安装低版本的numpy，但是可能把你的其他库影响了，比如我的话，tensorflow会被影响 报错是arraypad.py缺少函数，那我们就把函数添加进去，比如我的路径是： 1C:\Users\EDZ\AppData\Roaming\Python\Python37\site-packages\numpy\lib\arraypad.py 添加代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899def _normalize_shape(ndarray, shape, cast_to_int=True): """ Private function which does some checks and normalizes the possibly much simpler representations of 'pad_width', 'stat_length', 'constant_values', 'end_values'. Parameters ---------- narray : ndarray Input ndarray shape : &#123;sequence, array_like, float, int&#125;, optional The width of padding (pad_width), the number of elements on the edge of the narray used for statistics (stat_length), the constant value(s) to use when filling padded regions (constant_values), or the endpoint target(s) for linear ramps (end_values). ((before_1, after_1), ... (before_N, after_N)) unique number of elements for each axis where `N` is rank of `narray`. ((before, after),) yields same before and after constants for each axis. (constant,) or val is a shortcut for before = after = constant for all axes. cast_to_int : bool, optional Controls if values in ``shape`` will be rounded and cast to int before being returned. Returns ------- normalized_shape : tuple of tuples val =&gt; ((val, val), (val, val), ...) [[val1, val2], [val3, val4], ...] =&gt; ((val1, val2), (val3, val4), ...) ((val1, val2), (val3, val4), ...) =&gt; no change [[val1, val2], ] =&gt; ((val1, val2), (val1, val2), ...) ((val1, val2), ) =&gt; ((val1, val2), (val1, val2), ...) [[val , ], ] =&gt; ((val, val), (val, val), ...) ((val , ), ) =&gt; ((val, val), (val, val), ...) """ ndims = ndarray.ndim # Shortcut shape=None if shape is None: return ((None, None), ) * ndims # Convert any input `info` to a NumPy array shape_arr = np.asarray(shape) try: shape_arr = np.broadcast_to(shape_arr, (ndims, 2)) except ValueError: fmt = "Unable to create correctly shaped tuple from %s" raise ValueError(fmt % (shape,)) # Cast if necessary if cast_to_int is True: shape_arr = np.round(shape_arr).astype(int) # Convert list of lists to tuple of tuples return tuple(tuple(axis) for axis in shape_arr.tolist())def _validate_lengths(narray, number_elements): """ Private function which does some checks and reformats pad_width and stat_length using _normalize_shape. Parameters ---------- narray : ndarray Input ndarray number_elements : &#123;sequence, int&#125;, optional The width of padding (pad_width) or the number of elements on the edge of the narray used for statistics (stat_length). ((before_1, after_1), ... (before_N, after_N)) unique number of elements for each axis. ((before, after),) yields same before and after constants for each axis. (constant,) or int is a shortcut for before = after = constant for all axes. Returns ------- _validate_lengths : tuple of tuples int =&gt; ((int, int), (int, int), ...) [[int1, int2], [int3, int4], ...] =&gt; ((int1, int2), (int3, int4), ...) ((int1, int2), (int3, int4), ...) =&gt; no change [[int1, int2], ] =&gt; ((int1, int2), (int1, int2), ...) ((int1, int2), ) =&gt; ((int1, int2), (int1, int2), ...) [[int , ], ] =&gt; ((int, int), (int, int), ...) ((int , ), ) =&gt; ((int, int), (int, int), ...) """ normshp = _normalize_shape(narray, number_elements) for i in normshp: chk = [1 if x is None else x for x in i] chk = [1 if x &gt;= 0 else -1 for x in chk] if (chk[0] &lt; 0) or (chk[1] &lt; 0): fmt = "%s cannot contain negative values." raise ValueError(fmt % (number_elements,)) return normshp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179"""Description of the video:Mimic of Star Wars' opening title. A text with a (false)perspective effect goes towards the end of space, on abackground made of stars. Slight fading effect on the text."""import numpy as npfrom skimage import transform as tffrom moviepy.editor import *from moviepy.video.tools.drawing import color_gradient# RESOLUTIONw = 720h = int(w*9/16) # 16/9 screen //这里转一下，否则后面需要整型的时候会报错，同时，像素点这种，float也是不科学的moviesize = w,h# THE RAW TEXTtxt = "\n".join(["A long time ago, in a faraway galaxy,","there lived a prince and a princess","who had never seen the stars, for they","lived deep underground.","","Many years before, the prince's","grandfather had ventured out to the","surface and had been burnt to ashes by","solar winds.","","One day, as the princess was coding","and the prince was shopping online, a","meteor landed just a few megameters","from the couple's flat."])# Add blankstxt = 10*"\n" +txt + 10*"\n"# CREATE THE TEXT IMAGEclip_txt = TextClip(txt,color='white', align='West',fontsize=25, font='Xolonium-Bold', method='label')# SCROLL THE TEXT IMAGE BY CROPPING A MOVING AREAtxt_speed = 27fl = lambda gf,t : gf(t)[int(txt_speed*t):int(txt_speed*t)+h,:]moving_txt= clip_txt.fl(fl, apply_to=['mask'])# ADD A VANISHING EFFECT ON THE TEXT WITH A GRADIENT MASKgrad = color_gradient(moving_txt.size,p1=(0,2*h/3), p2=(0,h/4),col1=0.0,col2=1.0)gradmask = ImageClip(grad,ismask=True)fl = lambda pic : np.minimum(pic,gradmask.img)moving_txt.mask = moving_txt.mask.fl_image(fl)# WARP THE TEXT INTO A TRAPEZOID (PERSPECTIVE EFFECT)def trapzWarp(pic,cx,cy,ismask=False): """ Complicated function (will be latex packaged as a fx) """ Y,X = pic.shape[:2] src = np.array([[0,0],[X,0],[X,Y],[0,Y]]) dst = np.array([[cx*X,cy*Y],[(1-cx)*X,cy*Y],[X,Y],[0,Y]]) tform = tf.ProjectiveTransform() tform.estimate(src,dst) im = tf.warp(pic, tform.inverse, output_shape=(Y,X)) return im if ismask else (im*255).astype('uint8')fl_im = lambda pic : trapzWarp(pic,0.2,0.3)fl_mask = lambda pic : trapzWarp(pic,0.2,0.3, ismask=True)warped_txt= moving_txt.fl_image(fl_im)warped_txt.mask = warped_txt.mask.fl_image(fl_mask)# BACKGROUND IMAGE, DARKENED AT 60%stars = ImageClip('./images/base.jpg')stars_darkened = stars.fl_image(lambda pic: (0.6*pic).astype('int16'))# COMPOSE THE MOVIEfinal = CompositeVideoClip([ stars_darkened, warped_txt.set_pos(('center','bottom'))], size = moviesize)# WRITE TO A FILEfinal.set_duration(8).write_gif("starworms.gif", fps=5)# This script is heavy (30s of computations to render 8s of video)"""===================================================================== CODE FOR THE VIDEO TUTORIAL We will now code the video tutorial for this video. When you think about it, it is a code for a video explaining how to make another video using some code (this is so meta !). This code uses the variables of the previous code (it should be placed after that previous code to work).====================================================================="""def annotate(clip,txt,txt_color='white',bg_color=(0,0,255)): """ Writes a text at the bottom of the clip. """ txtclip = TextClip(txt, fontsize=20, font='Ubuntu-bold', color=txt_color) txtclip = txtclip.on_color((clip.w,txtclip.h+6), color=(0,0,255), pos=(6,'center')) cvc = CompositeVideoClip([clip , txtclip.set_pos((0,'bottom'))]) return cvc.set_duration(clip.duration)def resizeCenter(clip): return clip.resize( height=h).set_pos('center') def composeCenter(clip): return CompositeVideoClip([clip.set_pos('center')],size=moviesize)annotated_clips = [ annotate(clip,text) for clip,text in [ (composeCenter(resizeCenter(stars)).subclip(0,3), "This is a public domain picture of stars"),(CompositeVideoClip([stars],moviesize).subclip(0,3), "We only keep one part."),(CompositeVideoClip([stars_darkened],moviesize).subclip(0,3), "We darken it a little."),(composeCenter(resizeCenter(clip_txt)).subclip(0,3), "We generate a text image."),(composeCenter(moving_txt.set_mask(None)).subclip(6,9), "We scroll the text by cropping a moving region of it."),(composeCenter(gradmask.to_RGB()).subclip(0,2), "We add this mask to the clip."),(composeCenter(moving_txt).subclip(6,9), "Here is the result"),(composeCenter(warped_txt).subclip(6,9), "We now warp this clip in a trapezoid."),(final.subclip(6,9), "We finally superimpose with the stars.")]]# Concatenate and write to a fileconcatenate_videoclips(annotated_clips).write_gif('tutorial.gif', fps=5)]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[12.特效_一个音频示例]]></title>
    <url>%2Fmoviepy%2F12.%E7%89%B9%E6%95%88_%E4%B8%80%E4%B8%AA%E9%9F%B3%E9%A2%91%E7%A4%BA%E4%BE%8B.html</url>
    <content type="text"><![CDATA[一个音频示例使用MoviePy组合带有声音的影片剪辑的示例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960"""Description of the video:The screen is split in two parts showing Carry and Audrey at the phone,talking at the same time, because it is actually two scenes of a samemovie put together."""from moviepy.editor import *from moviepy.video.tools.drawing import color_splitduration = 6 # duration of the final clip# LOAD THE MAIN SCENE# this small video contains the two scenes that we will put together.main_clip = VideoFileClip("../../videos/charadePhone.mp4")W,H = main_clip.size# MAKE THE LEFT CLIP : cut, crop, add a mask mask = color_split((2*W/3,H), p1=(W/3,H), p2=(2*W/3,0), col1=1, col2=0, grad_width=2) mask_clip = ImageClip(mask, ismask=True) clip_left = (main_clip.coreader() .subclip(0,duration) .crop( x1=60, x2=60 + 2*W/3) .set_mask(mask_clip))# MAKE THE RIGHT CLIP : cut, crop, add a mask mask = color_split((2*W/3,H), p1=(2,H), p2=(W/3+2,0), col1=0, col2=1, grad_width=2)mask_clip = ImageClip(mask, ismask=True)clip_right = (main_clip.coreader() .subclip(21,21+duration) .crop(x1=70, x2=70+2*W/3) .set_mask(mask_clip))# ASSEMBLE AND WRITE THE MOVIE TO A FILEcc = CompositeVideoClip([clip_right.set_pos('right').volumex(0.4), clip_left.set_pos('left').volumex(0.4)], size = (W,H))#cc.preview()cc.write_videofile("../../biphone3.avi",fps=24, codec='mpeg4')]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[11.特效_一个简单的音乐视频]]></title>
    <url>%2Fmoviepy%2F11.%E7%89%B9%E6%95%88_%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E9%9F%B3%E4%B9%90%E8%A7%86%E9%A2%91.html</url>
    <content type="text"><![CDATA[一个简单的音乐视频这是一个没有声音（音乐视频的声音）的示例，很快将被一个真实的音乐视频示例替换（代码将保持99％相同）。MoviePy的理念是，对于我要制作的每个新音乐视频，我只需要复制/粘贴此代码，并修改几行。 123456789101112131415161718192021222324252627282930313233343536373839404142from moviepy.editor import *# UKULELE CLIP, OBTAINED BY CUTTING AND CROPPING# RAW FOOTAGEukulele = VideoFileClip("../../videos/moi_ukulele.MOV", audio=False).\ subclip(60+33, 60+50).\ crop(486, 180, 1196, 570)w,h = moviesize = ukulele.size# THE PIANO FOOTAGE IS DOWNSIZED, HAS A WHITE MARGIN, IS# IN THE BOTTOM RIGHT CORNER piano = (VideoFileClip("../../videos/douceamb.mp4",audio=False). subclip(30,50). resize((w/3,h/3)). # one third of the total screen margin( 6,color=(255,255,255)). #white margin margin( bottom=20, right=20, opacity=0). # transparent set_pos(('right','bottom')) )# A CLIP WITH A TEXT AND A BLACK SEMI-OPAQUE BACKGROUNDtxt = TextClip("V. Zulkoninov - Ukulele Sonata", font='Amiri-regular', color='white',fontsize=24)txt_col = txt.on_color(size=(ukulele.w + txt.w,txt.h-10), color=(0,0,0), pos=(6,'center'), col_opacity=0.6)# THE TEXT CLIP IS ANIMATED.# I am *NOT* explaining the formula, understands who can/want.txt_mov = txt_col.set_pos( lambda t: (max(w/30,int(w-0.5*w*t)), max(5*h/6,int(100*t))) )# FINAL ASSEMBLYfinal = CompositeVideoClip([ukulele,txt_mov,piano])final.subclip(0,5).write_videofile("ukulele.avi",fps=24,codec='libx264')]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[10.特效_重建15世纪舞蹈]]></title>
    <url>%2Fmoviepy%2F10.%E7%89%B9%E6%95%88_%E9%87%8D%E5%BB%BA15%E4%B8%96%E7%BA%AA%E8%88%9E%E8%B9%88.html</url>
    <content type="text"><![CDATA[重建15世纪舞蹈素材来源在注释中，这个代码比较综合，适合实战练习 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# -*- coding: utf-8 -*-"""Result: https://www.youtube.com/watch?v=Qu7HJrsEYFgThis is how we can imagine knights dancing at the 15th century, based on a veryserious historical study here: https://www.youtube.com/watch?v=zvCvOC2VwDcHere is what we do:0- Get the video of a dancing knight, and a (Creative Commons) audio music file.1- load the audio file and automatically find the tempo2- load the video and automatically find a segment that loops well3- extract this segment, slow it down so that it matches the audio tempo, and make it loop forever.4- Symmetrize this segment so that we will get two knights instead of one5- Add a title screen and some credits, write to a file.This example has been originally edited in an IPython Notebook, which makes iteasy to preview and fine-tune each part of the editing."""from moviepy.editor import *from moviepy.video.tools.cuts import find_video_periodfrom moviepy.audio.tools.cuts import find_audio_period# Next lines are for downloading the required videos from Youtube.# To do this you must have youtube-dl installed, otherwise you will need to# download the videos by hand and rename them, as follows:# https://www.youtube.com/watch?v=zvCvOC2VwDc =&gt; knights.mp4# https://www.youtube.com/watch?v=lkY3Ek9VPtg =&gt; frontier.mp4import osif not os.path.exists("knights.mp4"): os.system("youtube-dl zvCvOC2VwDc -o knights.mp4") os.system("youtube-dl lkY3Ek9VPtg -o frontier.mp4")#==========# LOAD, EDIT, ANALYZE THE AUDIOaudio = (AudioFileClip("frontier.mp4") .subclip((4,7), (4,18)) .audio_fadein(1) .audio_fadeout(1))audio_period = find_audio_period(audio)print ('Analyzed the audio, found a period of %.02f seconds'%audio_period)# LOAD, EDIT, ANALYZE THE VIDEOclip = (VideoFileClip("./knights.mp4", audio=False) .subclip((1,24.15),(1,26)) .crop(x1=332, x2=910, y2=686))video_period = find_video_period(clip, tmin=.3)print ('Analyzed the video, found a period of %.02f seconds'%video_period)edited_right = (clip.subclip(0,video_period) .speedx(final_duration=2*audio_period) .fx(vfx.loop, duration=audio.duration) .subclip(.25))edited_left = edited_right.fx(vfx.mirror_x)dancing_knights = (clips_array([[edited_left, edited_right]]) .fadein(1).fadeout(1).set_audio(audio).subclip(.3))# MAKE THE TITLE SCREENtxt_title = (TextClip("15th century dancing\n(hypothetical)", fontsize=70, font="Century-Schoolbook-Roman", color="white") .margin(top=15, opacity=0) .set_position(("center","top")))title = (CompositeVideoClip([dancing_knights.to_ImageClip(), txt_title]) .fadein(.5) .set_duration(3.5))# MAKE THE CREDITS SCREENtxt_credits = """CREDITSVideo excerpt: Le combat en armure au XVe siècleBy J. Donzé, D. Jaquet, T. Schmuziger,Université de Genève, Musée National de Moyen AgeMusic: "Frontier", by DOCTOR VOXUnder licence Creative Commonshttps://www.youtube.com/user/DOCTORVOXofficialVideo editing © Zulko 2014 Licence Creative Commons (CC BY 4.0)Edited with MoviePy: http://zulko.github.io/moviepy/"""credits = (TextClip(txt_credits, color='white', font="Century-Schoolbook-Roman", fontsize=35, kerning=-2, interline=-1, bg_color='black', size=title.size) .set_duration(2.5) .fadein(.5) .fadeout(.5))# ASSEMBLE EVERYTHING, WRITE TO FILEfinal = concatenate_videoclips([title, dancing_knights, credits])final.write_videofile("dancing_knights.mp4", fps=clip.fps, audio_bitrate="1000k", bitrate="4000k")]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[9.特效_炫动的字母]]></title>
    <url>%2Fmoviepy%2F9.%E7%89%B9%E6%95%88_%E7%82%AB%E5%8A%A8%E7%9A%84%E5%AD%97%E6%AF%8D.html</url>
    <content type="text"><![CDATA[炫动的字母 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npfrom moviepy.editor import *from moviepy.video.tools.segmenting import findObjects # 目标是创建炫动的文字，先创建TextClip，然后设置它居中 screensize = (720,460)txtClip = TextClip('Cool effect',color='white', font="Amiri-Bold", kerning = 5, fontsize=100)cvc = CompositeVideoClip( [txtClip.set_pos('center')], size=screensize) # 下面的四个函数，定义了四种移动字母的方式 # helper functionrotMatrix = lambda a: np.array( [[np.cos(a),np.sin(a)], [-np.sin(a),np.cos(a)]] ) def vortex(screenpos,i,nletters): d = lambda t : 1.0/(0.3+t**8) #damping a = i*np.pi/ nletters # angle of the movement v = rotMatrix(a).dot([-1,0]) if i%2 : v[1] = -v[1] return lambda t: screenpos+400*d(t)*rotMatrix(0.5*d(t)*a).dot(v) def cascade(screenpos,i,nletters): v = np.array([0,-1]) d = lambda t : 1 if t&lt;0 else abs(np.sinc(t)/(1+t**4)) return lambda t: screenpos+v*400*d(t-0.15*i) def arrive(screenpos,i,nletters): v = np.array([-1,0]) d = lambda t : max(0, 3-3*t) return lambda t: screenpos-400*v*d(t-0.2*i) def vortexout(screenpos,i,nletters): d = lambda t : max(0,t) #damping a = i*np.pi/ nletters # angle of the movement v = rotMatrix(a).dot([-1,0]) if i%2 : v[1] = -v[1] return lambda t: screenpos+400*d(t-0.1*i)*rotMatrix(-0.2*d(t)*a).dot(v) # WE USE THE PLUGIN findObjects TO LOCATE AND SEPARATE EACH LETTER letters = findObjects(cvc) # a list of ImageClips # 让字母动起来def moveLetters(letters, funcpos): return [ letter.set_pos(funcpos(letter.screenpos,i,len(letters))) for i,letter in enumerate(letters)] clips = [ CompositeVideoClip( moveLetters(letters,funcpos), size = screensize).subclip(0,5) for funcpos in [vortex, cascade, arrive, vortexout] ] # 连接，写入文件final_clip = concatenate_videoclips(clips)final_clip.write_gif('./images/cool_worlds.gif',fps=10)]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[8.创建和导出video_clips]]></title>
    <url>%2Fmoviepy%2F8.%E5%88%9B%E5%BB%BA%E5%92%8C%E5%AF%BC%E5%87%BAvideo_clips.html</url>
    <content type="text"><![CDATA[创建和导出video clipsvideo 和 audio clips是moviepy中的核心的对象。这篇博文，我们会介绍不同的clip，展示如何创建他们，以及如何将它们导出到文件中。关于修改和处理vedio clip的信息点这里。一些基本的clips。VIDEO CLIPS clip = VideoClip(make_frame, duration=4) # 自定义动画 clip = VideoFileClip(“my_vedio_file.mp4”) # 文件格式还可以是avi、webm、 gif等 clip = ImageSequenceClip([‘imagefile.jpeg’, … ], fps=24) clip = ImageClip(‘my_picture.png’) # 文件格式还可以是 png、tiff等 clip = TextClip(‘Hello!’, font=”Amiri-Bold”, fintsize=70, color=’black’) clip = ColorClip(size=(460, 380), color=[R,G,B]) AUDIO CLIPS clip = AudioFileClip(“my_audio_file.mp3”) #文件格式还可以是ogg、wav或者也可以是一个vedio clip = AudioArrayClip(numpy_array, fps=44100) # 一个numpy数组 clip = AudioClip(make_frame, duration=3) # 使用一个方法make_frame(t) video clip的分类video clip好比是较长的video的积木块。从技术上讲，这些clip都是使用clip.get_frame(t)方法，在某一时间t获得的HxWx3 numpy 格式的剪辑帧的数组。主要有两种分类，首先是动画形式的clip（包括VideoClip、VideoFileClip）;其次是非动画形式的clip，循环显示相同的画面（包括ImageClip、TextClip、ColorClip都在此列）。还有一种特殊的clip，那就是masks，它属于上面的类别，但是他输出的灰色帧表示在另一个剪辑中哪些部分是可见的还有哪些部分是不可见的。 VideoClipVideoClip是movipy中其他所有的vedio clips的父类。如果你的需求仅仅是编辑视频文件，你是不会用到他的。事实上，一般只有在我们通过一些第三方的库生成动画的时候才会用到这个类。我们所需要做的，就是定义一个make_frame(t)函数，这个函数在一个给定的时刻t返回HxWx3形式的numpy数组。下面是一个使用第三方图像库Gizeh的例子： 123456789101112import gizehimport moviepy.editor as mpy def make_frame(t): surface = gizeh.Surface(128, 128) # 宽、高 radius = W*(1+ (t*(2-t))**2 )/6 # 半径随时间变化 circle = gizeh.circle(radius, xy=(64,64), fill(1,0,0)) circle.draw(surface) return surface.getnpimage() # 返回一个8bit RGB数组 clip = mpy.VideoClip(make_frame, duration=2)clip.write_gif('circle.gif', fps=15) 注意：在上面的make_frame函数并没有一个默认的帧率，所以我们必须给write_gif或者write_videofile提供一个fps（frames per second）. Python有一个称为PyCairo的快速强大的矢量图形库，但是它很难学习和使用。Gizeh在Cairo之上实现了一些类，这些类使它更易于使用 VideoFileClipVideoFileClip就是从视频文件（大部分的视频格式都支持）或者GIF格式文件读取生成的clip。我们可以像下面这样去加载一个video。 12myclip = VideoFileClip("some_video.avi")myclip = VideoFileClip("some_animation,gif") 每一个clip都有一个fps参数，当我们使用write_videofile, write_gif等去将一个clip切分成小的clip去保存的时候，原有的clip的fps会默认地传递给新产生的clip。 12345myclip = VideoFileClip("some_video.avi")print(myclip.fps) # 输出为30# 剪辑这个clip 的10s-25s，这会保存fpsmyclip2 = myclip.subclip(10, 25)myclip2.write_gif("test.gif") # 这个gif文件的fps=30 ImageSequenceClip顾名思义，就是一系列图片创建的clip，你可以像下面这样调用： 1clip = ImageSequenceClip(images_list, fps=25) images_list可以有三种形式： 图片name list，将会按照list顺序播放 一个目录，这个目录下面的所有的图片都会被播放，按照字母或者数字的顺序播放 一系列frames (Numpy arrays)，从其他的clip剪辑得到的，顺序是按照list顺序播放 当我们使用第一种或者第二种的时候，我们可以通过选择load_images=True来指定是否将所有的图片直接加载到RAM中。当然，这样的操作仅仅是在我们的图片不是很多，且每张图片都被使用超过1次的时候。 ImageClipImageClip就是一个一直播同一张图片的vedio clip，我们可以按照下面的方式创建： 123myclip = ImageClip("some_pic.jpeg")myclip = ImageClip(some_array) # 一个（高 × 宽 × 3）RGB numpy 数组myclip = some_video_clip.to_ImageClip(t = '01:00:00') # 在1h时刻的帧画面 TextClip生成一个TextClip需要安装ImageMagick，并且和moviepy关联。 下面是一个创建TextClip的示例 1myclip = TextClip('Hello!', font='Amiri-Bold') 注意：这里的font可以是任何安装在你的电脑上的font，如果照抄这里的代码，有可能会有问题，因为你的电脑上并没有这个样式。那么如何查看当前设备可用的样式呢？ 1TextClip.list('font') # 可以获得当前设备所有可用的font 但是这里会报错，教程是复数形式，显示让用font，这里不知道是什么问题，不过我把ttf格式的字体单独复制出来，就能够正常加载了，我觉得这是个更好的方案，因为字体版权问题比较复杂，单独建立一个免费的字体库，也容易避免纠纷。就不用固定字体，而是设置成路径变量 ImageMagick对样式也有自己的指定的名称，比如：通常的normal Amiri被称为Amiri-Regular，Impact font被称为Impact-Normal。 查找所有给定字体相关的所有字体，使用下面的方法： 1TextClip.search('Amiri','fonts') # 查找所有包含Amiri的字体 问题同上注意：使用笔划或者（轮廓）的话，对于较小的字母来说不会有很好的结果。如果我们需要一个小的具有轮廓的TextClip，我们最好是先生成一个大点的文字，然后再将它缩小，这样的效果才会好。这个效果就像字体镂空一样 1myclip = TextClip('Hello!', fontsize=70, stroke_width=5).resize(height=15) TextClip是有很多很多的可选项的，对齐、字距（字母距离）、笔划大小、背景、字体环绕等等。如果需要详细的信息，可以去官网参考手册去找或者等待后续中文文档更新。 Mask clipsmask clip是一种特殊的vide clip，当一个带有mask的video clip 和其他的video clips合成的时候，可以用来指示哪些像素是可见的，哪些是不显示的。在导出GIF或者PNG文件的时候，mask也可以用来定义透明度。 mask clip和普通的vedio clip最基本的不同在于： 标准的clip的每一帧中的每一个像素是由3个色元即R-G-B组合成的，取值范围是0-255。（0-255， 0-255，0-255） mask clip每个像素仅一个色元组件， 取值范围只有0-1。要么着一个像素可见，要么这个像素透明。另一方面看，mask只有灰色的通道。 当我们创建或者加载一个带有mask 的clip的时候，我们会像下面这样声明： 123maskclip = VideoClip(makeframe, duration=4, ismask=True)maskclip = ImageClip('my_mask.jpeg', ismask=True)maskclip = VideoFileClip("myvideo.mp4", ismask=True) 将一个mask clip覆盖在一个和他同样尺寸的clip时使用 1myclip.set_mask(mask_clip) 一些通过一个alpha 层来支持透明度的图片格式，比如PNG，moviepy中应该使用mask来对应： 12myclip = ImageClip("image.png", transparent=True) # 默认为Truemyclip.mask # 图片的alpha层 任何一个video clip都可以通过clip.to_mask()转换为一个mask，任何一个mask也可以通过my_mask_clip.to_RGB()转换为标准的RGB video clip. 导出video clipsvideo文件（.mp4 , .webm, .ogv…） 将一个clip写入一个文件 myclip.write_videofile(‘movie.mp4’) # 默认编码 ‘libx264’, 24 fps myclip.write_videofile(‘movie.mp4’, fps=15) myclip.write_videofile(‘movie.webm’) myclip.write_videofile(‘movie.webm’, audio=False) # 不使用音频MoviePy对于大多数的扩展名都有默认的编解码，如果我们希望使用其他的编解码，或者我们不喜欢默认的编解码的话，我们可以通过codec=’mpeg4’来使用自己指定的编解码。当我们导出为一个文件的时候，也有许多的可选项（比特率，是否将音频也导出，文件大小优化，使用处理器个数，等等） 有些时候，我们使用moviepy是无法获得一个clip的duration这个值的。然后，duration这个必须通过clip.set_duration()来指定: 创建一个clip，显示一个花的图片5秒钟 my_clip = ImageClip(‘flower.jpeg’) # 现在是无限时间，不停地循环 my_clip.write_videofile(‘flower.mp4’) # 这一步将会出错，因为没有指定duration my_clip.set_duration(5).write_videofile(‘flower.mp4’) # 正确执行 GIF 动画将vedio clip导出为一个gif动画 my_clip.write_gif(‘test.gif’, fps=12)注意：需要安装ImageMagick。此外，我们还可以通过添加选项program=&#39;ffmpeg&#39;来使用ffmpeg生成gif动画。使用ffmpeg生成gif动图，虽然要快一点，但是看起来效果可能会不太好，而且没有办法去优化。 导出图片我们可以将某一帧导出为图片 myclip.save_frame(‘frame.png’) # 默认保存第一帧画面 myclip.save_frame(‘frame.jpeg’, t=’01:00:00’) # 保存1h时刻的帧画面如果这些clip有mask的话，mask将会作为图片的alpha层被保存。除非我们使用withmask=False指定不保存mask。 示例代码我的一篇讲OpenCV羽化问题的博客，也提到过mask原理，当时是在图片上的蒙版，如果我们想要在视频中达到类似的效果应该怎么做呢？先回顾一下效果图： 原图：背景图：mask图：最终羽化图： ！！！待续1做出来有问题的，教程讲的不够详细，等解决了再来补充]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[7.MoviePy中的Audio]]></title>
    <url>%2Fmoviepy%2F7.MoviePy%E4%B8%AD%E7%9A%84Audio.html</url>
    <content type="text"><![CDATA[MoviePy中的Audio下面主要是演示在moviepy中如何创建和编辑audio clips。之前曾经说过，在moviepy中，当我们剪切，混合，拼接video clip的时候，audio clip并不需要我们去操心，他会自动的随着video完成相应的处理。这篇博文提到的对于audio clip的操作主要是为了开发者的两种情况 首先，那就是我们有兴趣只对音频做处理 其次就是我们希望为video配自定义的音频。 什么是audio clipsAudioClips和moviepy中VideoClips的概念很像：autio clip有长度，像video clip一样可以被剪切，组合等等。一个显著的区别在于audioclip.get_frame(t)。 创建一个新的audio clipaudio clip创建可以有两种方式 一种是从一个文件直接创建autio clip 第二种办法呢，就是从一个视频文件中提取声音的部分。 123from moviepy.editor import *audioclip = AudioFileClip("some_audiofile.mp3")audioclip = AudioFileCLip("some_video.avi") 另外，也可以从已经创建的VideoClip对象中直接获取audio clip 12videoclip = VideoFileClip("some_video.avi")audioclip = videoclip.audio 合成音频（待续）导出和预览audio clips我们可以将一个audio clip分配到一个vedio clip 声音轨道上 1videoclip2 = videoclip.set_audio(my_audioclip) 我有话说其实我现在还没接触音频处理的部分，虽然单独处理音频挺有意思，但是我目前没有好的录制条件，而且去噪什么的音频处理属于另一个领域了，我尚且图像还没有精通，短时间内不考虑改变方向，等对图像领域有深刻认识的时候再来研究。某种意义上来说，图像音频都是波，说不定哪天我就融会贯通兼修悟道了呢。]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[6.使用matplotlib]]></title>
    <url>%2Fmoviepy%2F6.%E4%BD%BF%E7%94%A8matplotlib.html</url>
    <content type="text"><![CDATA[使用matplotlib到现在为止也没有做一个matplotlib的教程，原因在于其实我使用matplotlib的频率其实不是很高，而且有很多现成的画廊代码，这个对于数据可视化来讲还是挺重要的，记得还是要做啊，少玩几把游戏把时间腾出来用户自定义动画moviepy允许开发者自定义动画：定义一个方法，以numpy数组的形式在动画中给定的时间返回一帧画面。 示例代码123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npfrom moviepy.editor import VideoClipfrom moviepy.video.io.bindings import mplfig_to_npimage x = np.linspace(-2, 2, 200) duration = 2 fig,ax = plt.subplots()def make_frame(t): ax.clear() ax.plot(x, np.sin(x**2) + np.sin(x + 2*np.pi/duration * t), lw=3) ax.set_ylim(-1.5, 2.5) return mplfig_to_npimage(fig) animation = VideoClip(make_frame, duration=duration)animation.write_gif("./images/matplotlib.gif", fps=20) 这里使用了write_gif，这样的话，我们就方便在文档中嵌入效果图了，前面也提到过效果如下：如果和我一样对机器学习感兴趣，你甚至可以将你的训练过程可视化出来，这样就能更直观地观察数据了]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[5.如何高效率使用MoviePy]]></title>
    <url>%2Fmoviepy%2F5.%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E7%8E%87%E4%BD%BF%E7%94%A8MoviePy.html</url>
    <content type="text"><![CDATA[如何高效率使用MoviePy如何高效率使用MoviePy接下来，这一节将会介绍许多全世界MoviePy开发者总结的开发经验。开始学习使用MoviePy最好的方式是使用IPython Notebook：它可以让预览clip变得简单，有自动补全的功能，而且可以显示函数库中不同方法的文档。我们是不是应该使用moviepy.editor？ 这个文档中的大部分例子都会用到子模块moviepy.editor，但是这个子模块并不适用于所有需求。所以，我们应该使用他吗？ 简短回答： 如果我们是使用moviepy手动地编辑视频，那就使用他。 如果我们使用的是MoviePy内置的庞大的库、程序、或者网络服务，最好还是避免它，只需要加载我们需要的方法就够了。 预览clip的方式当我们使用MoviePy编辑视频或者实现一个效果的时候，我们会进行一些试验、和错误的过程。通常情况我没每一次的试验都会话费相对较长的时间，这一节，会告诉大家很多的小窍门，让这些尝试的过程变得快一点。大多情况下，我们只需要得到一帧画面就可以知道程序有没有正确执行，这样，我们只需要保存一帧画面到一个文件，就可以去验证，像下面这样： save_frame 12my_clip.save_frame("frame.jpeg") #保存第一帧my_clip.save_frame("frame.png", t=2) #保存2s时刻的那一帧 clip.show / clip.previewclip.show和clip.preview这两个方法可以在Pygame 的窗口中展示。这是最快的预览方式，clip的生成和显示几乎是同时发生的，而且对于获取一个像素点的颜色和坐标也有用。完成后按Esc键退出。 一个clip可以按照下面的方式来预览 1234my_clip.preview() # 默认fps=15 预览my_clip.preview(fps=25)my_clip.preview(fps=15, audio=False) # 不生成/播放音频my_audio_clip.preview(fps=22000) 假如你在预览的画面中点击了某些地方，将会输出被点击像素的位置坐标和颜色。按Esc可以终端预览。 如果当clip十分复杂而且电脑配置很渣，运行很慢的时候，预览一般会出现比真实画面速度慢的情况，这个时候，我没可以尝试降低帧率，或者使用clip.resize减小clip的尺寸。 ipython_display 如果不使用clip.show()或者clip.preview(),使用IPython Notebook显示clip也是一个不错的选择。 通过ipython_display，我们可以将图片、视频、音频等嵌入，既可以来自于一个文件，也可以来自于一个clip 1234567ipython_display(my_video_clip)ipython_display(my_image_clip)ipython_display(my_audio_clip) ipython_display("my_picture.jpeg")ipython_display("my_video.mp4")ipython_display("my_sound.mp3") 只有当rendering在notebook的最后一行时才有效。我们也可以把ipython_display作为一个clip方法调用 my_video_clip.ipython_display() 如果我们的clip需要一个帧率，我们可以指定ipython_display的fps=25。 如果我们仅仅需要显示一下视频clip的快照，那么，我们直接指定time就行 my_video_clip.ipython_display(t=15) #将会显示15s处的快照 我们也可以提供任何的HTML5选项作为关键词参数，例如：当一个clip特别大的时候，我们一般会这么写 ipython_display(my_clip, width=400) # HTML5 将会把他的尺寸变成400像素当我们编辑一个GIF格式的动画，并且确保他有完好的循环。我们可以让视频自动开始，并且循环播放。 ipython_display(my_clip, autoplay=1, loop=1)注意：事实上，ipython_display是将clip完全地入我们的notebook。这样做的优点是无论我们移动nootbook还是放到线上，视频都会正常的播放。缺点在于，nootbook文件的大小将会变得很大。取决于你的浏览器，多次重新计算和显示视频不光是花费很多的时间，也会占用特别多的内存（RAM），重新启动浏览器会解决这个问题。 我有话说我在使用过程中，发现preview()或者show()的时候，能够正常播放，但是会存在卡死未响应的情况出现，暂时没有找到解决方案]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4.MoviePy-Clips变换与特效]]></title>
    <url>%2Fmoviepy%2F4.MoviePy-Clips%E5%8F%98%E6%8D%A2%E4%B8%8E%E7%89%B9%E6%95%88.html</url>
    <content type="text"><![CDATA[4.MoviePy-Clips变换与特效MoviePy中的几种对clip的修改修改clip属性的方法有 clip.set_duration, clip.set_audio clip.set_mask clip.set_start 等 已经实现的特效 clip.subclip(t1,t2):截取t1到t2时间段内的片段 还有一些高级效果，loop：让clip循环播放 time_mirror:让clip倒播，这些方法位于特殊的模块moviepy.video.fx, moviepy.audio.fx，应用clip.fx方法，比如clip.fx(time_mirror)让视频倒播。 也可以你自己创造一些需要的特效。以上的特效其实本质上并不是原地直接修改的（没有对原始视频修改），而是根据修改产生新的clip。所以，我们如果想让修改生效，需要将修改过的产生的clip赋值给某clip，保存修改。举个栗子。 123my_clip = VideoFileClip("some_file.mp4")my_clip.set_start(t=5) #没有做任何改变，修改会丢失my_new_clip = my_clip.set_start(t=5) #这样才对。moviepy中，修改过的clip要重新赋值给变量，修改才会被保存 所以，当我们写出clip.resize(width=640),moviepy并不是立刻就逐帧修改clip。一般只会先修改第一帧，其他的左右的帧只有在需要的时候(最后写入文件或者预览)才会被resize。 另一方面，可以这样讲，创建一个clip，几乎是不会占用时间和内存的，几乎所有的计算其实发生在最后转换的时刻。 moviepy中的时间表示很多方法都接受时间参数，clip.subclip(t_start,t_end)，截取两个时间点之间的clip片段，在这个方法中，时间既可以用(t_start=230.54),以秒的时间来表示，也可以用(t_start=(3,50.54))，以3分50.54秒的形式来表示，还可以 (t_start=(0,3,50.54))或者 (t_start=(00:03:50.54)),以，小时，分钟，秒的形式来表示。 大多数没有赋值的时间参数会有一个默认值，比如clip.subclip(t_start=50)，t_end的默认值就是视频的长度，clip.subclip(t_end=50)，那么t_start就默认为0.当时间是负数的时候，代表倒数n秒。比如，clip.subclip(-20, -10)会截取倒数20秒到倒数10秒之间的片段。 改变clip属性的方法clip.fx假定我们很多修改clip的方法。这些方法呢，都是输入一个clip和一些参数，输出一个新的clip。 123effect_1(clip, args1) -&gt; new clipeffect_2(clip, args2) -&gt; new clipeffect_3(clip, args3) -&gt; new clip 如果我们要按顺序，依次修改clip，那么你可能会这样写： 1newclip = effect_3( effect_2( effect_1(clip, args3), args2), args1) 但是上面的代码可读性不高，我们可以利用clip.fx来实现一种更简练的写法。 123newclip = (clip.fx( effect_1, args1) .fx( effect_2, args2) .fx( effect_3, args3)) 看上去是不是好多了。在模块moviepy.video.fx和moviepy.audio.fx中已经实现来一些修改clip的方法，这些fx的方法会自动的作用到和clip相关的声音和mask上，所以我们在修改clip的时候并不需要去关心声音和mask的处理，除非我们确实要对声音或者mask做一些特殊的处理。 在实际应用中，当我们使用from moviepy import.editor *的时候，这两个模块会被加载为vfx和afx，所以我们会像下面这样写 12345from moviepy.editor import *clip = (VideoFileClip("myvideo.avi") .fx( vfx.resize, width=460) # 尺寸变化，保持纵横比 .fx( vfx.speedx, 2) # 2倍速 .fx( vfx.colorx, 0.5)) # 画面调暗 为方便起见，当我们使用moviepy.editor的时候，比如我们使用resize的时候，我们经常会使用clip.resize(...)这样的简便的写法来代替clip.fx( vfx.resize, ...)的写法。按照上面的操作，我们生成一个视频来看一下 代码示例：1234567from moviepy.editor import *clip = (VideoFileClip("./MY_VIDEOS/my_concatenate.mp4") .fx( vfx.resize, width=460) # 尺寸变化，保持纵横比 .fx( vfx.speedx, 2) # 2倍速 .fx( vfx.colorx, 0.5)) # 画面调暗clip.write_videofile('./MY_VIDEOS/my_vfx.mp4') 就不附上效果图了，加速也不太好表示，后面学制作GIF就可以展示了 创建用户自定义特效的方法clip.fl我们可以使用clip.fl_time、clip.fl_image或者更普遍的clip.fl来修改一个clip。 使用clip.fl_time修改clip的时间线 12modifiedClip1 = my_clip.fl_time(lambda t: 3*t)modifiedClip2 = my_clip.fl_time(lambda t: 1+sin(t)) 现在，modifiedClip1是my_clip的三倍速度播放效果。modifiedClip2只会在t=0s和t=2s之间播放，并且会有倒放效果。在后一个例子中，我们已经创建了一个无限持续的clip（现在并不会出什么问题）。 用clip.fl_image来修改一个clip的显示 123def invert_green_blue(image): return image[:,:,[0,2,1]]modifiedClip = my_clip.fl_image(invert_green_blue) 将会把一个clip每一帧的绿色和蓝色通道转换。接下来，我没还有可能需要同时对时间和每一帧的画面都做处理，我们用clip.fl(filter)方法也是可以实现的，这个filter必须是传入两个参数，返回一个画面的函数。第一个参数是get_frame方法（例如：g(t)给定一个时刻，然后返回在那一时刻这个clip中对应的一帧），第二个参数就是时间。 123456789def scroll(get_frame, t): ''' 这个函数返回：当前帧的一个‘区域’ 上述所说的‘区域’的位置依赖于第二个参数，时间 ''' frame = get_frame(t) frame_region = frame[int(t):int(t)+360,:] return frame_regionmodifiedClip = my_clip.fl(scroll) 以上代码，将会向下滚动360个像素高度。 当我们在编写一些新的效果的时候，应该尽可能地使用fl_time和fl_image，而不是fl。原因在于，在这些效果被应用于ImageClip的时候，MoviePy会认为这些方法不需要应用于每一帧，渲染速度会更快。 代码示例12345678910111213141516171819202122from moviepy.editor import *from math import sinmy_clip = VideoFileClip('./MY_VIDEOS/my_concatenate.mp4')modifiedClip1 = my_clip.fl_time(lambda t: 3*t).set_duration(3) //这里按照教程不加duration会报错modifiedClip2 = my_clip.fl_time(lambda t: 1+sin(t)).set_duration(20)def invert_green_blue(image): return image[:,:,[0,2,1]]modifiedClip3 = my_clip.fl_image(invert_green_blue)def scroll(get_frame, t): frame = get_frame(t) frame_region = frame[int(t):int(t)+360,:] return frame_regionmodifiedClip4 = my_clip.fl(scroll)modifiedClip1.write_videofile('./MY_VIDEOS/my_fl_time1.mp4')modifiedClip2.write_videofile('./MY_VIDEOS/my_fl_time2.mp4')modifiedClip3.write_videofile('./MY_VIDEOS/my_fl_image.mp4')modifiedClip4.write_videofile('./MY_VIDEOS/my_fl.mp4') modifiedClip4的效果如下：]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3.MoviePy-视频合成]]></title>
    <url>%2Fmoviepy%2F3.MoviePy-%E8%A7%86%E9%A2%91%E5%90%88%E6%88%90.html</url>
    <content type="text"><![CDATA[3.MoviePy-视频合成Mixing clips（就是视频合成，拼接混合）视频的排版，众所周知，这是一个非线性编辑的技术活，有时候还要把几段clip在一个新的clip里面一起播放。还有一点很重要，一般情况下，每一个video clip都带有一个audio 和一个 mask，分别是audio clip和mask clip，通俗地讲就是每一个视频切片带有一个音频切片和一个蒙版切片。所以，当我们把几个视频排版混合到一起的时候。声音轨道和mask也同时被混合在一起形成最终的clip。所以但多数时候我们是不需要操心声音和mask的。 视频拼接和视频叠加这里有三个重要的函数：concatenate_videoclips，clip_array和CompositeVideoClips 视频拼接视频一个接一个地播放1234567891011from moviepy.editor import VideoFileClip, concatenate_videoclips # 将我的几个短视频拼接起来clip1 = VideoFileClip("./videos/1.mp4")clip2 = VideoFileClip("./videos/2.mp4")clip3 = VideoFileClip("./videos/3.mp4")clip4 = VideoFileClip("./videos/4.mp4") finalclip = concatenate_videoclips([clip1 , clip2 , clip3 , clip4])finalclip.write_videofile("my_concatenate.mp4") 如果你只想裁剪1.mp4中的一部分，你可以使用clip1 = VideoFileClip(&quot;./videos/1.mp4&quot;).subclip(0,1) 一个大的画面同时播几个视频123456789101112from moviepy.editor import VideoFileClip, clips_array, vfx clip1 = VideoFileClip("./videos/1.mp4").margin(10)clip2 = clip1.fx(vfx.mirror_x)#x轴镜像clip3 = clip1.fx(vfx.mirror_y)#y轴镜像clip4 = clip1.resize(0.6)#尺寸等比缩放0.6 final_clip = clips_array([ [clip1, clip2], [clip3, clip3] ])final_clip.resize(width=480).write_videofile("./MY_VIDEOS/my_stack.mp4") 效果如下： 视频叠加这个相对而言是比较高级的，和ps里面的图层类似，或者说，假设你想做一个星球大战的视频，而你正是男主，你也不可能飞到银河系，所以你可以自己录制一个单一背景的动作，再把你提取出来放在星球大战的视频上面，一个特效小电影就完成了，当然你也可以做其他一些有趣的，嘿嘿。 语法12video = CompositeVideoClip([clip1, clip2, clip3])// clip2压在clip1上，clip3压在最上面 有时候是需要我们的clip浮在一个比较大的合成体的上方的，那这个时候我们就需要声明合成clip的尺寸。看看下面的栗子。 1video = CompositeVideoClip([clip1, clip2, clip3], size=(720,480)) 开始和结束时间每个clip会在通过clip.start函数声明的时间开始播放，我们可以像下面这样去设置 1clip1 = clip1.set_start(5) #在5秒时开始 所以一般情况下，我们代码中的视频堆叠都长这个样子。 12345video = CompositeVideoClip([ clip1, #在第0秒开始 clip2.set_start(5), #在第5秒开始 clip3.set_start(9) #第9秒开始 ]) 为了避免过渡僵硬，我们可以使用一些特效，可能clip2开始的时候，正好是clip1将要结束的时机，这样的情况，我们可以让clip2使用在一秒内“渐入”的特效来显示。 12345video = CompositeVideoClip([ clip1, clip2.set_start(5).crossfadein(1), clip3.set_start(9).crossfadein(1.5) ]) 位置设定下面的栗子就是通过指定坐标的形式（距离左上方的像素距离）把clip2和clip3在画面中指定位置。 12345video = CompositeVideoClip([ clip1, clip2.set_pos((45,150)), clip3.set_pos((90,100)) ]) 在moviepy中我们有很多的方法定位clip的位置 123456clip2.set_pos((45,150)) #像素坐标clip2.set_pos("center") #居中clip2.set_pos(("center","top")) #水平方向居中，但是处置方向放置在顶部clip2.set_pos(("left","center")) #水平方向放置在左边，垂直方向居中clip2.set_pos((0.4,0.7), relative=True) #0.4倍宽，0.7倍高处clip2.set_pos(lambda t: ('center', 50+t)) #水平居中，向下移动 合成声音在混合vedio clips时，MoviePy将会自动地把这些vedio自带的audio clip按照一定形式混合在一起形成最终的clip如果你有一些特殊的的定制合成音频的需求，应该使用CompositeAudioClip和concatenate_audioclips这俩类。 12345678from moviepy.editor imports * concat = concatenate_audioclips([clip1, clip2, clip3])compo = CompositeAudioClip([ aclip1.volumex(1.2), aclip2.set_start(5), # start at t=5s aclip3.set_start(9) ]) 视频堆叠代码教程没有完整的示例代码，不妨学完操作一下 12345678910from moviepy.editor import VideoFileClip,CompositeVideoClip clip1 = VideoFileClip('./MY_VIDEOS/my_concatenate.mp4')clip2 = VideoFileClip('./MY_VIDEOS/my_stack.mp4')final_clip = CompositeVideoClip([ clip1.set_start(1) , clip2.set_position((20,20)).crossfadein(1) ])final_clip.write_videofile("./MY_VIDEOS/my_CompositeVideoClip.mp4") 效果如下：]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2.快速入门]]></title>
    <url>%2Fmoviepy%2F2.%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[2.快速入门代码示例在一个MoviePy脚本中，我们可以加载视频和音频，然后修改它们，将他们合并，然后把最终结果写入到一个新的文件中。下面的例子，加载视频，在视频中间添加一个标题显示10秒钟，然后把结果写入到一个新的文件内。1234567891011121314151617181920# 导入需要的库from moviepy.editor import *# 从本地载入视频base.mp4并截取00:00:20 - 00:00:30部分clip = VideoFileClip("base.mp4").subclip(20,30)# 调低音频音量 (volume x 0.8)clip = clip.volumex(0.8)# 做一个txt clip. 自定义样式，颜色，字体txt_clip = TextClip("大家好，才是真的好",fontsize=70,color='white',font='simhei.ttf')# 文本clip在屏幕正中显示持续10秒txt_clip = txt_clip.set_pos('center').set_duration(10)# 把 text clip 的内容覆盖 video clipvideo = CompositeVideoClip([clip, txt_clip])# 把最后生成的视频导出到文件内video.write_videofile("hilo.webm") 这里因为使用了中文，所以要声明一下字体，有些字体是不支持中文的，可以建立一个自己的字体库 MoviePy如何工作MoviePy使用软件ffmpeg读取和导出视频和音频文件，使用ImageMagick生产文字和GIF图。这些处理过程都有赖于Python强大的数学处理库，高级特效和软件加强用到了许多的Python图像处理库。 基本概念MoviePy中最核心的对象就是clips 。AudioClips和VedioClips，开发者可以对clips进行修改(剪切，调速度，调亮度…)或者和其他clip混合拼接到一起。使用PyGame或者IPython NoteBook还可以预览。 VedioClips可以由视频文件，图像，文本或者动画来创建实例。vedio clip可以拥有一个音频轨道(audio clip) 和一个叠加层的vedio clip(这是一个特殊的VedioClip，这意味着，当一个视频和其他VedioClip混合的时候，这个叠加层clip是隐藏的) 一个clip可以被MoviePy中多多种效果作用，比如(clip.resize(width=”360”), clip.subclip(t1,t2), or clip.fx(vfx.black_white)). 当然，也可以被用户自定义的效果作用。MoviePy实现了许多类似（clip.fl, clip.fx）这样的方法，可以简便地修改效果。 在moviepy.video.tools里面，还可以找到许多好东西，实现很多高级功能，比如跟踪视频中的一个对象，画一些简单的图形，斜线，或者颜色，制作副标题等等效果 即使moviepy没有图形化的用户交互界面，但是在我们高质量地加工我们的视频的时候，我们仍然有很多可以允许很好地控制和调节脚本的预览方式。 我有话说官方手册中的参考手册大概是这样的构成： ClipClasses of Video ClipsVideoFileClipImageClipColorClipTextClipCompositeVideoClipAudioClipAudioClipAudioFileClipCompositeAudioClipmoviepy.video.fx (vfx)audio.fxvideo.toolsaudio.toolsFFMPEG toolsDecorators目录部分就将这个库的思想表达的很清楚了，一个视频编辑库的组成大致就是这样，我没遇到这个库的时候，思路也是这样设计的，视频编辑无非就是对帧图像的处理，所以独立出来文字，图像，音频作为对象OOP的优势就体现出来了，MoviePy就是把整个视频编辑当作一个对象，视频分为视频图像和音频两部分，视频又由图像，文字，视频流组成。]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MoviePy介绍]]></title>
    <url>%2Fmoviepy%2FMoviePy%E4%BB%8B%E7%BB%8D.html</url>
    <content type="text"><![CDATA[MoviePy - 介绍 参考：https://blog.csdn.net/ucsheep/article/details/80999939 MoviePy是一个用于视频编辑的python模块，你可以用它实现一些基本的操作(比如视频剪辑，视频拼接，插入标题),还可以实现视频合成，还有视频处理，抑或用它加入一些自定义的高级的特效。总之，它的功能还是蛮丰富的。此外，MoviePy可以读写绝大多数常见的视频格式，甚至包括GIF格式！是不是很兴奋呢？基于Python2.7以上的版本，MoviePy可以轻松实现跨Mac/Windows/Linux统统没问题，这也以意味着，MoviePy项目可以部署到服务端，在服务端进行视频处理。真是福音啊！ 下面的内容介绍以下MoviePy适用于何种场景以及MoviePy如何工作。 我需要使用MoviePy吗？出于以下的情景或原因，我们可能会有使用Python做视频编辑的需求。 我们有大量的视频需要处理，或者采用复杂的方式将他们拼接。 我们需要在服务端自动地创建大量视频或者GIF图。 我们需要在视频中创建视频编辑器中所没有的一些特殊的特效，我们只能敲代码来实现。 为其他Python库(例如：Matplotlib, Mayavi, Gizeh, scikit-images)生产的图片创建动画效果。 当然，MoviePy并不是万能的，下面这样的需求，MoviePy也无能为力。 当我们需要逐帧的做图像分析时(例如人脸检测)，这真的不是MoviePy的强项，不如使用ImageIO，OpenCV，SimpleCV这样专业的库去处理 我们仅仅是要将一段视频，或者一系列图片接进一个目标视频中时，我们使用ffmpeg就搞定了，也不用强行使用MoviePy MoviePy的优点与局限优点 简单直观，基本操作一般一行代码搞定。对于初学者，代码很容易理解和学习。 灵活弹性，开发者拥有对视频或者音频中每一帧的全部控制权，这也使得我们在创建自定义效果时得心应手。 跨平台，使用的ffmpeg各个平台都有，可以移植到不同的平台运行。 局限性 不支持流媒体，它也确实不是为了处理这样的视频而设计的。 当同时使用太多(官网说&gt;100)的视频，音频，或者图片的时候，我们有可能会遇到内存问题。内存问题亟待优化。]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[1.下载安装]]></title>
    <url>%2Fmoviepy%2F1.%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[1.下载和安装安装MoviePy1pip install moviepy -i http://pypi.douban.com/simple环境介绍MoviePy 依赖的python库有 Numpy,imageio,Decorator, 和tqdm这些都会在安装MoviePy的时候自动安装.MoviePy的视频读写依赖于我们大名鼎鼎的FFMPEG这个软件，你也无需担心，如果你之前没有安装过的话，FFMPEG应该在ImageIO安装的时候就被自动下载和安装了(当然，这有可能会话费一段时间),有一点值得注意的是，MoviePy对于FFMPEG的版本要求是3.2.4，一般自己安装会高于这个版本，所以会报错，我们可以参考源码文件moviepy/config_defaults.py修改FFMPEG_BINARY属性，进而使用自定义的版本 安装ImageMagick官网下载 通常还是要设置一下路径，让程序能够找到它，例如： 我的安装位置：1C:\Program Files\ImageMagick-7.0.8-Q16\magick.exe 需要修改的文件：1H:\blog_anaconda_use\anaconda\Lib\site-packages\moviepy\config_defaults.py 修改内容：12345import osFFMPEG_BINARY = os.getenv('FFMPEG_BINARY', 'ffmpeg-imageio')# IMAGEMAGICK_BINARY = os.getenv('IMAGEMAGICK_BINARY', 'auto-detect') // 修改前IMAGEMAGICK_BINARY = r'C:\Program Files\ImageMagick-7.0.8-Q16\magick.exe' // 修改后]]></content>
      <categories>
        <category>MoviePy 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV实现指定轮廓羽化效果]]></title>
    <url>%2Fopencv%2FOpenCV%E5%AE%9E%E7%8E%B0%E6%8C%87%E5%AE%9A%E8%BD%AE%E5%BB%93%E7%BE%BD%E5%8C%96%E6%95%88%E6%9E%9C.html</url>
    <content type="text"><![CDATA[OpenCV实现指定轮廓羽化效果查阅了大多数OpenCV的特效教程，发现羽化效果讲解的较少，要么干脆全图羽化，要么蒙版盖住，全背景羽化，我在这里也试过很多方法，发现像素分配的方法较为简单，且过渡效果比较好，所以记录一下。1.素材准备用一张合适的图片，不要太大，避免处理起来很慢，现在的代码还未进行优化，只是为了讲清楚原理 2.用OpenCV看一下图片12345678import cv2 as cvimport numpy as npimage = cv.imread('images/feather0.jpg')cv.imshow('image',image)cv.waitKey(0)cv.destroyAllWindows() 3.简单裁剪一下方便计算圆心位置，当然也可以用CV的函数，只是这里我个人觉得没必要再用mask这些复杂的， 先看一下图像形状 12image.shapeOut[15]: (470, 700, 3) 决定取（400,600）大小，使用numpy切片操作就可以了 123456789import cv2 as cvimport numpy as npimage = cv.imread('images/feather0.jpg')[:400,:600]cv.imshow('image',image)cv.imwrite('./images/400x600.jpg',image)cv.waitKey(0)cv.destroyAllWindows() 400x600.jpg显示如下： 我们就拿这张图片作为原图进行操作了 目的：在图像中心画一个圆，让圆内像素不变，圆外的亮度变暗，圆的边缘进行羽化效果的实现 4.来左边和我一起画个圆圆的公式比较简单，我们做一个一样大小的蒙版，让圆内像素为白色，圆外为黑色 12345678910111213141516171819202122import cv2 as cvimport numpy as npimage = cv.imread('images/400x600.jpg')h,w,c = image.shapec_x = w//2c_y = h//2# 创建一样大小的数组mask = np.zeros((400,600),"uint8")# 小于150半径的圆内像素填充为255for i in range(h): for j in range(w): if (i-200)**2 + (j-300)**2 &lt; 150**2: mask[i,j] = 255cv.imshow('mask',mask)cv.imwrite('images/mask.jpg',mask)cv.imshow('image',image)cv.waitKey(0)cv.destroyAllWindows() mask.jpg显示如下： 5.对蒙版图像进行blur平滑处理123blur = cv.blur(mask,(60,60))cv.imwrite('images/blur.jpg',blur) blur.jpg显示如下： 6.做暗背景图我们选择比较简单的方式，直接按比例减小图像像素值 1234567for i in range(h): for j in range(w): image[i,j,0] = int(image[i,j,0] * 0.3) image[i,j,1] = int(image[i,j,1] * 0.3) image[i,j,2] = int(image[i,j,2] * 0.3)cv.imwrite('images/dark.jpg',image) 7.进行融合现在我们手上就有了原图：400x600.jpg，平滑mask图：blur.jpg，黑暗背景图：dark.jpg 现在我们读入这三张图片，进行一个融合，让边缘达到一个羽化的效果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import cv2 as cvimport numpy as npfrom matplotlib import pyplot as plt image = cv.imread('images/400x600.jpg')blur = cv.imread('images/blur.jpg',0)dark = cv.imread('images/dark.jpg')h,w,c = image.shapec_x = w//2c_y = h//2feather = np.zeros((400,600,3),"uint8")for i in range(h): for j in range(w): if blur[i,j] == 0: feather[i,j] = dark[i,j] if blur[i,j] == 255: feather[i,j] = image[i,j] else: a = blur[i,j] b = 255 - a feather[i,j] = image[i,j]*(a/255)+ \ dark[i,j]*(b/255)plt.figure()plt.subplot(2,2,1)plt.imshow(image[:,:,::-1])plt.xticks([])plt.yticks([])plt.subplot(2,2,2)plt.imshow(blur,cmap='gray')plt.xticks([])plt.yticks([])plt.subplot(2,2,3)plt.imshow(dark[:,:,::-1])plt.xticks([])plt.yticks([])plt.subplot(2,2,4)plt.imshow(feather[:,:,::-1])plt.xticks([])plt.yticks([]) #cv.imshow('image',image)#cv.imshow('blur',blur)#cv.imshow('dark',dark)#cv.imshow('feather',feather)cv.waitKey(0)cv.destroyAllWindows() 8.一些注意点 虽然我们保存blur的时候只用了单通道，但是读取的时候依然会当做三通道来读取，所以我们这里读取的时候，声明了读取灰度图： 1blur = cv.imread('images/blur.jpg',0) OpenCV使用的是BGR而我们画图的matplotlib是RGB，那么如何转换呢？这里有三种方式： 1234567891011img = cv2.imread('test.jpg')B, G, R = cv2.split(img) #BGR转RGB，方法1img_rgb1 = cv2.merge([R, G, B]) #BGR转RGB，方法2img_rgb2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #BGR转RGB，方法3img_rgb3 = img[:,:,::-1] 我们发现blur.jpg本来就是灰度图，那么就不需要转换了。但是这里会显示出一种热力图，还是得声明一下给matplotlib传入的是灰度图，所以： 1plt.imshow(blur,cmap='gray') 9.最终显示效果 10.完整代码参考12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import cv2 as cvimport numpy as npdef load_image(imagepath): return cv.imread(imagepath)def crop_image(image,x1,y1,x2,y2): return image[x1:x2,y1:y2]def creat_mask(image,circle_size,blur_size): mask = np.zeros((h,w),"uint8") for i in range(h): for j in range(w): if (i-c_y)**2 + (j-c_x)**2 &lt; circle_size**2: mask[i,j] = 255 mask = cv.blur(mask,(blur_size,blur_size)) return maskdef creat_back(image,scale): dark = np.zeros((h,w,3),"uint8") for i in range(h): for j in range(w): dark[i,j,0] = int(image[i,j,0] * scale) dark[i,j,1] = int(image[i,j,1] * scale) dark[i,j,2] = int(image[i,j,2] * scale) return darkdef gena_img(image,blur,dark): feather = np.zeros((h,w,3),"uint8") for i in range(h): for j in range(w): a = blur[i,j] / 255 b = 1 - a feather[i,j] = image[i,j]*a+ \ dark[i,j]*b return feather if __name__ == "__main__": path = './images/feather0.jpg' x1 = 0 y1 = 0 x2 = 400 y2 = 600 img = load_image(path) image = crop_image(img,x1,y1,x2,y2) h,w,c = image.shape c_x = w//2 c_y = h//2 circle_size = 200 blur_size = 60 scale = 0.1 blur = creat_mask(image,circle_size,blur_size) dark = creat_back(image,scale) feather = gena_img(image,blur,dark) cv.imshow('blur',blur) cv.imshow('dark',dark) cv.imshow('image',image) cv.imshow('feather',feather) cv.waitKey(0) cv.destroyAllWindows()]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV-Python绑定]]></title>
    <url>%2Fopencv%2FOpenCV-Python%E7%BB%91%E5%AE%9A.html</url>
    <content type="text"><![CDATA[目标学习： OpenCV-Python绑定如何生成？ 如何将新的OpenCV模块扩展到Python？ OpenCV-Python绑定如何生成？在OpenCV中，所有算法均以C++实现。但是这些算法可以从不同的语言（例如Python，Java等）中使用。绑定生成器使这成为可能。这些生成器在C++和Python之间建立了桥梁，使用户能够从Python调用C++函数。为了全面了解后台发生的事情，需要对Python/C API有充分的了解。在官方Python文档中可以找到一个有关将C++函数扩展到Python的简单示例。因此，通过手动编写包装函数将OpenCV中的所有函数扩展到Python是一项耗时的任务。因此，OpenCV以更智能的方式进行操作。OpenCV使用位于中的某些Python脚本，从C++头自动生成这些包装器函数modules/python/src2。我们将调查他们的工作。 首先，modules/python/CMakeFiles.txt是一个CMake脚本，它检查要扩展到Python的模块。它将自动检查所有要扩展的模块并获取其头文件。这些头文件包含该特定模块的所有类，函数，常量等的列表。 其次，将这些头文件传递给Python脚本modules/python/src2/gen2.py。这是Python绑定生成器脚本。它调用另一个Python脚本modules/python/src2/hdr_parser.py。这是标头解析器脚本。此标头解析器将完整的标头文件拆分为较小的Python列表。因此，这些列表包含有关特定函数，类等的所有详细信息。例如，将对一个函数进行解析以获取一个包含函数名称，返回类型，输入参数，参数类型等的列表。最终列表包含所有函数，枚举的详细信息，头文件中的structs，classs等。 但是标头解析器不会解析标头文件中的所有函数/类。开发人员必须指定应将哪些函数导出到Python。为此，在这些声明的开头添加了某些宏，这些宏使标头解析器可以标识要解析的函数。这些宏由对特定功能进行编程的开发人员添加。简而言之，开发人员决定哪些功能应该扩展到Python，哪些不应该。这些宏的详细信息将在下一个会话中给出。 因此头解析器将返回已解析函数的最终大列表。我们的生成器脚本（gen2.py）将为标头解析器解析的所有函数/类/枚举/结构创建包装函数（在编译期间，您可以在build/modules/python/文件夹中找到这些标头文件为pyopencv_genic _ *h文件）。但是可能会有一些基本的OpenCV数据类型，例如Mat，Vec4i，Size。它们需要手动扩展。例如，Mat类型应扩展为Numpy数组，Size应扩展为两个整数的元组，等等。类似地，可能会有一些复杂的结构/类/函数等需要手动扩展。所有此类手动包装功能都放在中modules/python/src2/cv2.cpp。 所以现在剩下的就是这些包装文件的编译了，这给了我们cv2模块。因此，当您使用res = equalizeHist(img1,img2)Python 调用函数时，您传递了两个numpy数组，并且期望另一个numpy数组作为输出。因此，将这些numpy数组转换为cv::Mat，然后在C++中调用equalizeHist()函数。最终结果将res转换回Numpy数组。简而言之，几乎所有操作都是在C++中完成的，这给了我们几乎与C++相同的速度。 因此，这是OpenCV-Python绑定生成方式的基本版本。 如何将新模块扩展到Python？头解析器根据添加到函数声明中的一些包装宏来解析头文件。枚举常量不需要任何包装宏。它们会自动包装。但是其余的函数，类等需要包装宏。 使用CV_EXPORTS_W宏扩展功能。一个例子如下所示。 1CV_EXPORTS_W void equalizeHist( InputArray src, OutputArray dst ); 标头解析器可以理解诸如InputArray，OutputArray等关键字的输入和输出参数。但是有时，我们可能需要对输入和输出进行硬编码。为此，像宏CV_OUT，CV_IN_OUT使用等。 12CV_EXPORTS_W void minEnclosingCircle( InputArray points, CV_OUT Point2f&amp; center, CV_OUT float&amp; radius ); 1对于大类，`CV_EXPORTS_W`也使用。使用扩展类方法`CV_WRAP`。同样，`CV_PROP`用于类字段。 1234567class CV_EXPORTS_W CLAHE : public Algorithm&#123;public: CV_WRAP virtual void apply(InputArray src, OutputArray dst) = 0; CV_WRAP virtual void setClipLimit(double clipLimit) = 0; CV_WRAP virtual double getClipLimit() const = 0;&#125; 可以使用重载功能CV_EXPORTS_AS。但是我们需要传递一个新名称，以便在Python中使用该名称调用每个函数。以下面的积分函数为例。提供了三个函数，因此每个函数在Python中都带有一个后缀。同样CV_WRAP_AS可以用于包装重载方法。 123456CV_EXPORTS_W void integral( InputArray src, OutputArray sum, int sdepth = -1 );CV_EXPORTS_AS(integral2) void integral( InputArray src, OutputArray sum, OutputArray sqsum, int sdepth = -1, int sqdepth = -1 );CV_EXPORTS_AS(integral3) void integral( InputArray src, OutputArray sum, OutputArray sqsum, OutputArray tilted, int sdepth = -1, int sqdepth = -1 ); 小类/结构使用扩展CV_EXPORTS_W_SIMPLE。这些结构按值传递给C++函数。例子是KeyPoint，Match等他们的方法是通过扩展CV_WRAP和领域被扩展CV_PROP_RW。 1234567891011class CV_EXPORTS_W_SIMPLE DMatch&#123;public: CV_WRAP DMatch(); CV_WRAP DMatch(int _queryIdx, int _trainIdx, float _distance); CV_WRAP DMatch(int _queryIdx, int _trainIdx, int _imgIdx, float _distance); CV_PROP_RW int queryIdx; // query descriptor index CV_PROP_RW int trainIdx; // train descriptor index CV_PROP_RW int imgIdx; // train image index CV_PROP_RW float distance;&#125;; 可以使用CV_EXPORTS_W_MAP导出到Python本机字典的位置来导出其他一些小类/结构。Moments()是一个例子。 1234567class CV_EXPORTS_W_MAP Moments&#123;public: CV_PROP_RW double m00, m10, m01, m20, m11, m02, m30, m21, m12, m03; CV_PROP_RW double mu20, mu11, mu02, mu30, mu21, mu12, mu03; CV_PROP_RW double nu20, nu11, nu02, nu30, nu21, nu12, nu03;&#125;; 因此，这些是OpenCV中可用的主要扩展宏。通常，开发人员必须将适当的宏放在适当的位置。其余的由生成器脚本完成。有时，在某些特殊情况下，生成器脚本无法创建包装。此类功能需要手动处理，为此，您需要编写自己的pyopencv_*.hpp扩展标头并将其放入模块的misc/python子目录中。但是在大多数情况下，根据OpenCV编码指南编写的代码将由生成器脚本自动包装。 更高级的情况涉及为Python提供C++接口中不存在的其他功能，例如额外的方法，类型映射或提供默认参数。稍后我们将以UMat数据类型为例。首先，提供特定于Python的方法的方法CV_WRAP_PHANTOM与相似CV_WRAP，除了使用方法标头作为其参数，而且您需要在自己的pyopencv_*.hpp扩展名中提供方法主体。UMat::queue()并且UMat::context()是C ++接口中不存在的此类幻影方法的示例，但在Python端处理OpenCL功能是必需的。其次，如果一个已经存在的数据类型可以映射到您的类，则最好使用CV_WRAP_MAPPABLE以源类型为参数，而不是设计自己的绑定函数。这是UMat来自的映射的情况Mat。最后，如果需要默认参数，但本机C++接口中未提供默认参数，则可以在Python端将其作为参数提供CV_WRAP_DEFAULT。按照以下UMat::getMat示例： 123456789101112class CV_EXPORTS_W UMat&#123;public: // You would need to provide `static bool cv_mappable_to(const Ptr&lt;Mat&gt;&amp; src, Ptr&lt;UMat&gt;&amp; dst)` CV_WRAP_MAPPABLE(Ptr&lt;Mat&gt;); /! returns the OpenCL queue used by OpenCV UMat. // You would need to provide the method body in the binder code CV_WRAP_PHANTOM(static void* queue()); // You would need to provide the method body in the binder code CV_WRAP_PHANTOM(static void* context()); CV_WRAP_AS(get) Mat getMat(int flags CV_WRAP_DEFAULT(ACCESS_RW)) const;&#125;;]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[对象检测（objdetect模块）]]></title>
    <url>%2Fopencv%2F%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B%EF%BC%88objdetect%E6%A8%A1%E5%9D%97%EF%BC%89.html</url>
    <content type="text"><![CDATA[级联分类器目标 我们将学习Haar级联对象检测的工作原理。 我们将使用基于Haar Feature的Cascade分类器了解人脸检测和眼睛检测的基础知识我们将使用cv :: CascadeClassifier类来检测视频流中的对象。特别是，我们将使用以下功能：cv :: CascadeClassifier :: load来加载.xml分类器文件。它可以是Haar或LBP分类器cv :: CascadeClassifier :: detectMultiScale执行检测。 理论使用基于Haar特征的级联分类器进行对象检测是Paul Viola和Michael Jones在其论文“使用简单特征的增强级联进行快速对象检测”中于2001年提出的一种有效的对象检测方法。这是一种基于机器学习的方法，其中从许多正负图像中训练级联函数。然后用于检测其他图像中的对象。 在这里，我们将进行人脸检测。最初，该算法需要大量正图像（面部图像）和负图像（无面部图像）来训练分类器。然后，我们需要从中提取特征。为此，使用下图所示的Haar功能。它们就像我们的卷积核。每个特征都是通过从黑色矩形下的像素总和中减去白色矩形下的像素总和而获得的单个值。 现在，每个内核的所有可能大小和位置都用于计算许多功能。（试想一下它需要多少计算？即使是一个24x24的窗口也会产生超过160000个特征）。对于每个特征计算，我们需要找到白色和黑色矩形下的像素总和。为了解决这个问题，他们引入了整体形象。无论您的图像有多大，它都会将给定像素的计算减少到仅涉及四个像素的操作。很好，不是吗？它使事情变得超快。 但是在我们计算的所有这些功能中，大多数都不相关。例如，考虑下图。第一行显示了两个良好的功能。选择的第一个特征似乎着眼于眼睛区域通常比鼻子和脸颊区域更暗的性质。选择的第二个功能依赖于眼睛比鼻梁更黑的属性。但是，将相同的窗口应用于脸颊或其他任何地方都是无关紧要的。那么，我们如何从16万多个功能中选择最佳功能？它是由Adaboost实现的。 为此，我们将所有功能应用于所有训练图像。对于每个功能，它会找到最佳的阈值，该阈值会将人脸分为正面和负面。显然，会出现错误或分类错误。我们选择错误率最低的特征，这意味着它们是对人脸和非人脸图像进行最准确分类的特征。（此过程并非如此简单。在开始时，每个图像的权重均相等。在每次分类后，错误分类的图像的权重都会增加。然后执行相同的过程。将计算新的错误率。还要计算新的权重。继续进行此过程，直到达到所需的精度或错误率或找到所需的功能数量为止。 最终分类器是这些弱分类器的加权和。之所以称为弱分类，是因为仅凭它不能对图像进行分类，而是与其他分类一起形成强分类器。该论文说，甚至200个功能都可以提供95％的准确度检测。他们的最终设置具有大约6000个功能。（想象一下，从160000多个功能减少到6000个功能。这是很大的收获）。 因此，现在您拍摄一张照片。取每个24x24窗口。向其应用6000个功能。检查是否有脸。哇..这不是效率低下又费时吗？是的。作者对此有一个很好的解决方案。 在图像中，大多数图像是非面部区域。因此，最好有一种简单的方法来检查窗口是否不是面部区域。如果不是，请一次性丢弃它，不要再次对其进行处理。相反，应将重点放在可能有脸的区域。这样，我们将花费更多时间检查可能的面部区域。 为此，他们引入了级联分类器的概念。不是将所有6000个功能部件应用到一个窗口中，而是将这些功能部件分组到不同阶段的分类器中，并一一应用。（通常前几个阶段将包含很少的功能）。如果窗口在第一阶段失败，则将其丢弃。我们不考虑它的其余功能。如果通过，则应用功能的第二阶段并继续该过程。经过所有阶段的窗口是一个面部区域。那计划怎么样？ 作者的检测器具有6000多个特征，具有38个阶段，在前五个阶段具有1、10、25、25和50个特征。（上图中的两个功能实际上是从Adaboost获得的最佳两个功能）。根据作者的说法，每个子窗口平均评估了6000多个特征中的10个特征。 因此，这是Viola-Jones人脸检测工作原理的简单直观说明。阅读本文以获取更多详细信息，或查看其他资源部分中的参考资料。 OpenCV中的Haar级联检测OpenCV提供了一种训练方法（请参阅Cascade Classifier Training）或预先训练的模型，可以使用cv::CascadeClassifier::load方法读取它。预训练的模型位于OpenCV安装的data文件夹中，或在此处找到。 以下代码示例将使用预训练的Haar级联模型来检测图像中的面部和眼睛。首先，创建一个cv::CascadeClassifier并使用cv::CascadeClassifier::load方法加载必要的XML文件。然后，使用cv::CascadeClassifier::detectMultiScale方法完成检测，该方法返回检测到的脸部或眼睛的边界矩形。 本教程的代码如下所示。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from __future__ import print_functionimport cv2 as cvimport argparsedef detectAndDisplay(frame): frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) frame_gray = cv.equalizeHist(frame_gray) #-- Detect faces faces = face_cascade.detectMultiScale(frame_gray) for (x,y,w,h) in faces: center = (x + w//2, y + h//2) frame = cv.ellipse(frame, center, (w//2, h//2), 0, 0, 360, (255, 0, 255), 4) faceROI = frame_gray[y:y+h,x:x+w] #-- In each face, detect eyes eyes = eyes_cascade.detectMultiScale(faceROI) for (x2,y2,w2,h2) in eyes: eye_center = (x + x2 + w2//2, y + y2 + h2//2) radius = int(round((w2 + h2)*0.25)) frame = cv.circle(frame, eye_center, radius, (255, 0, 0 ), 4) cv.imshow('Capture - Face detection', frame)parser = argparse.ArgumentParser(description='Code for Cascade Classifier tutorial.')parser.add_argument('--face_cascade', help='Path to face cascade.', default='data/haarcascades/haarcascade_frontalface_alt.xml')parser.add_argument('--eyes_cascade', help='Path to eyes cascade.', default='data/haarcascades/haarcascade_eye_tree_eyeglasses.xml')parser.add_argument('--camera', help='Camera devide number.', type=int, default=0)args = parser.parse_args()face_cascade_name = args.face_cascadeeyes_cascade_name = args.eyes_cascadeface_cascade = cv.CascadeClassifier()eyes_cascade = cv.CascadeClassifier()#-- 1. Load the cascadesif not face_cascade.load(cv.samples.findFile(face_cascade_name)): print('--(!)Error loading face cascade') exit(0)if not eyes_cascade.load(cv.samples.findFile(eyes_cascade_name)): print('--(!)Error loading eyes cascade') exit(0)camera_device = args.camera#-- 2. Read the video streamcap = cv.VideoCapture(camera_device)if not cap.isOpened: print('--(!)Error opening video capture') exit(0)while True: ret, frame = cap.read() if frame is None: print('--(!) No captured frame -- Break!') break detectAndDisplay(frame) if cv.waitKey(10) == 27: break 结果这是运行上面的代码并将内置摄像头的视频流用作输入的结果： 确保程序会找到文件haarcascade_frontalface_alt.xml和haarcascade_eye_tree_eyeglasses.xml的路径。它们位于opencv/data/haarcascades中 这是使用文件lbpcascade_frontalface.xml（经过LBP训练）进行人脸检测的结果。对于眼睛，我们继续使用本教程中使用的文件。 级联分类器训练介绍使用弱分类器的增强级联包括两个主要阶段：训练和检测阶段。对象检测教程中介绍了使用基于HAAR或LBP模型的检测阶段。本文档概述了训练自己的弱分类器的级联所需的功能。当前指南将分各个阶段进行：收集训练数据，准备训练数据并执行实际模型训练。 为了支持本教程，将使用几个官方的OpenCV应用程序：opencv_createsamples，opencv_annotation，opencv_traincascade和opencv_visualisation。 重要笔记 如果您遇到任何提及旧的opencv_haartraining工具（不推荐使用，仍在使用OpenCV1.x接口）的教程，请忽略该教程并坚持使用opencv_traincascade工具。此工具是较新的版本，根据OpenCV 2.x和OpenCV 3.x API用C ++编写。opencv_traincascade同时支持类似HAAR的小波特征和LBP（局部二进制模式）特征。与HAAR特征相比，LBP特征产生整数精度，从而产生浮点精度，因此LBP的训练和检测速度都比HAAR特征快几倍。关于LBP和HAAR的检测质量，主要取决于所使用的训练数据和选择的训练参数。可以训练基于LBP的分类器，该分类器将在训练时间的一定百分比内提供与基于HAAR的分类器几乎相同的质量。 来自OpenCV 2.x和OpenCV 3.x（cv :: CascadeClassifier）的较新的级联分类器检测接口支持使用新旧模型格式。如果由于某些原因而使用旧界面，则opencv_traincascade甚至可以旧格式保存（导出）经过训练的级联。然后至少可以在最稳定的界面中训练模型。 opencv_traincascade应用程序可以使用TBB进行多线程处理。要在多核模式下使用它，必须在启用TBB支持的情况下构建OpenCV。 准备训练数据为了训练弱分类器的增强级联，我们需要一组正样本（包含您要检测的实际对象）和一组负图像（包含您不想检测的所有内容）。负样本集必须手动准备，而正样本集是使用opencv_createsamples应用程序创建的。 负样本负样本取自任意图像，其中不包含要检测的对象。这些负图像（从中生成样本）应在特殊的负图像文件中列出，该文件每行包含一个图像路径（可以是绝对路径，也可以是相对路径）。注意，负样本和样本图像也称为背景样本或背景图像，在本文档中可以互换使用。 所描述的图像可能具有不同的尺寸。但是，每个图像都应等于或大于所需的训练窗口大小（与模型尺寸相对应，大多数情况下是对象的平均大小），因为这些图像用于将给定的负像子采样为几个图像具有此训练窗口大小的样本。 否定描述文件的示例： 目录结构： 1234/ IMG img1.jpg img2.jpgbg.txt 您的一组否定窗口样本将用于告诉机器学习步骤，在这种情况下，当尝试查找您感兴趣的对象时，可以增强不需要查找的内容。 正样本正样本由opencv_createsamples应用程序创建。增强过程使用它们来定义在尝试找到感兴趣的对象时模型应实际寻找的内容。该应用程序支持两种生成正样本数据集的方式。 1.您可以从单个正对象图像生成一堆正值。2.您可以自己提供所有肯定的内容，仅使用该工具将其切出，调整大小并以opencv所需的二进制格式放置。 虽然第一种方法对固定对象（例如非常刚性的徽标）效果不错，但对于刚性较差的对象，它往往很快就会失效。在这种情况下，我们建议使用第二种方法。网络上的许多教程甚至都指出，使用opencv_createsamples应用程序，与1000个人工生成的正片相比，可以生成100个真实的对象图像更好的模型。但是，如果您决定采用第一种方法，请记住以下几点： 请注意，在将其提供给上述应用程序之前，您需要多个正样本，因为它仅应用透视变换。 如果您需要一个健壮的模型，请获取涵盖对象类中可能出现的多种变化的样本。例如，对于面孔，您应该考虑不同的种族和年龄段，情绪以及胡须风格。当使用第二种方法时，这也适用。 第一种方法采用带有公司徽标的单个对象图像，并通过随机旋转对象，更改图像强度以及将图像放置在任意背景上，从给定的对象图像中创建大量正样本。随机性的数量和范围可以由opencv_createsamples应用程序的命令行参数控制。 命令行参数： -vec &lt;vec_file_name&gt; ：包含用于训练的正样本的输出文件的名称。 -img &lt;image_file_name&gt; ：源对象图像（例如公司徽标）。 -bg &lt;background_file_name&gt;：背景描述文件；包含图像列表，这些图像用作对象的随机变形版本的背景。 -num &lt;number_of_samples&gt; ：要生成的阳性样本数。 -bgcolor &lt;background_color&gt;：背景色（目前假设为灰度图像）；背景色表示透明色。由于可能存在压缩伪影，因此可以通过-bgthresh指定颜色容忍度。bgcolor-bgthresh和bgcolor + bgthresh范围内的所有像素均被解释为透明的。 -bgthresh &lt;background_color_threshold&gt; -inv ：如果指定，颜色将被反转。 -randinv ：如果指定，颜色将随机反转。 -maxidev &lt;max_intensity_deviation&gt; ：前景样本中像素的最大强度偏差。 -maxxangle &lt;max_x_rotation_angle&gt; ：相对于x轴的最大旋转角度，必须以弧度为单位。 -maxyangle &lt;max_y_rotation_angle&gt; ：朝向y轴的最大旋转角必须以弧度为单位。 -maxzangle &lt;max_z_rotation_angle&gt; ：朝向z轴的最大旋转角必须以弧度为单位。 -show：有用的调试选项。如果指定，将显示每个样本。按Esc将继续示例创建过程，而不会显示每个示例。 -w &lt;sample_width&gt; ：输出样本的宽度（以像素为单位）。 -h &lt;sample_height&gt; ：输出样本的高度（以像素为单位）。当以这种方式运行opencv_createsamples时，将使用以下过程来创建样本对象实例：给定的源图像围绕所有三个轴随机旋转。所选择的角由限制-maxxangle，-maxyangle和-maxzangle。然后，像素具有[bg_color-bg_color_threshold; bg_color + bg_c​​olor_threshold]范围被解释为透明。白噪声被添加到前景的强度。如果-inv指定了键，则前景像素强度会反转。如果-randinv指定了key，则算法将随机选择是否应将反演应用于此样本。最后，将获得的图像放置在背景描述文件中的任意背景上，并调整为由-w和指定的所需大小-h并存储到由-vec命令行选项指定的vec文件中。 也可以从以前标记的图像的集合中获取正样本，这是构建鲁棒对象模型时的理想方式。该集合由类似于背景描述文件的文本文件描述。该文件的每一行都对应一个图像。该行的第一个元素是文件名，后跟对象注释的数量，后跟描述包围矩形（x，y，宽度，高度）的对象坐标的数字。 描述文件的示例： 目录结构： 1234/ IMG img1.jpg img2.jpginfo.dat 文件info.dat： 12img / img1.jpg 1140100 45 45img / img2.jpg 210020050 50 50 30 25 25 图像img1.jpg包含具有以下边界矩形坐标的单个对象实例：（140，100，45，45）。图像img2.jpg包含两个对象实例。 为了从此类集合中创建正样本，-info应指定参数而不是-img： -info &lt;collection_file_name&gt; ：标记的图像集合的描述文件。请注意，在这种情况下，像这样-bg, -bgcolor, -bgthreshold, -inv, -randinv, -maxxangle, -maxyangle, -maxzangle的参数将被简单地忽略并且不再使用。在这种情况下，样本创建的方案如下。通过从原始图像中切出提供的边界框，从给定图像中获取对象实例。然后将它们调整为目标样本大小（由-w和定义-h），并存储在由-vec参数定义的输出vec文件中。无失真应用，所以只能影响参数是-w，-h，-show和-num。 -info也可以使用opencv_annotation工具完成手动创建文件的过程。这是一个开放源代码工具，用于在任何给定图像中直观地选择对象实例的关注区域。以下小节将详细讨论如何使用此应用程序。 额外备注 opencv_createsamples实用程序可用于检查存储在任何给定正样本文件中的样本。为了做到这一点只-vec，-w并-h应指定的参数。 此处提供了vec-file的示例opencv/data/vec_files/trainingfaces_24-24.vec。它可用于训练具有以下窗口大小的面部检测器：-w 24 -h 24。 使用OpenCV的集成注释工具从OpenCV 3.x开始，社区一直在提供和维护用于生成-info文件的开源注释工具。如果构建了OpenCV应用程序，则可以通过命令opencv_annotation访问该工具。 使用该工具非常简单。该工具接受几个必需参数和一些可选参数： --annotations （必需）：注释txt文件的路径，您要在其中存储注释，然后将其传递到-info参数[example-/data/annotations.txt] --images （必填）：包含带有您的对象的图像的文件夹的路径[示例-/ data / testimages /] --maxWindowHeight （可选）：如果输入图像的高度大于此处的给定分辨率，请使用调整图像的大小以便于注释--resizeFactor。 --resizeFactor （可选）：使用-maxWindowHeight参数时用于调整输入图像大小的因子。请注意，可选参数只能一起使用。可以使用的命令示例如下所示 1opencv_annotation --annotations = /path/to/annotations/file.txt --images=/path/to/image/folder/ 此命令将启动一个窗口，其中包含第一张图像和您的鼠标光标，这些窗口将用于注释。有关如何使用注释工具的视频，请参见此处。基本上，有几个按键可以触发一个动作。鼠标左键用于选择对象的第一个角，然后一直进行绘图直到您感觉很好为止，并在记录第二次鼠标左键单击时停止。每次选择后，您有以下选择： 按c：确认注释，将注释变为绿色并确认已存储 按下d：从注释列表中删除最后一个注释（易于删除错误的注释） 按下n：继续下一张图像 按下ESC：这将退出注释软件最后，您将获得一个可用的注释文件，该文件可以传递给-infoopencv_createsamples的参数。 级联训练下一步是基于预先准备的正数和负数数据集对弱分类器的增强级联进行实际训练。 opencv_traincascade应用程序的命令行参数按用途分组： 常用参数： -data &lt;cascade_dir_name&gt;：应将经过训练的分类器存储在哪里。此文件夹应事先手动创建。 -vec &lt;vec_file_name&gt; ：带有正样本的vec文件（由opencv_createsamples实用程序创建）。 -bg &lt;background_file_name&gt;：背景描述文件。这是包含阴性样本图像的文件。 -numPos &lt;number_of_positive_samples&gt; ：每个分类器阶段用于训练的阳性样本数。 -numNeg &lt;number_of_negative_samples&gt; ：每个分类器阶段用于训练的阴性样本数。 -numStages &lt;number_of_stages&gt; ：要训练的级联级数。 -precalcValBufSize &lt;precalculated_vals_buffer_size_in_Mb&gt;：用于预先计算的特征值的缓冲区大小（以Mb为单位）。您分配的内存越多，培训过程就越快，但是请记住，-precalcValBufSize和的-precalcIdxBufSize总和不应超过您的可用系统内存。 -precalcIdxBufSize &lt;precalculated_idxs_buffer_size_in_Mb&gt;：用于预先计算的特征索引的缓冲区大小（以Mb为单位）。您分配的内存越多，培训过程就越快，但是请记住，-precalcValBufSize和的-precalcIdxBufSize总和不应超过您的可用系统内存。 -baseFormatSave：对于类似Haar的功能，此参数是实际的。如果指定，级联将以旧格式保存。仅出于向后兼容的原因，并且允许用户停留在旧的不赞成使用的界面上，至少可以使用较新的界面训练模型，才可以使用此功能。 -numThreads &lt;max_number_of_threads&gt;：训练期间要使用的最大线程数。请注意，实际使用的线程数可能会更少，具体取决于您的计算机和编译选项。默认情况下，如果您使用TBB支持构建了OpenCV，则将选择最大可用线程，这是此优化所必需的。 -acceptanceRatioBreakValue &lt;break_value&gt;：此参数用于确定模型应保持学习的精确度以及何时停止。良好的指导原则是进行不超过10e-5的训练，以确保模型不会对您的训练数据过度训练。默认情况下，此值设置为-1以禁用此功能。 级联参数： -stageType &lt;BOOST(default)&gt;：阶段类型。目前仅支持提升分类器作为阶段类型。 -featureType&lt;{HAAR(default), LBP}&gt; ：功能类型：HAAR-类似Haar的功能，LBP-本地二进制模式。 -w &lt;sampleWidth&gt;：训练样本的宽度（以像素为单位）。必须具有与训练样本创建期间使用的值完全相同的值（opencv_createsamples实用程序）。 -h &lt;sampleHeight&gt;：训练样本的高度（以像素为单位）。必须具有与训练样本创建期间使用的值完全相同的值（opencv_createsamples实用程序）。 提升分类器参数： -bt &lt;{DAB, RAB, LB, GAB(default)}&gt; ：增强分类器的类型：DAB-离散AdaBoost，RAB-真实AdaBoost，LB-LogitBoost，GAB-温和AdaBoost。 -minHitRate &lt;min_hit_rate&gt;：分类器每个阶段的最低期望命中率。总命中率可以估计为（min_hit_rate ^ number_of_stages），[228] §4.1。 -maxFalseAlarmRate &lt;max_false_alarm_rate&gt;：分类器每个阶段的最大期望误报率。总体误报率可以估计为（max_false_alarm_rate ^ number_of_stages）。 -weightTrimRate &lt;weight_trim_rate&gt;：指定是否应使用修剪及其重量。不错的选择是0.95。 -maxDepth &lt;max_depth_of_weak_tree&gt;：一棵弱树的最大深度。一个不错的选择是1，这是树桩的情况。 -maxWeakCount &lt;max_weak_tree_count&gt;：每个级联阶段的弱树的最大数量。提升分类器（阶段）将具有太多弱树（&lt;= maxWeakCount），这是实现给定所需的-maxFalseAlarmRate。 类似Haar的特征参数： -mode &lt;BASIC (default) | CORE | ALL&gt;：选择训练中使用的Haar功能集的类型。BASIC仅使用直立功能，而ALL使用整套直立和45度旋转功能集。 本地二进制模式参数：本地二进制模式没有参数。opencv_traincascade应用程序完成工作后，经过训练的级联将保存cascade.xml在该-data文件夹中的文件中。此文件夹中的其他文件是为中断培训而创建的，因此您可以在培训完成后将其删除。 训练已完成，您可以测试级联分类器！ 可视化级联分类器有时，可视化受过训练的级联，查看其选择的功能以及其阶段的复杂性可能会很有用。为此，OpenCV提供了一个opencv_visualisation应用程序。该应用程序具有以下命令： --image （必需）：对象模型的参考图像的路径。这应该是带有标注[ -w，-h]的注释，同时传递给opencv_createsamples和opencv_traincascade应用程序。 --model （必需）：训练模型的路径，该路径应该在-dataopencv_traincascade应用程序的参数提供的文件夹中。 --data （可选）：如果提供了数据文件夹（必须事先手动创建），则将存储舞台输出和功能视频。下面是一个示例命令 1opencv_visualisation --image = /data/object.png --model = /data/model.xml --data = /data/result/ 当前可视化工具的一些限制 仅处理由opencv_traincascade工具训练的级联分类器模型，其中包含stumps作为决策树[默认设置]。 提供的图像必须是带有原始模型尺寸的样本窗口，并传递给--image参数。HAAR/LBP人脸模型的示例在Angelina Jolie的给定窗口上运行，该窗口具有与级联分类器文件相同的预处理-&gt; 24x24像素图像，灰度转换和直方图均衡化： 每个阶段都会制作一个视频，以显示每个功能：每个阶段都作为图像存储，以供将来对功能进行验证：]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习]]></title>
    <url>%2Fopencv%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[K最近邻了解k最近邻居目标 在本章中，我们将了解k最近邻（kNN）算法的概念。 理论kNN是可用于监督学习的最简单的分类算法之一。这个想法是在特征空间中搜索测试数据的最匹配。我们将用下面的图片来研究它。在图像中，有两个家族，蓝色正方形和红色三角形。我们称每个家庭为Class。他们的房屋显示在他们的城镇地图中，我们称之为要素空间。（您可以将要素空间视为投影所有数据的空间。例如，考虑一个2D坐标空间。每个数据都有两个要素x和y坐标。您可以在2D坐标空间中表示此数据，对吧？现在假设如果有三个要素，则需要3D空间，现在考虑N个要素，需要N维空间，对吗？这个N维空间就是其要素空间，在我们的图像中，您可以将其视为2D情况有两个功能）。 现在有一个新成员进入城镇并创建了一个新房屋，显示为绿色圆圈。他应该被添加到这些蓝色/红色家族之一中。我们称该过程为分类。我们所做的？由于我们正在处理kNN，因此让我们应用此算法。 一种方法是检查谁是他的最近邻居。从图像中可以明显看出它是红色三角形家族。因此，他也被添加到了Red Triangle中。此方法简称为Nearest Neighbor，因为分类仅取决于最近的邻居。 但这是有问题的。红色三角可能是最近的。但是，如果附近有很多蓝色广场怎么办？然后，蓝色方块在该地区的实力比红色三角更大。因此，仅检查最接近的一个是不够的。相反，我们检查一些k最近的家庭。那么，无论谁占多数，新人都属于那个家庭。在我们的图像中，让我们取k = 3，即3个最近的家庭。他有两个红色和一个蓝色（有两个等距的蓝色，但是由于k = 3，我们只取其中一个），所以他又应该加入红色家族。但是，如果我们取k = 7怎么办？然后，他有5个蓝色家庭和2个红色家庭。大！！现在，他应该被加入Blue家族。因此，所有这些都随k的值而变化。更有趣的是，如果k = 4怎么办？他有2个红色邻居和2个蓝色邻居。这是一条领带！因此最好将k作为奇数。k最近邻，因为分类取决于k最近邻。 同样，在kNN中，我们确实在考虑k个邻居，但我们对所有人都给予同等的重视，对吗？是正义吗？例如，以k ＝ 4的情况为例。我们说这是平局。但是请注意，这两个红色家庭比其他两个蓝色家庭离他更近。因此，他更有资格被添加到Red。那么我们如何用数学解释呢？我们根据每个家庭到新来者的距离来给他们一些权重。对于那些靠近他的人，权重增加，而那些远离他的人，权重减轻。然后，我们分别添加每个家庭的总权重。谁得到的总权重最高，新人就去那个家庭。这称为修改的kNN。 那么您在这里看到的一些重要内容是什么？ 您需要了解镇上所有房屋的信息，对吗？因为，我们必须检查从新移民到所有现有房屋的距离，以找到最近的邻居。如果有很多房屋和家庭，则需要大量的内存，并且需要更多的时间进行计算。几乎没有时间进行任何形式的培训或准备。现在让我们在OpenCV中看到它。 OpenCV中的kNN就像上面一样，我们将在这里做一个简单的例子，有两个家庭（类）。然后在下一章中，我们将做一个更好的例子。 因此，在这里，我们将红色系列标记为Class-0（因此用0表示），将蓝色系列标记为Class-1（用1表示）。我们创建25个家庭或25个训练数据，并将它们标记为0类或1类。我们借助Numpy中的Random Number Generator来完成所有这些工作。 然后我们在Matplotlib的帮助下对其进行绘制。红色系列显示为红色三角形，蓝色系列显示为蓝色正方形。 1234567891011121314import cv2 as cvimport numpy as npimport matplotlib.pyplot as plt# Feature set containing (x,y) values of 25 known/training datatrainData = np.random.randint(0,100,(25,2)).astype(np.float32)# Labels each one either Red or Blue with numbers 0 and 1responses = np.random.randint(0,2,(25,1)).astype(np.float32)# Take Red families and plot themred = trainData[responses.ravel()==0]plt.scatter(red[:,0],red[:,1],80,'r','^')# Take Blue families and plot themblue = trainData[responses.ravel()==1]plt.scatter(blue[:,0],blue[:,1],80,'b','s')plt.show() 您会得到与我们的第一张图片相似的东西。由于您使用的是随机数生成器，因此每次运行代码都将获得不同的数据。 接下来启动kNN算法，并传递trainData和响应以训练kNN（它会构建搜索树）。 然后，我们将在OpenCV中的kNN的帮助下将一个新人带入一个家庭并将其分类。在进入kNN之前，我们需要了解测试数据（新手的数据）上的知识。我们的数据应该是大小为$number of testdata×number of features$,然后我们找到新来者的最近邻居。我们可以指定我们想要多少个邻居。它返回： 给新人的标签取决于我们之前看到的kNN理论。如果要使用“最近邻居”算法，只需指定k = 1即可，其中k是邻居数。 k最近邻居的标签。 从新来者到每个最近邻居的相应距离。因此，让我们看看它是如何工作的。新角标记为绿色。 123456789newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)plt.scatter(newcomer[:,0],newcomer[:,1],80,'g','o')knn = cv.ml.KNearest_create()knn.train(trainData, cv.ml.ROW_SAMPLE, responses)ret, results, neighbours ,dist = knn.findNearest(newcomer, 3)print( "result: &#123;&#125;\n".format(results) )print( "neighbours: &#123;&#125;\n".format(neighbours) )print( "distance: &#123;&#125;\n".format(dist) )plt.show() 我得到的结果如下： 123result: [[ 1.]]neighbours: [[ 1. 1. 1.]]distance: [[ 53. 58. 61.]] 它说我们的新来者有3个邻居，全部来自Blue家族。因此，他被标记为蓝色家庭。从下面的图可以明显看出： 如果您有大量数据，则可以将其作为数组传递。还获得了相应的结果作为数组。 1234# 10 new comersnewcomers = np.random.randint(0,100,(10,2)).astype(np.float32)ret, results,neighbours,dist = knn.findNearest(newcomer, 3)# The results also will contain 10 labels. 使用kNN的OCR手写数据目标在这一章当中 我们将使用有关kNN的知识来构建基本的OCR应用程序。 我们将尝试使用OpenCV随附的数字和字母数据。 手写数字的OCR我们的目标是构建一个可以读取手写数字的应用程序。为此，我们需要一些train_data和test_data。OpenCV带有一个图片digits.png（在文件夹opencv/samples/data/中），其中包含5000个手写数字（每个数字500个）。每个数字都是20x20的图像。因此，我们的第一步是将图像分割成5000个不同的数字。对于每个数字，我们将其展平为400像素的一行。那就是我们的功能集，即所有像素的强度值。这是我们可以创建的最简单的功能集。我们将每个数字的前250个样本用作train_data，然后将250个样本用作test_data。因此，让我们先准备它们。 12345678910111213141516171819202122232425import numpy as npimport cv2 as cvimg = cv.imread('digits.png')gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)# Now we split the image to 5000 cells, each 20x20 sizecells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]# Make it into a Numpy array. It size will be (50,100,20,20)x = np.array(cells)# Now we prepare train_data and test_data.train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)# Create labels for train and test datak = np.arange(10)train_labels = np.repeat(k,250)[:,np.newaxis]test_labels = train_labels.copy()# Initiate kNN, train the data, then test it with test data for k=1knn = cv.ml.KNearest_create()knn.train(train, cv.ml.ROW_SAMPLE, train_labels)ret,result,neighbours,dist = knn.findNearest(test,k=5)# Now we check the accuracy of classification# For that, compare the result with test_labels and check which are wrongmatches = result==test_labelscorrect = np.count_nonzero(matches)accuracy = correct*100.0/result.sizeprint( accuracy ) 因此，我们的基本OCR应用程序已准备就绪。这个特定的例子给我的准确性是91％。一种提高准确性的选择是添加更多数据进行训练，尤其是错误的数据。因此，与其每次启动应用程序时都找不到该培训数据，不如将其保存，以便下次我直接从文件中读取此数据并开始分类。您可以借助一些Numpy函数（例如np.savetxt，np.savez，np.load等）来完成此操作。请查看其文档以获取更多详细信息。 1234567# save the datanp.savez('knn_data.npz',train=train, train_labels=train_labels)# Now load the datawith np.load('knn_data.npz') as data: print( data.files ) train = data['train'] train_labels = data['train_labels'] 在我的系统中，它需要大约4.4 MB的内存。由于我们使用强度值（uint8数据）作为特征，因此最好先将数据转换为np.uint8，然后保存。在这种情况下，仅占用1.1 MB。然后在加载时，您可以转换回float32。 英文字母的OCR接下来，我们将对英语字母执行相同的操作，但是数据和功能集会稍有变化。在这里，OpenCV除了图像以外，还带有一个数据文件，即opencv/samples/cpp/文件夹中的letter-recognitiontion.data。如果打开它，您将看到20000行，乍一看可能看起来像垃圾。实际上，在每一行中，第一列是一个字母，这是我们的标签。接下来的16个数字是它的不同功能。这些功能可从UCI机器学习存储库获得。您可以在此页面中找到这些功能的详细信息。 现有20000个样本，因此我们将前10000个数据作为训练样本，其余10000个作为测试样本。我们应该将字母更改为ASCII字符，因为我们不能直接使用字母。 1234567891011121314151617import cv2 as cvimport numpy as np# Load the data, converters convert the letter to a numberdata= np.loadtxt('letter-recognition.data', dtype= 'float32', delimiter = ',', converters= &#123;0: lambda ch: ord(ch)-ord('A')&#125;)# split the data to two, 10000 each for train and testtrain, test = np.vsplit(data,2)# split trainData and testData to features and responsesresponses, trainData = np.hsplit(train,[1])labels, testData = np.hsplit(test,[1])# Initiate the kNN, classify, measure accuracy.knn = cv.ml.KNearest_create()knn.train(trainData, cv.ml.ROW_SAMPLE, responses)ret, result, neighbours, dist = knn.findNearest(testData, k=5)correct = np.count_nonzero(result == labels)accuracy = correct*100.0/10000print( accuracy ) 它给我的准确性为93.22％。同样，如果要提高准确性，则可以迭代地在每个级别中添加错误数据。 支持向量机（SVM）了解SVM目标在这一章当中 我们将看到对SVM的直观了解 理论线性可分离数据考虑下面的图像，它具有两种数据类型，红色和蓝色。在kNN中，对于测试数据，我们用来测量其与所有训练样本的距离，并以最小的距离作为样本。测量所有距离都需要花费大量时间，并且需要大量内存来存储所有训练样本。但是考虑到图像中给出的数据，我们是否需要那么多？ 您可以看到很多这样的行都是可能的。我们会选哪一个？非常直观地，我们可以说直线应该从所有点尽可能远地经过。为什么？因为传入的数据中可能会有噪音。此数据不应影响分类准确性。因此，走最远的线将提供更大的抗干扰能力。因此，SVM要做的是找到到训练样本的最小距离最大的直线（或超平面）。请参阅下面图像中穿过中心的粗线。 因此，要找到此决策边界，您需要训练数据。需要全部吗？没有。仅接近相反组的那些就足够了。在我们的图像中，它们是一个蓝色填充的圆圈和两个红色填充的正方形。我们可以称它们为支持向量，而通过它们的线称为支持平面。它们足以找到我们的决策边界。我们不必担心所有数据。它有助于减少数据量。 非线性可分离数据可以使用二维空间中的平方点积来实现三维空间中的点积。这可以应用于更高维度的空间。因此，我们可以从较低尺寸本身计算较高尺寸的特征。一旦将它们映射，我们将获得更高的空间。 除了所有这些概念之外，还存在分类错误的问题。因此，仅找到具有最大余量的决策边界是不够的。我们还需要考虑分类错误的问题。有时，可能会找到裕度较小但分类错误减少的决策边界。无论如何，我们需要修改我们的模型，以便它应该找到具有最大裕度但分类错误较少的决策边界。 如何选择参数C？显然，这个问题的答案取决于训练数据的分布方式。尽管没有一般性的答案，但考虑以下规则会很有用： C的值越大，解决方案的分类错误越少，但裕度也越小。考虑到在这种情况下，进行错误分类错误是昂贵的。由于优化的目的是使参数最小化，因此几乎没有错误分类错误。 C的值越小，解决方案的裕度就越大，分类误差也越大。在这种情况下，最小化对总和项的考虑不多，因此它更多地关注于找到具有大余量的超平面。 使用SVM的手写数据的OCR目标在这一章当中 我们将重新访问手写数据OCR，但是使用SVM而不是kNN。 手写数字的OCR 在kNN中，我们直接使用像素强度作为特征向量。这次我们将使用定向梯度直方图（HOG）作为特征向量。 在这里，在找到HOG之前，我们使用其二阶矩对图像进行去偏斜。因此，我们首先定义一个函数deskew()，该函数获取一个数字图像并将其校正。下面是deskew()函数： 12345678def deskew(img): m = cv.moments(img) if abs(m['mu02']) &lt; 1e-2: return img.copy() skew = m['mu11']/m['mu02'] M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]]) img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags) return img 下图显示了应用于零图像的上偏移校正功能。左图像是原始图像，右图像是去歪斜图像。 接下来，我们必须找到每个单元格的HOG描述符。为此，我们找到了每个单元在X和Y方向上的Sobel导数。然后在每个像素处找到它们的大小和梯度方向。该梯度被量化为16个整数值。将此图像划分为四个子正方形。对于每个子正方形，计算权重大小方向的直方图（16个bin）。因此，每个子正方形为您提供了一个包含16个值的向量。（四个子正方形的）四个这样的向量共同为我们提供了一个包含64个值的特征向量。这是我们用于训练数据的特征向量。 12345678910def hog(img): gx = cv.Sobel(img, cv.CV_32F, 1, 0) gy = cv.Sobel(img, cv.CV_32F, 0, 1) mag, ang = cv.cartToPolar(gx, gy) bins = np.int32(bin_n*ang/(2*np.pi)) # quantizing binvalues in (0...16) bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:] mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:] hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)] hist = np.hstack(hists) # hist is a 64 bit vector return hist 最后，与前面的情况一样，我们首先将大数据集拆分为单个单元格。对于每个数字，保留250个单元用于训练数据，其余250个数据保留用于测试。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!/usr/bin/env pythonimport cv2 as cvimport numpy as npSZ=20bin_n = 16 # Number of binsaffine_flags = cv.WARP_INVERSE_MAP|cv.INTER_LINEARdef deskew(img): m = cv.moments(img) if abs(m['mu02']) &lt; 1e-2: return img.copy() skew = m['mu11']/m['mu02'] M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]]) img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags) return imgdef hog(img): gx = cv.Sobel(img, cv.CV_32F, 1, 0) gy = cv.Sobel(img, cv.CV_32F, 0, 1) mag, ang = cv.cartToPolar(gx, gy) bins = np.int32(bin_n*ang/(2*np.pi)) # quantizing binvalues in (0...16) bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:] mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:] hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)] hist = np.hstack(hists) # hist is a 64 bit vector return histimg = cv.imread('digits.png',0)if img is None: raise Exception("we need the digits.png image from samples/data here !")cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]# First half is trainData, remaining is testDatatrain_cells = [ i[:50] for i in cells ]test_cells = [ i[50:] for i in cells]deskewed = [list(map(deskew,row)) for row in train_cells]hogdata = [list(map(hog,row)) for row in deskewed]trainData = np.float32(hogdata).reshape(-1,64)responses = np.repeat(np.arange(10),250)[:,np.newaxis]svm = cv.ml.SVM_create()svm.setKernel(cv.ml.SVM_LINEAR)svm.setType(cv.ml.SVM_C_SVC)svm.setC(2.67)svm.setGamma(5.383)svm.train(trainData, cv.ml.ROW_SAMPLE, responses)svm.save('svm_data.dat')deskewed = [list(map(deskew,row)) for row in test_cells]hogdata = [list(map(hog,row)) for row in deskewed]testData = np.float32(hogdata).reshape(-1,bin_n*4)result = svm.predict(testData)[1]mask = result==responsescorrect = np.count_nonzero(mask)print(correct*100.0/result.size) 这种特殊的技术给了我近94％的准确性。您可以为SVM的各种参数尝试不同的值，以检查是否可以实现更高的精度。或者，您可以阅读有关此领域的技术论文并尝试实施它们。 K均值聚类了解K均值聚类目标 在本章中，我们将了解K-Means聚类的概念，其工作原理等。 理论我们将用一个常用的例子来处理这个问题。 T恤尺寸问题考虑一家公司，该公司将向市场发布新型号的T恤。显然，他们将不得不制造不同尺寸的模型，以满足各种规模的人们的需求。因此，该公司会记录人们的身高和体重数据，并将其绘制到图形上，如下所示： 公司无法制作所有尺寸的T恤。取而代之的是，他们将人划分为小，中和大，并仅制造这三种适合所有人的模型。可以通过k均值聚类将人员分为三组，并且算法可以为我们提供最佳的3种尺寸，这将满足所有人员的需求。如果不是这样，公司可以将人员分为更多的组，可能是五个，依此类推。查看下面的图片： 它是如何工作的 ？该算法是一个迭代过程。我们将在图像的帮助下逐步解释它。 考虑如下数据（您可以将其视为T恤问题）。我们需要将此数据分为两类。 步骤1-算法随机选择两个质心$C1$个和$C2$ （有时，将任何两个数据作为质心）。 步骤2-计算每个点到两个质心的距离。如果测试数据更接近$C1$个，然后将该数据标记为“ 0”。如果更接近$C2$，然后标记为“ 1”（如果存在更多质心，则标记为“ 2”，“ 3”等）。 在我们的示例中，我们将为所有标记为红色的“ 0”和标记为蓝色的所有“ 1”着色。因此，经过以上操作，我们得到以下图像。步骤3-接下来，我们分别计算所有蓝点和红点的平均值，这将成为我们的新质心。那是$C1$个和$C2$转移到新计算的质心。（请记住，显示的图像不是真实值，也不是真实比例，仅用于演示）。 再次，使用新的质心执行步骤2，并将标签数据设置为’0’和’1’。 因此我们得到如下结果：现在，迭代步骤2和步骤3，直到两个质心都收敛到固定点 最终结果几乎如下所示： OpenCV中的K-均值聚类目标 了解如何在OpenCV中使用cv.kmeans()函数进行数据聚类 了解参数输入参数 sample：它应该是np.float32数据类型，并且每个功能都应该放在单个列中。 nclusters(K)：结束时所需的集群数 criteria：这是迭代终止准则。满足此条件后，算法迭代将停止。实际上，它应该是3个参数的元组。它们是（type，max_iter，epsilon）：a.终止条件的类型。它具有3个标志，如下所示：1.cv.TERM_CRITERIA_EPS-如果达到指定的精度epsilon，则停止算法迭代。2.cv.TERM_CRITERIA_MAX_ITER-在指定的迭代次数max_iter之后停止算法。3.cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER-满足以上任何条件时，停止迭代。b.max_iter-一个整数，指定最大迭代次数。c.epsilon-要求的精度 attempts：标志来指定的算法是使用不同的初始labellings执行次数。该算法返回产生最佳紧密度的标签。该紧凑性作为输出返回。 flags：此标志用于指定初始中心的获取方式。通常，为此使用两个标志：cv.KMEANS_PP_CENTERS和cv.KMEANS_RANDOM_CENTERS。 输出参数 compactness：是每个点到其对应中心的平方距离的总和。 labels：这是标签数组（与上一篇文章中的“代码”相同），其中每个元素标记为“ 0”，“ 1” ….. centers：这是群集中心的阵列。现在，我们将通过三个示例了解如何应用K-Means算法。 1.仅具有一维的数据考虑一下，您有一组仅具有一个特征（即一维）的数据。例如，我们可以解决我们的T恤问题，您只用身高来决定T恤的尺寸。 因此，我们首先创建数据并将其绘制在Matplotlib 123456789import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltx = np.random.randint(25,100,25)y = np.random.randint(175,255,25)z = np.hstack((x,y))z = z.reshape((50,1))z = np.float32(z)plt.hist(z,256,[0,256]),plt.show() 因此，我们有一个“ z”，它是一个大小为50的数组，值的范围是0到255。我已经将“ z”重塑为列向量。如果存在多个功能，它将更加有用。然后我制作了np.float32类型的数据。 我们得到以下图像：现在我们应用KMeans函数。在此之前，我们需要指定标准。我的标准是，每当运行10次算法迭代或达到epsilon = 1.0的精度时，就停止算法并返回答案。 123456# Define criteria = ( type, max_iter = 10 , epsilon = 1.0 )criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)# Set flags (Just to avoid line break in the code)flags = cv.KMEANS_RANDOM_CENTERS# Apply KMeanscompactness,labels,centers = cv.kmeans(z,2,None,criteria,10,flags) 这为我们提供了紧凑性，标签和中心。在这种情况下，我得到的中心分别为60和207。标签的大小将与测试数据的大小相同，其中每个数据的质心都将标记为“ 0”，“ 1”，“ 2”等。现在，我们根据标签将数据分为不同的群集。 12A = z[labels==0]B = z[labels==1] 现在我们以红色绘制A，以蓝色绘制B，以黄色绘制其质心。 12345# Now plot 'A' in red, 'B' in blue, 'centers' in yellowplt.hist(A,256,[0,256],color = 'r')plt.hist(B,256,[0,256],color = 'b')plt.hist(centers,32,[0,256],color = 'y')plt.show() 以下是我们得到的输出： 2.具有多种功能的数据在前面的示例中，我们仅考虑了T恤问题的身高。在这里，我们将同时考虑身高和体重，即两个特征。 请记住，在以前的情况下，我们将数据制作为单个列向量。每个特征排列在一列中，而每一行对应于一个输入测试样本。 例如，在这种情况下，我们设置了一个大小为50x2的测试数据，即50人的身高和体重。第一列对应于全部50个人的身高，第二列对应于他们的体重。第一行包含两个元素，其中第一个是第一人称的身高，第二个是他的体重。类似地，剩余的行对应于其他人的身高和体重。查看下面的图片：现在，我直接转到代码： 1234567891011121314151617181920import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltX = np.random.randint(25,50,(25,2))Y = np.random.randint(60,85,(25,2))Z = np.vstack((X,Y))# convert to np.float32Z = np.float32(Z)# define criteria and apply kmeans()criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)ret,label,center=cv.kmeans(Z,2,None,criteria,10,cv.KMEANS_RANDOM_CENTERS)# Now separate the data, Note the flatten()A = Z[label.ravel()==0]B = Z[label.ravel()==1]# Plot the dataplt.scatter(A[:,0],A[:,1])plt.scatter(B[:,0],B[:,1],c = 'r')plt.scatter(center[:,0],center[:,1],s = 80,c = 'y', marker = 's')plt.xlabel('Height'),plt.ylabel('Weight')plt.show() 下面是我们得到的输出： 3.颜色量化颜色量化是减少图像中颜色数量的过程。这样做的原因之一是减少内存。有时，某些设备可能会受到限制，因此只能产生有限数量的颜色。同样在那些情况下，执行颜色量化。在这里，我们使用k均值聚类进行颜色量化。 这里没有新内容要解释。有3个功能，例如R，G，B。因此，我们需要将图像重塑为Mx3大小的数组（M是图像中的像素数）。在聚类之后，我们将质心值（也是R，G，B）应用于所有像素，这样生成的图像将具有指定的颜色数。再一次，我们需要将其重塑为原始图像的形状。下面是代码： 1234567891011121314151617import numpy as npimport cv2 as cvimg = cv.imread('home.jpg')Z = img.reshape((-1,3))# convert to np.float32Z = np.float32(Z)# define criteria, number of clusters(K) and apply kmeans()criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)K = 8ret,label,center=cv.kmeans(Z,K,None,criteria,10,cv.KMEANS_RANDOM_CENTERS)# Now convert back into uint8, and make original imagecenter = np.uint8(center)res = center[label.flatten()]res2 = res.reshape((img.shape))cv.imshow('res2',res2)cv.waitKey(0)cv.destroyAllWindows() K = 8，请参见以下结果：]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算摄影]]></title>
    <url>%2Fopencv%2F%E8%AE%A1%E7%AE%97%E6%91%84%E5%BD%B1.html</url>
    <content type="text"><![CDATA[图像去噪目标 您将了解有关去除图像中噪声的非局部均值去噪算法。 您将看到不同的函数，例如cv.fastNlMeansDenoising()，cv.fastNlMeansDenoisingColored()等。 理论在前面的章节中，我们看到了许多图像平滑技术，例如高斯模糊，中值模糊等，它们在某种程度上可以消除少量噪声。在这些技术中，我们在像素周围采取了一个较小的邻域，并进行了一些操作（例如高斯加权平均值，值的中位数等）来替换中心元素。简而言之，在像素处去除噪声是其周围的局部现象。 有噪音的性质。通常认为噪声是零均值的随机变量。考虑一个有噪声的像素，，其中是像素的真实值，是该像素中的噪声。您可以从不同的图像中获取大量相同的像素（例如）并计算其平均值。 您可以通过简单的设置自己进行验证。将静态相机固定在某个位置几秒钟。这将为您提供很多帧或同一场景的很多图像。然后编写一段代码，找到视频中所有帧的平均值（这对您现在应该太简单了）。比较最终结果和第一帧。您会看到噪音降低。不幸的是，这种简单的方法对摄像机和场景的运动并不稳健。通常，只有一张嘈杂的图像可用。 因此想法很简单，我们需要一组相似的图像来平均噪声。考虑图像中的一个小窗口（例如5x5窗口）。很有可能同一修补程序可能位于图像中的其他位置。有时在它周围的一个小社区中。一起使用这些相似的补丁并找到它们的平均值怎么办？对于那个特定的窗口，这很好。请参阅下面的示例图片： 图像中的蓝色补丁看起来很相似。绿色补丁看起来很相似。因此，我们获取一个像素，在其周围获取一个小窗口，在图像中搜索相似的窗口，对所有窗口求平均，然后用得到的结果替换该像素。此方法是“非本地均值消噪”。与我们之前看到的模糊技术相比，它花费了更多时间，但是效果非常好。在更多资源的第一个链接中可以找到更多详细信息和在线演示。 对于彩色图像，图像将转换为CIELAB色彩空间，然后分别对L和AB分量进行降噪。 OpenCV中的图像去噪OpenCV提供了此技术的四个变体。 cv.fastNlMeansDenoising() -使用单个灰度图像 cv.fastNlMeansDenoisingColored() -使用彩色图像。 cv.fastNlMeansDenoisingMulti() -处理在短时间内捕获的图像序列（灰度图像） cv.fastNlMeansDenoisingColoredMulti() -与上面相同，但用于彩色图像。常见的参数有： h：决定滤波器强度的参数。较高的h值可以更好地消除噪点，但同时也会消除图像细节。（10可以） hForColorComponents：与h相同，但仅适用于彩色图像。（通常与h相同） templateWindowSize：应为奇数。（推荐7） searchWindowSize：应该为奇数。（建议21）请访问其他资源中的第一个链接，以获取有关这些参数的更多详细信息。 我们将在此处演示2和3。剩下的留给您。 1. cv.fastNlMeansDenoisingColored()如上所述，它用于消除彩色图像中的噪点。（噪声可能是高斯的）。请参阅以下示例： 12345678import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('die.png')dst = cv.fastNlMeansDenoisingColored(img,None,10,10,7,21)plt.subplot(121),plt.imshow(img)plt.subplot(122),plt.imshow(dst)plt.show() 以下是结果的放大版本。我的输入图像具有$σ$的高斯噪声= 25 2. cv.fastNlMeansDenoisingMulti()现在，我们将对视频应用相同的方法。第一个参数是噪声帧列表。第二个参数imgToDenoiseIndex指定我们需要去噪的帧，为此我们在输入列表中传递帧的索引。第三是temporalWindowSize，它指定要用于降噪的附近帧的数量。应该很奇怪。在那种情况下，总共使用temporalWindowSize帧，其中中心帧是要去噪的帧。例如，您传递了一个5帧的列表作为输入。令imgToDenoiseIndex = 2，temporalWindowSize =3。然后使用frame-1，frame-2和frame-3去噪frame-2。让我们来看一个例子。 12345678910111213141516171819202122import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltcap = cv.VideoCapture('vtest.avi')# create a list of first 5 framesimg = [cap.read()[1] for i in xrange(5)]# convert all to grayscalegray = [cv.cvtColor(i, cv.COLOR_BGR2GRAY) for i in img]# convert all to float64gray = [np.float64(i) for i in gray]# create a noise of variance 25noise = np.random.randn(*gray[1].shape)*10# Add this noise to imagesnoisy = [i+noise for i in gray]# Convert back to uint8noisy = [np.uint8(np.clip(i,0,255)) for i in noisy]# Denoise 3rd frame considering all the 5 framesdst = cv.fastNlMeansDenoisingMulti(noisy, 2, 5, None, 4, 7, 35)plt.subplot(131),plt.imshow(gray[2],'gray')plt.subplot(132),plt.imshow(noisy[2],'gray')plt.subplot(133),plt.imshow(dst,'gray')plt.show() 下图显示了我们得到的结果的放大版本： 计算需要花费大量时间。结果，第一个图像是原始帧，第二个是噪点图像，第三个是去噪图像。 其他资源http://www.ipol.im/pub/art/2011/bcm_nlm/（包含详细信息，在线演示等。强烈建议访问。我们的测试图像是从此链接生成的） 图像修复目标在这一章当中， 我们将学习如何通过称为“修复”的方法消除旧照片中的小噪音，笔触等。 我们将在OpenCV中看到修复功能。 基础你们大多数人家里都会有一些旧的退化照片，上面有黑点，一些笔触等。您是否曾经想过将其还原？我们不能简单地在绘画工具中擦除它们，因为它将简单地用白色结构代替黑色结构，这是没有用的。在这些情况下，将使用一种称为图像修复的技术。基本思想很简单：用附近的像素替换那些不良标记，使其看起来像附近。考虑下面显示的图像（摘自Wikipedia）： 为此目的设计了几种算法，OpenCV提供了其中两种。两者都可以通过相同的函数cv.inpaint()访问 第一种算法基于Alexandru Telea在2004年发表的论文“基于快速行进方法的图像修复技术” 。它基于快速行进方法。考虑图像中要修复的区域。算法从该区域的边界开始，并进入该区域内部，然后逐渐填充边界中的所有内容。在要修复的邻域上的像素周围需要一个小的邻域。该像素被附近所有已知像素的归一化加权总和所代替。权重的选择很重要。那些位于该点附近，边界法线附近的像素和那些位于边界轮廓线上的像素将获得更大的权重。修复像素后，将使用快速行进方法将其移动到下一个最近的像素。FMM确保首先修复已知像素附近的那些像素，以便像手动启发式操作一样工作。通过使用标志启用此算法，cv.INPAINT_TELEA。 第二种算法基于Bertalmio，Marcelo，Andrea L.Bertozzi和Guillermo Sapiro在2001年发表的论文“ Navier-Stokes，流体动力学以及图像和视频修补”。该算法基于流体动力学并利用了偏微分方程。基本原理是启发式的。它首先沿着边缘从已知区域移动到未知区域（因为边缘是连续的）。它延续了等渗线（线条连接具有相同强度的点，就像轮廓线连接具有相同高程的点一样），同时在修复区域的边界匹配梯度矢量。为此，使用了一些流体动力学方法。获得它们后，将填充颜色以减少该区域的最小差异。通过使用标志cv.INPAINT_NS启用此算法。 示例代码我们需要创建一个与输入图像大小相同的蒙版，其中非零像素对应于要修复的区域。其他一切都很简单。我的图像因一些黑色笔画而退化（我手动添加了）。我使用“绘画”工具创建了相应的笔触。 12345678import numpy as npimport cv2 as cvimg = cv.imread('messi_2.jpg')mask = cv.imread('mask2.png',0)dst = cv.inpaint(img,mask,3,cv.INPAINT_TELEA)cv.imshow('dst',dst)cv.waitKey(0)cv.destroyAllWindows() 请参阅下面的结果。第一张图片显示了降级的输入。第二个图像是蒙版。第三个图像是第一个算法的结果，最后一个图像是第二个算法的结果。 高动态范围（HDR）目标 了解如何根据曝光顺序生成和显示HDR图像。 使用曝光融合来合并曝光序列。 理论高动态范围成像（HDRI或HDR）是一种用于成像和摄影的技术，可比标准数字成像或摄影技术重现更大的动态光度范围。虽然人眼可以适应各种光照条件，但是大多数成像设备每个通道使用8位，因此我们仅限于256级。当我们拍摄现实世界的照片时，明亮的区域可能会曝光过度，而黑暗的区域可能会曝光不足，因此我们无法一次曝光就捕获所有细节。HDR成像适用于每个通道使用8位以上（通常为32位浮点值）的图像，从而允许更大的动态范围。 获取HDR图像的方法有多种，但是最常见的一种方法是使用以不同曝光值拍摄的场景照片。要组合这些曝光，了解相机的响应功能并有一些算法可以对其进行估计是很有用的。合并HDR图像后，必须将其转换回8位才能在常规显示器上查看。此过程称为音调映射。当场景或相机的对象在两次拍摄之间移动时，还会增加其他复杂性，因为应记录并调整具有不同曝光度的图像。 在本教程中，我们展示了两种算法（Debevec，Robertson）来根据曝光序列生成和显示HDR图像，并演示了另一种称为曝光融合（Mertens）的方法，该方法可以生成低动态范围图像，并且不需要曝光时间数据。此外，我们估计相机响应函数（CRF）对于许多计算机视觉算法都具有重要价值。HDR流水线的每个步骤都可以使用不同的算法和参数来实现，因此请查看参考手册以了解所有内容。 曝光顺序HDR在本教程中，我们将查看以下场景，其中有4张曝光图像，曝光时间分别为15、2.5、1 / 4和1/30秒。（您可以从Wikipedia下载图像） 1.将曝光图像加载到列表中第一步只是将所有图像加载到列表中。此外，我们将需要常规HDR算法的曝光时间。请注意数据类型，因为图像应为1通道或3通道8位（np.uint8），并且曝光时间必须为float32，以秒为单位。 123456import cv2 as cvimport numpy as np# Loading exposure images into a listimg_fn = ["img0.jpg", "img1.jpg", "img2.jpg", "img3.jpg"]img_list = [cv.imread(fn) for fn in img_fn]exposure_times = np.array([15.0, 2.5, 0.25, 0.0333], dtype=np.float32) 2.将曝光合并到HDR图像中在此阶段，我们将曝光序列合并为一张HDR图像，显示了OpenCV中的两种可能性。第一种方法是Debevec，第二种方法是Robertson。请注意，HDR图像的类型为float32，而不是uint8，因为它包含所有曝光图像的完整动态范围。 12345# Merge exposures to HDR imagemerge_debevec = cv.createMergeDebevec()hdr_debevec = merge_debevec.process(img_list, times=exposure_times.copy())merge_robertson = cv.createMergeRobertson()hdr_robertson = merge_robertson.process(img_list, times=exposure_times.copy()) 3.色调图HDR图像我们将32位浮点HDR数据映射到[0..1]范围内。实际上，在某些情况下，该值可以大于1或小于0，因此请注意，我们稍后将不得不裁剪数据以避免溢出。 123# Tonemap HDR imagetonemap1 = cv.createTonemap(gamma=2.2)res_debevec = tonemap1.process(hdr_debevec.copy()) 4.使用Mertens融合合并曝光在这里，我们展示了一种替代算法，用于合并曝光图像，而我们不需要曝光时间。我们也不需要使用任何色调映射算法，因为Mertens算法已经为我们提供了[0..1]范围内的结果。 123# Exposure fusion using Mertensmerge_mertens = cv.createMergeMertens()res_mertens = merge_mertens.process(img_list) 5.转换为8位并保存为了保存或显示结果，我们需要将数据转换为[0..255]范围内的8位整数。 1234567# Convert datatype to 8-bit and saveres_debevec_8bit = np.clip(res_debevec*255, 0, 255).astype('uint8')res_robertson_8bit = np.clip(res_robertson*255, 0, 255).astype('uint8')res_mertens_8bit = np.clip(res_mertens*255, 0, 255).astype('uint8')cv.imwrite("ldr_debevec.jpg", res_debevec_8bit)cv.imwrite("ldr_robertson.jpg", res_robertson_8bit)cv.imwrite("fusion_mertens.jpg", res_mertens_8bit) 结果您可以看到不同的结果，但可以认为每种算法都有其他额外的参数，您应该将它们附加以达到期望的结果。最佳实践是尝试不同的方法，然后看看哪种方法最适合您的场景。 Debevec: Robertson: Mertenes Fusion: 估计相机响应功能摄像机响应功能（CRF）使我们可以将场景辐射度与测得的强度值联系起来。CRF在某些计算机视觉算法（包括HDR算法）中非常重要。在这里，我们估计逆相机响应函数并将其用于HDR合并。 1234567# Estimate camera response function (CRF)cal_debevec = cv.createCalibrateDebevec()crf_debevec = cal_debevec.process(img_list, times=exposure_times)hdr_debevec = merge_debevec.process(img_list, times=exposure_times.copy(), response=crf_debevec.copy())cal_robertson = cv.createCalibrateRobertson()crf_robertson = cal_robertson.process(img_list, times=exposure_times)hdr_robertson = merge_robertson.process(img_list, times=exposure_times.copy(), response=crf_robertson.copy()) 相机响应功能由每个颜色通道的256长度向量表示。对于此序列，我们得到以下估计：]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[相机校准和3D重建]]></title>
    <url>%2Fopencv%2F%E7%9B%B8%E6%9C%BA%E6%A0%A1%E5%87%86%E5%92%8C3D%E9%87%8D%E5%BB%BA.html</url>
    <content type="text"><![CDATA[相机校准目标在本节中，我们将学习 相机造成的失真类型 如何找到相机的内在和外在特性 如何根据这些属性使图像不失真 基础一些针孔相机会给图像带来明显的失真。两种主要的变形是径向变形和切向变形。径向变形会导致直线出现弯曲。距图像中心越远，径向畸变越大。例如，下面显示一个图像，其中棋盘的两个边缘用红线标记。但是，您会看到国际象棋棋盘的边框不是直线，并且与红线不匹配。所有预期的直线都凸出。有关更多详细信息，请访问“ 失真（光学） ”。 同样，由于摄像镜头未完全平行于成像平面对齐，因此会发生切向畸变。因此，图像中的某些区域看起来可能比预期的要近。 简而言之，我们需要找到五个参数，称为失真系数，公式如下：$$Distortioncoefficients=(k1 k2 p1 p2 k3)$$除此之外，我们还需要其他一些信息，例如摄像机的内在和外在参数。内部参数特定于摄像机。它们包括像焦距和光学中心,焦距光学中心可用于创建相机矩阵，该相机矩阵可用于消除由于特定相机镜头而引起的畸变。相机矩阵对于特定相机而言是唯一的，因此一旦计算出，就可以在同一相机拍摄的其他图像上重复使用。它表示为3x3矩阵：$$\quadcamera matrix = \begin{bmatrix} f_x &amp; 0 &amp; c_x \ 0 &amp; f_y &amp; c_y \ 0 &amp; 0 &amp; 1\end{bmatrix}\quad$$外在参数对应于旋转和平移矢量，其将3D点的坐标平移为坐标系。 对于立体声应用，首先需要纠正这些失真。为了找到这些参数，我们必须提供一些定义良好的图案的示例图像（例如国际象棋棋盘）。我们找到一些已经知道相对位置的特定点（例如，国际象棋棋盘中的四角）。我们知道现实世界中这些点的坐标，也知道图像中的坐标，因此我们可以求解失真系数。为了获得更好的结果，我们至少需要10个测试模式。 示例代码如上所述，相机校准至少需要10个测试图案。OpenCV附带了一些国际象棋棋盘的图像（请参见samples/data/left01.jpg –left14.jpg），因此我们将利用这些图像。考虑棋盘的图像。相机校准所需的重要输入数据是3D现实世界点集以及图像中这些点的相应2D坐标。可以从图像中轻松找到2D图像点。（这些图像点是棋盘上两个黑色正方形相互接触的位置） 现实世界中的3D点呢？这些图像是从静态相机拍摄的，而国际象棋棋盘放置在不同的位置和方向。所以我们需要知道（X，Y，Z）价值观。但是为简单起见，我们可以说棋盘在XY平面上保持静止（因此Z始终为0），并且照相机也相应地移动了。这种考虑有助于我们仅找到X，Y值。现在对于X，Y值，我们可以简单地将点传递为（0,0），（1,0），（2,0），…，这表示点的位置。在这种情况下，我们得到的结果将是棋盘正方形的大小比例。但是，如果我们知道正方形大小（例如30毫米），则可以将值传递为（0,0），（30,0），（60,0），…。因此，我们得到的结果以毫米为单位。（在这种情况下，我们不知道正方形尺寸，因为我们没有拍摄那些图像，因此我们以正方形尺寸表示）。 3D点称为对象点，而2D图像点称为图像点。 设定因此，要在国际象棋棋盘中查找图案，我们可以使用函数cv.findChessboardCorners()。我们还需要传递所需的图案，例如8x8网格，5x5网格等。在此示例中，我们使用7x6网格。（通常，棋盘有8x8的正方形和7x7的内部角）。它返回角点和retval，如果获得图案，则为True。这些角将按顺序放置（从左到右，从上到下） 也可以看看此功能可能无法在所有图像中找到所需的图案。因此，一个不错的选择是编写代码，使它启动相机并检查每帧所需的图案。获得图案后，找到角并将其存储在列表中。另外，在阅读下一帧之前请提供一些时间间隔，以便我们可以在不同方向上调整棋盘。继续此过程，直到获得所需数量的良好图案为止。即使在此处提供的示例中，我们也不确定给出的14张图像中有多少张是好的。因此，我们必须阅读所有图像并仅拍摄好图像。除了棋盘，我们还可以使用圆形网格。在这种情况下，我们必须使用函数cv.findCirclesGrid()来找到模式。较少的图像足以使用圆形网格执行相机校准。找到角点后，可以使用cv.cornerSubPix()来提高其精度。我们还可以使用cv.drawChessboardCorners()绘制图案。所有这些步骤都包含在以下代码中： 123456789101112131415161718192021222324252627import numpy as npimport cv2 as cvimport glob# termination criteriacriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)objp = np.zeros((6*7,3), np.float32)objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)# Arrays to store object points and image points from all the images.objpoints = [] # 3d point in real world spaceimgpoints = [] # 2d points in image plane.images = glob.glob('*.jpg')for fname in images: img = cv.imread(fname) gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # Find the chess board corners ret, corners = cv.findChessboardCorners(gray, (7,6), None) # If found, add object points, image points (after refining them) if ret == True: objpoints.append(objp) corners2 = cv.cornerSubPix(gray,corners, (11,11), (-1,-1), criteria) imgpoints.append(corners) # Draw and display the corners cv.drawChessboardCorners(img, (7,6), corners2, ret) cv.imshow('img', img) cv.waitKey(500)cv.destroyAllWindows() 一张上面画有图案的图像如下所示： 校准现在我们有了目标点和图像点，现在可以进行校准了。我们可以使用函数cv.calibrateCamera()返回相机矩阵，失真系数，旋转和平移矢量等。 1ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None) 不失真现在，我们可以拍摄图像并对其进行扭曲。OpenCV提供了两种方法来执行此操作。但是，首先，我们可以使用cv.getOptimalNewCameraMatrix()基于自由缩放参数来优化相机矩阵。如果缩放参数alpha = 0，则返回具有最少不需要像素的未失真图像。因此，它甚至可能会删除图像角落的一些像素。如果alpha = 1，则所有像素都保留有一些额外的黑色图像。此函数还返回可用于裁剪结果的图像ROI。 因此，我们拍摄一张新图像（在本例中为left12.jpg。这是本章的第一张图像） 123img = cv.imread('left12.jpg')h, w = img.shape[:2]newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h)) 1.使用cv.undistort()这是最简单的方法。只需调用该函数并使用上面获得的ROI裁剪结果即可。 123456# undistortdst = cv.undistort(img, mtx, dist, None, newcameramtx)# crop the imagex, y, w, h = roidst = dst[y:y+h, x:x+w]cv.imwrite('calibresult.png', dst) 2.使用重映射这样比较困难。首先，找到从扭曲图像到未扭曲图像的映射函数。然后使用重映射功能。 1234567# undistortmapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w,h), 5)dst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)# crop the imagex, y, w, h = roidst = dst[y:y+h, x:x+w]cv.imwrite('calibresult.png', dst) 尽管如此，两种方法都给出相同的结果。看到下面的结果： 您可以看到所有边缘都是笔直的。 现在，您可以使用NumPy中的写入功能（np.savez，np.savetxt等）存储相机矩阵和失真系数，以备将来使用。 重投影误差重投影误差可以很好地估计找到的参数的精确度。重投影误差越接近零，我们发现的参数越准确。给定固有，失真，旋转和平移矩阵，我们必须首先使用cv.projectPoints()将对象点转换为图像点。然后，我们可以计算出通过变换得到的绝对值和拐角发现算法之间的绝对范数。为了找到平均误差，我们计算为所有校准图像计算的误差的算术平均值。 123456mean_error = 0for i in xrange(len(objpoints)): imgpoints2, _ = cv.projectPoints(objpoints[i], rvecs[i], tvecs[i], mtx, dist) error = cv.norm(imgpoints[i], imgpoints2, cv.NORM_L2)/len(imgpoints2) mean_error += errorprint( "total error: &#123;&#125;".format(mean_error/len(objpoints)) ) 姿势估计目标在这个部分， 我们将学习利用calib3d模块在图像中创建一些3D效果。 基本这将是一小部分。在上一次相机校准的过程中，您发现了相机矩阵，失真系数等。给定图案图像，我们可以利用以上信息来计算其姿势或物体在空间中的位置，例如其旋转方式，对于平面物体，我们可以假设Z = 0，这样，问题就变成了如何将相机放置在空间中以查看我们的图案图像。因此，如果我们知道对象在空间中的位置，则可以在其中绘制一些2D图以模拟3D效果。让我们来看看如何做。 我们的问题是，我们想在棋盘的第一个角上绘制3D坐标轴（X，Y，Z轴）。X轴为蓝色，Y轴为绿色，Z轴为红色。因此，实际上Z轴应该感觉像它垂直于我们的棋盘平面。 首先，让我们从先前的校准结果中加载相机矩阵和失真系数。 123456import numpy as npimport cv2 as cvimport glob# Load previously saved datawith np.load('B.npz') as X: mtx, dist, _, _ = [X[i] for i in ('mtx','dist','rvecs','tvecs')] 现在，让我们创建一个函数，绘制，该函数将棋盘上的角（使用cv.findChessboardCorners()获得）和轴点绘制为一个3D轴。 123456def draw(img, corners, imgpts): corner = tuple(corners[0].ravel()) img = cv.line(img, corner, tuple(imgpts[0].ravel()), (255,0,0), 5) img = cv.line(img, corner, tuple(imgpts[1].ravel()), (0,255,0), 5) img = cv.line(img, corner, tuple(imgpts[2].ravel()), (0,0,255), 5) return img 然后，与前面的情况一样，我们创建终止条件，对象点（棋盘角的3D点）和轴点。轴点是3D空间中用于绘制轴的点。我们绘制长度为3的轴（由于我们基于该尺寸进行校准，因此单位将以国际象棋正方形的尺寸为单位）。因此我们的X轴从（0,0,0）绘制为（3,0,0），因此对于Y轴。对于Z轴，从（0,0,0）绘制为（0,0，-3）。负号表示它被拉向相机。 1234criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)objp = np.zeros((6*7,3), np.float32)objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)axis = np.float32([[3,0,0], [0,3,0], [0,0,-3]]).reshape(-1,3) 现在，像往常一样，我们加载每个图像。搜索7x6网格。如果找到，我们将使用子角像素对其进行优化。然后，使用函数cv.solvePnPRansac()计算旋转和平移。一旦有了这些变换矩阵，就可以使用它们将轴点投影到图像平面上。简而言之，我们在图像平面上找到与3D空间中（3,0,0），（0,3,0），（0,0,3）中的每一个相对应的点。一旦获得它们，就可以使用draw()函数从第一个角到这些点中的每个点绘制线条。做完!!! 12345678910111213141516for fname in glob.glob('left*.jpg'): img = cv.imread(fname) gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) ret, corners = cv.findChessboardCorners(gray, (7,6),None) if ret == True: corners2 = cv.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria) # Find the rotation and translation vectors. ret,rvecs, tvecs = cv.solvePnP(objp, corners2, mtx, dist) # project 3D points to image plane imgpts, jac = cv.projectPoints(axis, rvecs, tvecs, mtx, dist) img = draw(img,corners2,imgpts) cv.imshow('img',img) k = cv.waitKey(0) &amp; 0xFF if k == ord('s'): cv.imwrite(fname[:6]+'.png', img)cv.destroyAllWindows() 请参阅下面的一些结果。请注意，每个轴长3个正方形。 渲染立方体如果要绘制立方体，请如下修改draw()函数和轴点。 修改后的draw()函数： 12345678910def draw(img, corners, imgpts): imgpts = np.int32(imgpts).reshape(-1,2) # draw ground floor in green img = cv.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3) # draw pillars in blue color for i,j in zip(range(4),range(4,8)): img = cv.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),3) # draw top layer in red color img = cv.drawContours(img, [imgpts[4:]],-1,(0,0,255),3) return img 修改的轴点。它们是3D空间中多维数据集的8个角： 12axis = np.float32([[0,0,0], [0,3,0], [3,3,0], [3,0,0], [0,0,-3],[0,3,-3],[3,3,-3],[3,0,-3] ]) 查看以下结果： 如果您对图形，增强现实等感兴趣，则可以使用OpenGL渲染更复杂的图形。 对极几何目标在这个部分， 我们将学习多视图几何的基础知识 我们将看到什么是极线，极线，极线约束等。 基本概念当我们使用针孔相机拍摄图像时，我们失去了重要信息，即图像深度。或者图像中的每个点距相机多远，因为它是3D到2D转换。因此，是否能够使用这些摄像机找到深度信息是一个重要的问题。答案是使用不止一台摄像机。在使用两台相机（两只眼睛）的情况下，我们的眼睛以类似的方式工作，这称为立体视觉。因此，让我们看看OpenCV在此字段中提供了什么。 （通过Gary Bradsky 学习OpenCV在该领域有很多信息。） 在深入图像之前，让我们首先了解多视图几何中的一些基本概念。在本节中，我们将讨论对极几何。请参见下图，该图显示了使用两台摄像机拍摄同一场景的图像的基本设置。 如果仅使用左摄像机，则找不到与点x对应的3D点在图像中，因为线$OX$上的每个点投影到图像平面上的同一点。但也要考虑正确的图像。现在$OX$线上的不同点投影到不同点（x“）。因此，使用这两个图像，我们可以对正确的3D点进行三角剖分。这就是整个想法。 OX上不同点的投影在右平面上形成一条线（线$l’$）。我们称它为与点x对应的Epiline。就是说找到点x在右边的图像上，沿着该主线搜索。它应该在这条线上的某处（以这种方式考虑，可以在其他图像中找到匹配点，而无需搜索整个图像，只需沿着Epiline搜索即可。这样可以提供更好的性能和准确性）。这称为对极约束。类似地，所有点在另一幅图像中将具有其对应的Epiline。$XOO’$被称为对极面。 $O$和$O’$是相机中心。从上面给出的设置中，您可以看到右摄像机$O’$的投影在左侧图像上可见。它被称为子极。Epipole是穿过相机中心和图像平面的线的交点。类似的“是左摄像头的子极。在某些情况下，您将无法在图像中找到子极，它们可能位于图像外部（这意味着一台摄像机看不到另一台）。 所有的Epilines都通过其Epipole。因此，要找到中心线的位置，我们可以找到许多中心线并找到它们的交点。 因此，我们将重点放在寻找对极线和极线。但是要找到它们，我们还需要另外两种成分，即基础矩阵（F）和基本矩阵（E）。Essential Matrix包含有关平移和旋转的信息，这些信息在全局坐标中描述了第二个摄像头相对于第一个摄像头的位置。参见下图（图像由Gary Bradsky提供：Learning OpenCV）： 但是我们更喜欢在像素坐标中进行测量，对吗？基本矩阵除包含有关两个摄像头的内在信息之外，还包含与基本矩阵相同的信息，因此我们可以将两个摄像头的像素坐标关联起来。（如果我们使用的是校正后的图像，并通过除以焦距F= E）。简而言之，基本矩阵F将一个图像中的点映射到另一图像中的线（上）。这是从两个图像的匹配点计算得出的。至少需要8个这样的点才能找到基本矩阵（使用8点算法时）。首选更多点，并使用RANSAC获得更可靠的结果。 示例代码因此，首先我们需要在两个图像之间找到尽可能多的匹配项，以找到基本矩阵。为此，我们将SIFT描述符与基于FLANN的匹配器和比率测试结合使用。 123456789101112131415161718192021222324import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg1 = cv.imread('myleft.jpg',0) #queryimage # left imageimg2 = cv.imread('myright.jpg',0) #trainimage # right imagesift = cv.SIFT()# find the keypoints and descriptors with SIFTkp1, des1 = sift.detectAndCompute(img1,None)kp2, des2 = sift.detectAndCompute(img2,None)# FLANN parametersFLANN_INDEX_KDTREE = 1index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)search_params = dict(checks=50)flann = cv.FlannBasedMatcher(index_params,search_params)matches = flann.knnMatch(des1,des2,k=2)good = []pts1 = []pts2 = []# ratio test as per Lowe's paperfor i,(m,n) in enumerate(matches): if m.distance &lt; 0.8*n.distance: good.append(m) pts2.append(kp2[m.trainIdx].pt) pts1.append(kp1[m.queryIdx].pt) 现在，我们有了两个图像的最佳匹配列表。让我们找到基本面矩阵。 123456pts1 = np.int32(pts1)pts2 = np.int32(pts2)F, mask = cv.findFundamentalMat(pts1,pts2,cv.FM_LMEDS)# We select only inlier pointspts1 = pts1[mask.ravel()==1]pts2 = pts2[mask.ravel()==1] 接下来，我们找到Epilines。在第二张图像上绘制与第一张图像中的点相对应的Epilines。因此，在这里提到正确的图像很重要。我们得到了一行线。因此，我们定义了一个新功能来在图像上绘制这些线条。 1234567891011121314def drawlines(img1,img2,lines,pts1,pts2): ''' img1 - image on which we draw the epilines for the points in img2 lines - corresponding epilines ''' r,c = img1.shape img1 = cv.cvtColor(img1,cv.COLOR_GRAY2BGR) img2 = cv.cvtColor(img2,cv.COLOR_GRAY2BGR) for r,pt1,pt2 in zip(lines,pts1,pts2): color = tuple(np.random.randint(0,255,3).tolist()) x0,y0 = map(int, [0, -r[2]/r[1] ]) x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ]) img1 = cv.line(img1, (x0,y0), (x1,y1), color,1) img1 = cv.circle(img1,tuple(pt1),5,color,-1) img2 = cv.circle(img2,tuple(pt2),5,color,-1) return img1,img2 现在，我们在两个图像中都找到了Epiline并将其绘制。 12345678910111213# Find epilines corresponding to points in right image (second image) and# drawing its lines on left imagelines1 = cv.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)lines1 = lines1.reshape(-1,3)img5,img6 = drawlines(img1,img2,lines1,pts1,pts2)# Find epilines corresponding to points in left image (first image) and# drawing its lines on right imagelines2 = cv.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)lines2 = lines2.reshape(-1,3)img3,img4 = drawlines(img2,img1,lines2,pts2,pts1)plt.subplot(121),plt.imshow(img5)plt.subplot(122),plt.imshow(img3)plt.show() 下面是我们得到的结果： 您可以在左侧图像中看到所有Epilines都在右侧图像的一点处收敛。那个汇合点就是极点。 为了获得更好的结果，应使用具有良好分辨率和许多非平面点的图像。 练习题 一个重要的话题是相机的前进。然后，将在两个位置的相同位置看到极点，并且从固定点出现极点。看到这个讨论。 基本矩阵估计对匹配，离群值等的质量敏感。如果所有选定的匹配都位于同一平面上，则情况会变得更糟。检查此讨论。 立体图像的深度图目标在这个环节中 我们将学习根据立体图像创建深度图。 基本我们看到了对极约束和其他相关术语等基本概念。我们还看到，如果我们有两个场景相同的图像，我们可以通过直观的方式从中获取深度信息。下面是一张图片和一些简单的数学公式证明了直觉。 上图包含等效三角形。编写它们的等式将产生以下结果：$$disparity=x−x’=Bf/Z$$所以它在两个图像之间找到对应的匹配。我们已经看到了外延约束如何使这个操作更快、更准确。一旦找到匹配，它就会找到差距。让我们看看如何使用opencv 示例代码下面的代码片段显示了创建视差图的简单过程。 123456789import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimgL = cv.imread('tsukuba_l.png',0)imgR = cv.imread('tsukuba_r.png',0)stereo = cv.StereoBM_create(numDisparities=16, blockSize=15)disparity = stereo.compute(imgL,imgR)plt.imshow(disparity,'gray')plt.show() 下面的图像包含原始图像（左）及其视差图（右）。如您所见，结果受到高度噪声的污染。通过调整numDisparities和blockSize的值，可以获得更好的结果。当您熟悉StereoBM时，会有一些参数，可能需要微调参数以获得更好，更平滑的结果。 参数： texture_threshold：过滤出纹理不足以进行可靠匹配的区域 Speckle range and size：基于块的匹配器通常会在对象边界附近产生“斑点”，匹配的窗口在一侧捕获前景，而在另一侧捕获背景。在此场景中，匹配器似乎还在桌子上投影的纹理中找到小的虚假匹配项。为了消除这些伪像，我们使用由speckle_size和speckle_range参数控制的散斑滤镜对视差图像进行后处理。speckle_size是将视差斑点忽略为“斑点”的像素数。+ speckle_range控制必须将值差异视为同一对象的一部分的程度。 Number of disparities：滑动窗口的像素数。它越大，可见深度的范围就越大，但是需要更多的计算。 min_disparity：从开始搜索的左像素的x位置开始的偏移量。 uniqueness_ratio：另一个后过滤步骤。如果最佳匹配视差不足够好于搜索范围内的所有其他视差，则将像素滤出。如果texture_threshold和散斑过滤仍在通过虚假匹配，则可以尝试进行调整。 prefilter_size和prefilter_cap：预过滤阶段，可标准化图像亮度并增强纹理，以准备块匹配。通常，您不需要调整这些。]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[视频分析]]></title>
    <url>%2Fopencv%2F%E8%A7%86%E9%A2%91%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[如何使用背景减法在视频流算法上找到用途，例如：运动提取，特征跟踪和前景提取。我们将学习如何从视频和图像序列中提取前景蒙版并显示它们。 背景减法（BS）是通过使用静态相机来生成前景蒙版（即，包含属于场景中的移动物体的像素的二进制图像）的通用且广泛使用的技术。 顾名思义，BS计算前景蒙板，在当前帧和背景模型之间执行减法运算，其中包含场景的静态部分，或者更一般而言，考虑到所观察场景的特征，可以将其视为背景的所有内容。 后台建模包括两个主要步骤： 1.后台初始化2.后台更新第一步，计算背景的初始模型，而在第二步中，更新模型以适应场景中可能的变化。 在本教程中，我们将学习如何使用OpenCV执行BS。 目标在本教程中，您将学习如何： 使用cv :: VideoCapture从视频或图像序列中读取数据； 通过使用cv :: BackgroundSubtractor类创建和更新背景模型； 通过使用cv :: imshow获取并显示前景蒙版； 示例代码在下面，您可以找到源代码。我们将让用户选择处理视频文件或图像序列。 在此示例中，我们将使用cv :: BackgroundSubtractorMOG2生成前景蒙版。 结果和输入数据显示在屏幕上。 1234567891011121314151617181920212223242526272829303132333435from __future__ import print_functionimport cv2 as cvimport argparseparser = argparse.ArgumentParser(description='This program shows how to use background subtraction methods provided by \ OpenCV. You can process both videos and images.')parser.add_argument('--input', type=str, help='Path to a video or a sequence of image.', default='vtest.avi')parser.add_argument('--algo', type=str, help='Background subtraction method (KNN, MOG2).', default='MOG2')args = parser.parse_args()if args.algo == 'MOG2': backSub = cv.createBackgroundSubtractorMOG2()else: backSub = cv.createBackgroundSubtractorKNN()capture = cv.VideoCapture(cv.samples.findFileOrKeep(args.input))if not capture.isOpened: print('Unable to open: ' + args.input) exit(0)while True: ret, frame = capture.read() if frame is None: break fgMask = backSub.apply(frame) cv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1) cv.putText(frame, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0)) cv.imshow('Frame', frame) cv.imshow('FG Mask', fgMask) keyboard = cv.waitKey(30) if keyboard == 'q' or keyboard == 27: break 说明我们讨论上面代码的主要部分： 一个cv :: BackgroundSubtractor对象将用于生成前景蒙版。在此示例中，使用了默认参数，但是也可以在create函数中声明特定的参数。 12345#create Background Subtractor objectsif args.algo == 'MOG2': backSub = cv.createBackgroundSubtractorMOG2()else: backSub = cv.createBackgroundSubtractorKNN() CV :: VideoCapture对象用于读取输入的视频或输入图像序列。 1234capture = cv.VideoCapture(cv.samples.findFileOrKeep(args.input))if not capture.isOpened: print('Unable to open: ' + args.input) exit(0) 每帧都用于计算前景蒙版和更新背景。如果要更改用于更新背景模型的学习率，可以通过将参数传递给apply方法来设置特定的学习率。 12#update the background modelfgMask = backSub.apply(frame) 当前帧号可以从cv :: VideoCapture对象中提取，并标记在当前帧的左上角。白色矩形用于突出显示黑色的帧编号。 1234#get the frame number and write it on the current framecv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)cv.putText(frame, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0)) 我们准备显示当前的输入框和结果。 123#show the current frame and the fg maskscv.imshow('Frame', frame)cv.imshow('FG Mask', fgMask) 结果对于vtest.avi视频，适用于以下框架： 1MOG2方法的程序输出如下所示（检测到灰色区域有阴影）： Meanshift和Camshift目标在这一章当中， 我们将学习用于跟踪视频中对象的Meanshift和Camshift算法。 均值漂移均值移位背后的直觉很简单。考虑一下你有几点。（它可以是像素分布，例如直方图反投影）。您会得到一个小窗口（可能是一个圆形），并且必须将该窗口移到最大像素密度（或最大点数）的区域。如下图所示： 初始窗口以蓝色圆圈显示，名称为“ C1”。其原始中心以蓝色矩形标记，名称为“ C1_o”。但是，如果您找到该窗口内点的质心，则会得到点“ C1_r”（标记为蓝色小圆圈），它是窗口的真实质心。当然，它们不匹配。因此，移动窗口，使新窗口的圆与上一个质心匹配。再次找到新的质心。很可能不会匹配。因此，再次移动它，并继续迭代，以使窗口的中心及其质心落在同一位置（或在很小的期望误差内）。因此，最终您获得的是一个具有最大像素分布的窗口。它带有一个绿色圆圈，名为“ C2”。正如您在图像中看到的，它具有最大的点数。 因此，我们通常会传递直方图反投影图像和初始目标位置。当对象移动时，显然该移动会反映在直方图反投影图像中。结果，meanshift算法将我们的窗口以最大密度移动到新位置。 OpenCV中的Meanshift要在OpenCV中使用均值偏移，首先我们需要设置目标，找到其直方图，以便我们可以将目标反投影到每帧上以计算均值偏移。我们还需要提供窗口的初始位置。对于直方图，此处仅考虑色相。另外，为避免由于光线不足而产生错误的值，可以使用cv.inRange()函数丢弃光线不足的值。 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport cv2 as cvimport argparseparser = argparse.ArgumentParser(description='This sample demonstrates the meanshift algorithm. \ The example file can be downloaded from: \ https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')parser.add_argument('image', type=str, help='path to image file')args = parser.parse_args()cap = cv.VideoCapture(args.image)# take first frame of the videoret,frame = cap.read()# setup initial location of windowx, y, w, h = 300, 200, 100, 50 # simply hardcoded the valuestrack_window = (x, y, w, h)# set up the ROI for trackingroi = frame[y:y+h, x:x+w]hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)# Setup the termination criteria, either 10 iteration or move by atleast 1 ptterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )while(1): ret, frame = cap.read() if ret == True: hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # apply meanshift to get the new location ret, track_window = cv.meanShift(dst, track_window, term_crit) # Draw it on image x,y,w,h = track_window img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2) cv.imshow('img2',img2) k = cv.waitKey(30) &amp; 0xff if k == 27: break else: break 我使用的视频中的三帧如下： Camshift您是否密切关注了最后结果？有一个问题。无论汽车离相机很近或非常近，我们的窗口始终具有相同的大小。这是不好的。我们需要根据目标的大小和旋转来调整窗口大小。该解决方案再次来自“ OpenCV Labs”，它被称为CAMshift（连续自适应平均移位），由Gary Bradsky在其1998年的论文《用于感知用户界面中的计算机视觉面部跟踪》中发表,它首先应用均值移位。一旦均值收敛，它将更新窗口的大小,它还可以计算出最合适的椭圆的方向。再次将均值偏移应用于新的缩放搜索窗口和先前的窗口位置。该过程一直持续到达到要求的精度为止。 OpenCV中的Camshift它与meanshift相似，但返回一个旋转的矩形（即我们的结果）和box参数（用于在下一次迭代中作为搜索窗口传递）。请参见下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport cv2 as cvimport argparseparser = argparse.ArgumentParser(description='This sample demonstrates the camshift algorithm. \ The example file can be downloaded from: \ https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')parser.add_argument('image', type=str, help='path to image file')args = parser.parse_args()cap = cv.VideoCapture(args.image)# take first frame of the videoret,frame = cap.read()# setup initial location of windowx, y, w, h = 300, 200, 100, 50 # simply hardcoded the valuestrack_window = (x, y, w, h)# set up the ROI for trackingroi = frame[y:y+h, x:x+w]hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)# Setup the termination criteria, either 10 iteration or move by atleast 1 ptterm_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )while(1): ret, frame = cap.read() if ret == True: hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # apply camshift to get the new location ret, track_window = cv.CamShift(dst, track_window, term_crit) # Draw it on image pts = cv.boxPoints(ret) pts = np.int0(pts) img2 = cv.polylines(frame,[pts],True, 255,2) cv.imshow('img2',img2) k = cv.waitKey(30) &amp; 0xff if k == 27: break else: break 结果显示三帧： 光流目标在这一章当中， 我们将了解光流的概念及其使用Lucas-Kanade方法的估算。 我们将使用cv.calcOpticalFlowPyrLK()之类的函数来跟踪视频中的特征点。 我们将使用cv.calcOpticalFlowFarneback()方法创建一个密集的光流场。 光流光流是图像对象在两个连续帧之间由对象或相机的运动力矩引起的视觉运动的模式。它是2D向量场，其中每个向量都是位移向量，表示点从第一帧到第二帧的运动。请看下面的图片（图片提供：Wikipedia关于Optical Flow的文章）。 它显示了一个球连续5帧运动。箭头显示其位移向量。光流在以下领域有许多应用： 运动结构 视频压缩 视频稳定…光流的工作基于以下几个假设： 物体的像素强度在连续的帧之间不改变。 相邻像素具有相似的运动。 Lucas-Kanade方法之前我们已经看到一个假设，即所有相邻像素将具有相似的运动。Lucas-Kanade方法在该点周围需要3x3色块。因此，所有9个点都具有相同的运动。（用Harris拐角检测器检查逆矩阵的相似性。这表示拐角是更好的跟踪点。） 因此，从用户的角度来看，这个想法很简单，我们给一些点进行跟踪，然后接收这些点的光流矢量。但是同样存在一些问题。到现在为止，我们只处理小动作，所以当大动作时它就失败了。为了解决这个问题，我们使用金字塔。当我们上金字塔时，较小的动作将被删除，较大的动作将变为较小的动作。因此，通过在此处应用Lucas-Kanade，我们可以获得与标尺一起的光流。 OpenCV中的Lucas-Kanade光流OpenCV在单个函数cv.calcOpticalFlowPyrLK()中提供所有这些功能。在这里，我们创建一个简单的应用程序来跟踪视频中的某些点。为了确定点，我们使用cv.goodFeaturesToTrack()。我们采用第一帧，检测其中的一些Shi-Tomasi角点，然后使用Lucas-Kanade光流迭代地跟踪这些点。对于函数cv.calcOpticalFlowPyrLK()，我们传递前一帧，前一点和下一帧。它返回下一个点以及一些状态编号，如果找到下一个点，则状态值为1，否则为零。我们迭代地将这些下一个点作为下一步中的上一个点传递。请参见下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import numpy as npimport cv2 as cvimport argparseparser = argparse.ArgumentParser(description='This sample demonstrates Lucas-Kanade Optical Flow calculation. \ The example file can be downloaded from: \ https://www.bogotobogo.com/python/OpenCV_Python/images/mean_shift_tracking/slow_traffic_small.mp4')parser.add_argument('image', type=str, help='path to image file')args = parser.parse_args()cap = cv.VideoCapture(args.image)# params for ShiTomasi corner detectionfeature_params = dict( maxCorners = 100, qualityLevel = 0.3, minDistance = 7, blockSize = 7 )# Parameters for lucas kanade optical flowlk_params = dict( winSize = (15,15), maxLevel = 2, criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))# Create some random colorscolor = np.random.randint(0,255,(100,3))# Take first frame and find corners in itret, old_frame = cap.read()old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)p0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)# Create a mask image for drawing purposesmask = np.zeros_like(old_frame)while(1): ret,frame = cap.read() frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) # calculate optical flow p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params) # Select good points good_new = p1[st==1] good_old = p0[st==1] # draw the tracks for i,(new,old) in enumerate(zip(good_new, good_old)): a,b = new.ravel() c,d = old.ravel() mask = cv.line(mask, (a,b),(c,d), color[i].tolist(), 2) frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1) img = cv.add(frame,mask) cv.imshow('frame',img) k = cv.waitKey(30) &amp; 0xff if k == 27: break # Now update the previous frame and previous points old_gray = frame_gray.copy() p0 = good_new.reshape(-1,1,2) （此代码不会检查下一个关键点的正确性。因此，即使任何特征点在图像中消失了，光流也有可能找到下一个看起来可能与它接近的下一个点。因此，对于稳健的跟踪，实际上应该以特定的时间间隔检测点。OpenCV样本附带了这样一个样本，该样本每5帧发现一次特征点，并且还对光流点进行了后向检查，以仅选择良好的流点。 lk_track.py）。 查看我们得到的结果： OpenCV中的密集光流Lucas-Kanade方法计算稀疏特征集的光流（在我们的示例中为使用Shi-Tomasi算法检测到的角）。OpenCV提供了另一种算法来查找密集的光流。它计算帧中所有点的光通量。它基于Gunner Farneback的算法，在2003年Gunner Farneback的“基于多项式展开的两帧运动估计”中对此进行了解释。 1234567891011121314151617181920212223import numpy as npimport cv2 as cvcap = cv.VideoCapture(cv.samples.findFile("vtest.avi"))ret, frame1 = cap.read()prvs = cv.cvtColor(frame1,cv.COLOR_BGR2GRAY)hsv = np.zeros_like(frame1)hsv[...,1] = 255while(1): ret, frame2 = cap.read() next = cv.cvtColor(frame2,cv.COLOR_BGR2GRAY) flow = cv.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0) mag, ang = cv.cartToPolar(flow[...,0], flow[...,1]) hsv[...,0] = ang*180/np.pi/2 hsv[...,2] = cv.normalize(mag,None,0,255,cv.NORM_MINMAX) bgr = cv.cvtColor(hsv,cv.COLOR_HSV2BGR) cv.imshow('frame2',bgr) k = cv.waitKey(30) &amp; 0xff if k == 27: break elif k == ord('s'): cv.imwrite('opticalfb.png',frame2) cv.imwrite('opticalhsv.png',bgr) prvs = next 看到下面的结果：]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征检测与描述]]></title>
    <url>%2Fopencv%2F%E7%89%B9%E5%BE%81%E6%A3%80%E6%B5%8B%E4%B8%8E%E6%8F%8F%E8%BF%B0.html</url>
    <content type="text"><![CDATA[了解功能目标 在本章中，我们将尝试理解什么是特征，为什么重要，拐角重要等。 说明你们大多数人都会玩拼图游戏。您会得到很多小图像，需要正确组装它们以形成大的真实图像。问题是，你怎么做？将相同的理论投影到计算机程序上，以便计算机可以玩拼图游戏呢？如果计算机可以玩拼图游戏，为什么我们不能给计算机提供很多自然风光的真实图像，并告诉计算机将所有这些图像拼接成一个大的单个图像呢？如果计算机可以将多个自然图像缝合在一起，那么如何给建筑物或任何结构提供大量图片并告诉计算机从中创建3D模型呢？ 好了，问题和想象力还在继续。但这全都取决于最基本的问题：您如何玩拼图游戏？您如何将许多加扰的图像片段排列成一个大的单个图像？您如何将许多自然图像拼接到一张图像上？ 答案是，我们正在寻找独特的，可以轻松跟踪和比较的独特模式或特定功能。如果我们对这种功能进行定义，我们可能会发现很难用语言来表达它，但是我们知道它们是什么。如果有人要求您指出一项可以在多张图像中进行比较的良好功能，则可以指出其中一项。这就是为什么即使是小孩也可以玩这些游戏的原因。我们在图像中搜索这些特征，找到它们，在其他图像中寻找相同的特征并将它们对齐。而已。（在拼图游戏中，我们更多地研究了不同图像的连续性）。所有这些能力都是我们固有的。 因此，我们的一个基本问题扩展到更多，但变得更加具体。这些功能是什么？。（答案对于计算机也应该是可以理解的。） 很难说人类如何发现这些特征。这已经在我们的大脑中进行了编程。但是，如果我们深入研究某些图片并搜索不同的样式，我们会发现一些有趣的东西。例如，拍摄下图：图像非常简单。在图像的顶部，给出了六个小图像块。您的问题是在原始图像中找到这些补丁的确切位置。您可以找到多少正确的结果？ A和B是平坦的表面，它们散布在很多区域上。很难找到这些补丁的确切位置。 C和D更简单。它们是建筑物的边缘。您可以找到一个大概的位置，但是准确的位置仍然很困难。这是因为沿着边缘的每个地方的图案都是相同的。但是，在边缘，情况有所不同。因此，与平坦区域相比，边缘是更好的功能，但不够好（在拼图游戏中比较边缘的连续性很好）。 最后，E和F是建筑物的某些角落。而且很容易找到它们。因为在拐角处，无论将此修补程序移动到何处，它的外观都将有所不同。因此，它们可以被视为很好的功能。因此，现在我们进入更简单（且被广泛使用的图像）以更好地理解。 就像上面一样，蓝色补丁是平坦区域，很难找到和跟踪。无论您将蓝色补丁移到何处，它看起来都一样。黑色补丁有一个边缘。如果沿垂直方向（即沿渐变）移动它，则它会改变。沿着边缘（平行于边缘）移动，看起来相同。对于红色补丁，这是一个角落。无论您将补丁移动到何处，它看起来都不同，这意味着它是唯一的。因此，基本上，角点被认为是图像中的良好特征。（不仅是角落，在某些情况下，斑点也被认为是不错的功能）。 因此，现在我们回答了我们的问题，“这些功能是什么？”。但是出现了下一个问题。我们如何找到它们？还是我们如何找到角落？我们以一种直观的方式回答了这一问题，即寻找图像中在其周围所有区域中移动（少量）变化最大的区域。在接下来的章节中，这将被投影到计算机语言中。因此，找到这些图像特征称为特征检测。 我们在图像中找到了功能。找到它之后，您应该能够在其他图像中找到相同的图像。怎么做？我们围绕该特征采取一个区域，用自己的语言解释它，例如“上部是蓝天，下部是建筑物的区域，在建筑物上有玻璃等”，而您在另一个建筑物中搜索相同的区域图片。基本上，您是在描述功能。同样，计算机还应该描述特征周围的区域，以便可以在其他图像中找到它。所谓的描述称为功能描述。获得功能及其描述后，您可以在所有图像中找到相同的功能并将它们对齐，缝合在一起或进行所需的操作。 因此，在此模块中，我们正在寻找OpenCV中的不同算法来查找功能，对其进行描述，对它们进行匹配等。 Harris特征目标在这一章当中， 我们将了解Harris拐角检测背后的概念。 我们将看到以下函数：cv.cornerHarris()，cv.cornerSubPix() 理论在上一章中，我们看到角是图像中各个方向上强度变化很大的区域。克里斯·哈里斯（Chris Harris）和迈克·史蒂芬斯（Mike Stephens）在1988年的论文《组合式拐角和边缘检测器》中做了一次尝试找到这些拐角的尝试，所以现在将其称为Harris拐角检测器。 OpenCV中的Harris角落探测器为此， OpenCV具有功能cv.cornerHarris()。它的参数是： img-输入图像，它应该是灰度和float32类型。 blockSize-它是考虑进行角点检测的邻域的大小 ksize-使用的Sobel导数的光圈参数。 k-等式中的哈里斯检测器自由参数。请参阅以下示例： 1234567891011121314import numpy as npimport cv2 as cvfilename = 'chessboard.png'img = cv.imread(filename)gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)gray = np.float32(gray)dst = cv.cornerHarris(gray,2,3,0.04)#result is dilated for marking the corners, not importantdst = cv.dilate(dst,None)# Threshold for an optimal value, it may vary depending on the image.img[dst&gt;0.01*dst.max()]=[0,0,255]cv.imshow('dst',img)if cv.waitKey(0) &amp; 0xff == 27: cv.destroyAllWindows() 以下是三个结果： 在这一章当中， 我们将了解Harris拐角检测背后的概念。 我们将看到以下函数：cv.cornerHarris()，cv.cornerSubPix() 理论在上一章中，我们看到角是图像中各个方向上强度变化很大的区域。克里斯·哈里斯（Chris Harris）和迈克·史蒂芬斯（Mike Stephens）在1988年的论文《组合式拐角和边缘检测器》中做了一次尝试找到这些拐角的尝试，所以现在将其称为Harris拐角检测器。 OpenCV中的Harris角落探测器为此， OpenCV具有功能cv.cornerHarris()。它的参数是： img-输入图像，它应该是灰度和float32类型。 blockSize-它是考虑进行角点检测的邻域的大小 ksize-使用的Sobel导数的光圈参数。 k-等式中的哈里斯检测器自由参数。请参阅以下示例： 1234567891011121314import numpy as npimport cv2 as cvfilename = 'chessboard.png'img = cv.imread(filename)gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)gray = np.float32(gray)dst = cv.cornerHarris(gray,2,3,0.04)#result is dilated for marking the corners, not importantdst = cv.dilate(dst,None)# Threshold for an optimal value, it may vary depending on the image.img[dst&gt;0.01*dst.max()]=[0,0,255]cv.imshow('dst',img)if cv.waitKey(0) &amp; 0xff == 27: cv.destroyAllWindows() 以下是三个结果： SubPixel精度的转角有时，您可能需要以最大的精度找到拐角。OpenCV带有一个函数cv.cornerSubPix()，该函数进一步细化了以亚像素精度检测到的角。下面是一个例子。和往常一样，我们需要先找到Harris。然后我们传递这些角的质心（在角处可能有一堆像素，我们取它们的质心）以细化它们。Harris角用红色像素标记，精制角用绿色像素标记。对于此功能，我们必须定义何时停止迭代的条件。我们会在指定的迭代次数或达到一定的精度之后（以先到者为准）停止它。我们还需要定义搜索拐角的邻域的大小。 12345678910111213141516171819202122import numpy as npimport cv2 as cvfilename = 'chessboard2.jpg'img = cv.imread(filename)gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)# find Harris cornersgray = np.float32(gray)dst = cv.cornerHarris(gray,2,3,0.04)dst = cv.dilate(dst,None)ret, dst = cv.threshold(dst,0.01*dst.max(),255,0)dst = np.uint8(dst)# find centroidsret, labels, stats, centroids = cv.connectedComponentsWithStats(dst)# define the criteria to stop and refine the cornerscriteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 100, 0.001)corners = cv.cornerSubPix(gray,np.float32(centroids),(5,5),(-1,-1),criteria)# Now draw themres = np.hstack((centroids,corners))res = np.int0(res)img[res[:,1],res[:,0]]=[0,0,255]img[res[:,3],res[:,2]] = [0,255,0]cv.imwrite('subpixel5.png',img) 以下是结果，其中一些重要位置显示在缩放窗口中以可视化： Shi-Tomasi拐角探测器和良好的跟踪功能目标在这一章当中， 我们将学习另一个拐角检测器：Shi-Tomasi拐角检测器 我们将看到以下函数：cv.goodFeaturesToTrack() 理论在上一章中，我们看到了Harris Corner Detector。1994年晚些时候，J.Shi和C.Tomasi在其论文Good Good to Track中对其进行了小的修改，与Harris Harris Detector相比，该方法显示出更好的结果。 示例代码OpenCV具有一个函数cv.goodFeaturesToTrack()。它通过Shi-Tomasi方法（或Harris检测，如果指定）找到图像中的N个最强角。像往常一样，图像应该是灰度图像。然后，指定要查找的角数。然后，您指定质量级别，该值是介于0-1之间的值，该值表示每个角落都被拒绝的最低拐角质量。然后，我们提供检测到的角之间的最小欧式距离。 利用所有这些信息，该功能可以找到图像中的角。低于质量水平的所有角点均被拒绝。然后，它会根据质量以降序对剩余的角进行排序。然后函数首先获取最强角，然后丢弃最小距离范围内的所有附近角，然后返回N个最强角。 在下面的示例中，我们将尝试找到25个最佳弯角： 1234567891011import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('blox.jpg')gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)corners = cv.goodFeaturesToTrack(gray,25,0.01,10)corners = np.int0(corners)for i in corners: x,y = i.ravel() cv.circle(img,(x,y),3,255,-1)plt.imshow(img),plt.show() 看到下面的结果： 此功能更适合跟踪。 SIFT（尺度不变特征变换）简介目标在这一章当中， 我们将学习SIFT算法的概念 我们将学习找到SIFT关键点和描述符。 理论在最后两章中，我们看到了一些像Harris这样的角检测器。它们是旋转不变的，这意味着即使图像旋转了，我们也可以找到相同的角。很明显，因为转角在旋转的图像中也仍然是转角。但是缩放呢？如果缩放图像，则角可能不是角。例如，检查下面的简单图像。当在同一窗口中放大图像时，在小窗口中的小图像中的角是平坦的。因此，Harris角不是尺度不变的。因此，在2004年，不列颠哥伦比亚大学的D.Lowe在他的论文《尺度不变关键点中的独特图像特征》中提出了一种新的尺度不变特征变换（SIFT）算法，该算法提取关键点并计算其描述符。（本文易于理解，被认为是SIFT上可获得的最佳材料。因此，此说明只是本文的简短摘要）。 SIFT算法主要包括四个步骤。我们将一一看到它们。 1.尺度空间极值检测从上图可以明显看出，我们不能使用相同的窗口来检测具有不同比例的关键点。用小角可以。但是要检测更大的角落，我们需要更大的窗口。为此，使用了比例空间过滤。在其中，找到具有各种值的图像的高斯拉普拉斯算子。LoG用作斑点检测器，可检测变化导致的各种大小的斑点。简而言之，用作缩放参数。例如，在上图中，低的高斯核对于较小的拐角给出较高的值，而高的高斯核对于较大的拐角给出很好的拟合。因此，我们可以找到整个比例尺和空间的局部最大值，$$σσσσσ（x ，y，σ）$$值，这意味着在比例下的$$（x，y）$$处可能存在关键点$σ$但是这种LoG有点昂贵，因此SIFT算法使用的是高斯差，它是LoG的近似值。高斯差是作为具有两个不同的图像的高斯模糊差而获得的，设为和。此过程是针对高斯金字塔中图像的不同八度完成的。 一旦找到该DoG，便会在图像上搜索比例和空间上的局部极值。例如，将图像中的一个像素与它的8个邻居以及下一个比例的9个像素和前一个比例的9个像素进行比较。如果是局部极值，则可能是关键点。从根本上说，关键点是最好的代表。如下图所示： 对于不同的参数，本文给出了一些经验数据，可以总结为：octaves = 4,scale levels = 5, initial $σ=1.6k=2–\sqrt{2}$ 2.关键点本地化一旦找到潜在的关键点位置，就必须对其进行优化以获取更准确的结果。他们使用了标度空间的泰勒级数展开来获得更精确的极值位置，如果该极值处的强度小于阈值（根据论文为0.03），则将其拒绝。该阈值被称为contrastThreshold DoG对边缘的响应较高，因此也需要删除边缘。为此，使用类似于哈里斯拐角检测器的概念。他们使用2x2的Hessian矩阵（H）计算主曲率。从哈里斯拐角检测器我们知道，对于边缘，一个特征值大于另一个特征值。所以他们在这里使用了一个简单的功能 如果此比率大于一个阈值（在OpenCV中称为edgeThreshold），则将丢弃该关键点。纸上为10。 因此，它消除了任何低对比度的关键点和边缘关键点，剩下的就是强烈的兴趣点。 3.方向分配现在，将方向分配给每个关键点，以实现图像旋转的不变性。根据比例在关键点位置附近采取邻域，并在该区域中计算梯度大小和方向。创建了一个具有36个覆盖360度的bin的方向直方图（它由梯度幅度和具有σ的高斯加权圆窗加权 4.关键点描述符现在创建了关键点描述符。在关键点周围采用了16x16的邻域。它分为16个4x4大小的子块。对于每个子块，创建8 bin方向直方图。因此共有128个bin值可用。它被表示为形成关键点描述符的向量。除此之外，还采取了几种措施来实现针对照明变化，旋转等的鲁棒性。 5.关键点匹配通过识别两个图像的最近邻居，可以匹配两个图像之间的关键点。但是在某些情况下，第二个最接近的匹配可能非常接近第一个。它可能是由于噪音或其他原因而发生的。在那种情况下，采用最接近距离与第二最接近距离之比。如果大于0.8，将被拒绝。根据论文，它可以消除大约90％的错误匹配，而只丢弃5％的正确匹配。 因此，这是SIFT算法的总结。有关更多详细信息和理解，强烈建议阅读原始论文。记住一件事，该算法已申请专利。所以这个算法包含在opencv contrib repo中 OpenCV中的SIFT现在让我们来看一下OpenCV中可用的SIFT功能。让我们从关键点检测开始并进行绘制。首先，我们必须构造一个SIFT对象。我们可以将不同的参数传递给它，这些参数是可选的，它们在docs中已得到很好的解释。 12345678import numpy as npimport cv2 as cvimg = cv.imread('home.jpg')gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)sift = cv.xfeatures2d.SIFT_create()kp = sift.detect(gray,None)img=cv.drawKeypoints(gray,kp,img)cv.imwrite('sift_keypoints.jpg',img) sift.detect()函数在图像中找到关键点。如果您只想搜索图像的一部分，则可以通过遮罩。每个关键点都是一个特殊的结构，具有许多属性，例如其（x，y）坐标，有意义的邻域的大小，指定其方向的角度，指定关键点强度的响应等。 OpenCV还提供cv.drawKeyPoints()函数，该函数在关键点的位置绘制小圆圈。如果将标志cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS传递给它，它将绘制一个具有关键点大小的圆，甚至会显示其方向。请参见以下示例。 12img=cv.drawKeypoints(gray,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)cv.imwrite('sift_keypoints.jpg',img) 现在要计算描述符，OpenCV提供了两种方法。 由于已经找到关键点，因此可以调用sift.compute()，它从我们发现的关键点计算描述符。例如：kp，des = sift.compute（gray，kp） 如果找不到关键点，则可以使用sift.detectAndCompute()函数在一个步骤中直接找到关键点和描述符。我们将看到第二种方法： 12sift = cv.xfeatures2d.SIFT_create()kp, des = sift.detectAndCompute(gray,None) 这里kp是一个关键点列表，des是一个numpy形状数组$ Number_{Keypoints} × 128$ 这样我们得到了关键点，描述符等。现在我们想看看如何在不同图像中匹配关键点。我们将在接下来的章节中学习。 SURF简介（加速的强大功能）目标在这一章当中， 我们将了解SURF的基础 我们将在OpenCV中看到SURF功能 理论在上一章中，我们看到了用于关键点检测和描述的SIFT。但是它相对较慢，人们需要更快的版本。2006年，三个人，H.Bay，Tytelaars.T。和Van Gool.L，发表了另一篇论文《SURF：加速了鲁棒特征》，介绍了一种称为SURF的新算法。顾名思义，它是SIFT的加速版本。 在SIFT中，Lowe用高斯差近似高斯的拉普拉斯算子来寻找尺度空间。SURF走得更远，使用Box Filter近似LoG。下图显示了这种近似值的演示。这种近似的一大优势是，借助积分图像可以轻松地计算出带盒滤波器的卷积。并且可以针对不同规模并行执行。SURF还依赖于Hessian矩阵的行列式来确定规模和位置。 对于方向分配，SURF在水平和垂直方向上对大小为6s的邻域使用小波响应。适当的高斯权重也适用于它。然后将它们绘制在下图所示的空间中。通过计算角度为60度的滑动方向窗口内所有响应的总和，可以估算主导方向。有趣的是，小波响应可以很容易地使用积分图像在任何规模下发现。对于许多应用，不需要旋转不变性，因此无需查找此方向，从而加快了过程。SURF提供了称为Upright-SURF或U-SURF的功能。它提高了速度，并且在非常强大± 15∘。OpenCV的支持，这取决于标志，直立。如果为0，则计算方向。如果为1，则不计算方向并且速度更快。对于功能描述，SURF在水平和垂直方向上使用小波响应（同样，使用积分图像使事情变得更容易）。在s是大小的关键点周围采用大小为20sX20s的邻域。它分为4x4子区域。对于每个子区域，采用水平和垂直小波响应，并形成一个矢量，表示为向量时，这将为SURF特征描述符提供总共64个维度。尺寸越小，计算和匹配速度越快，但特征的区分性更好。为了更加独特，SURF特征描述符具有扩展的128维版本。另一个重要的改进是对潜在兴趣点使用了拉普拉斯符号（Hessian矩阵的迹线）。它不增加计算成本，因为它已在检测期间进行了计算。拉普拉斯算子的标志将深色背景上的明亮斑点与相反的情况区分开。在匹配阶段，我们仅比较具有相同对比度类型的特征（如下图所示）。这些最少的信息可加快匹配速度，而不会降低描述符的性能。 简而言之，SURF添加了许多功能来提高每一步的速度。分析表明，它的速度是SIFT的3倍，而性能却与SIFT相当。SURF擅长处理具有模糊和旋转的图像，但不擅长处理视点变化和照明变化。 OpenCV中的SURFOpenCV提供类似于SIFT的SURF功能。您可以使用一些可选条件（例如64/128-dim描述符，Upright/Normal SURF等）来启动SURF对象。所有详细信息均在文档中进行了详细说明。然后，就像在SIFT中所做的那样，我们可以使用SURF.detect()，SURF.compute()等来查找关键点和描述符。 首先，我们将看到一个有关如何找到SURF关键点和描述符并进行绘制的简单演示。所有示例都在Python终端中显示，因为它仅与SIFT相同。 12345678&gt;&gt;&gt; img = cv.imread('fly.png',0)# Create SURF object. You can specify params here or later.# Here I set Hessian Threshold to 400&gt;&gt;&gt; surf = cv.xfeatures2d.SURF_create(400)# Find keypoints and descriptors directly&gt;&gt;&gt; kp, des = surf.detectAndCompute(img,None)&gt;&gt;&gt; len(kp) 699 图片中无法显示1199个关键点。我们将其减少到50左右以绘制在图像上。匹配时，我们可能需要所有这些功能，但现在不需要。因此，我们增加了黑森州阈值。 12345678910# Check present Hessian threshold&gt;&gt;&gt; print( surf.getHessianThreshold() )400.0# We set it to some 50000. Remember, it is just for representing in picture.# In actual cases, it is better to have a value 300-500&gt;&gt;&gt; surf.setHessianThreshold(50000)# Again compute keypoints and check its number.&gt;&gt;&gt; kp, des = surf.detectAndCompute(img,None)&gt;&gt;&gt; print( len(kp) )47 1它小于50。让我们在图像上绘制它。 12&gt;&gt;&gt; img2 = cv.drawKeypoints(img,kp,None,(255,0,0),4)&gt;&gt;&gt; plt.imshow(img2),plt.show() 请参阅下面的结果。您可以看到SURF更像是斑点检测器。它检测到蝴蝶翅膀上的白色斑点。您可以使用其他图像进行测试。 现在，我想应用U-SURF，以便它不会找到方向。 12345678# Check upright flag, if it False, set it to True&gt;&gt;&gt; print( surf.getUpright() )False&gt;&gt;&gt; surf.setUpright(True)# Recompute the feature points and draw it&gt;&gt;&gt; kp = surf.detect(img,None)&gt;&gt;&gt; img2 = cv.drawKeypoints(img,kp,None,(255,0,0),4)&gt;&gt;&gt; plt.imshow(img2),plt.show() 请参阅下面的结果。所有方向均以相同方向显示。它比以前更快。如果您正在处理方向不成问题的情况（例如全景拼接）等，那会更好。 最终，我们检查描述符的大小，如果仅为64维，则将其更改为128。 12345678910111213# Find size of descriptor&gt;&gt;&gt; print( surf.descriptorSize() )64# That means flag, "extended" is False.&gt;&gt;&gt; surf.getExtended() False# So we make it to True to get 128-dim descriptors.&gt;&gt;&gt; surf.setExtended(True)&gt;&gt;&gt; kp, des = surf.detectAndCompute(img,None)&gt;&gt;&gt; print( surf.descriptorSize() )128&gt;&gt;&gt; print( des.shape )(47, 128) 其余部分是匹配的，我们将在另一章中进行匹配。 用于角点检测的FAST算法目标在这一章当中， 我们将了解FAST算法的基础 我们将使用OpenCV功能的FAST算法找到角落。 理论我们看到了几个特征检测器，其中很多真的很好。但是，从实时应用程序的角度来看，它们不够快。最好的例子是计算资源有限的SLAM（同时定位和地图绘制）移动机器人。 为了解决此问题，Edward Rosten和Tom Drummond在2006年的论文“用于高速拐角检测的机器学习”中提出了FAST（加速段测试的特征）算法（后来在2010年进行了修订）。该算法的基本摘要如下。有关更多详细信息，请参阅原始纸张（所有图像均取自原始纸张）。 使用FAST进行特征检测 选择图像中是否要识别为兴趣点的像素$p$。使其强度为$lp$。 选择适当的阈值$t$。 考虑被测像素周围有16个像素的圆圈。（见下图） 圆圈中的连续像素（共16个像素）都比$p$亮或者黑，这16个就选成12 排除了大量的非角部。此测试仅检查1、9、5和13处的四个像素（如果第一个1和9太亮或太暗，则对其进行测试。如果是，则检查5和13）。如果$p$是一个角，那么至少其中三个必须比它亮或者暗。如果都不是，则$p$不能成为一个角落。然后，可以通过检查圆中的所有像素，将完整的分段测试标准应用于通过的候选项。该检测器本身具有很高的性能，但是存在一些缺点： 1.对于n &lt;12，它不会拒绝那么多候选。2.像素的选择不是最佳的，因为其效率取决于问题的顺序和角落外观的分布。3.高速测试的结果被丢弃了。4.彼此相邻地检测到多个特征。机器学习方法解决了前三点。使用非最大抑制来解决最后一个问题。 机器学习角检测器 选择一组图像进行训练（最好从目标应用程序域中进行训练） 在每个图像中运行FAST算法以查找特征点。 对于每个特征点，将其周围的16个像素存储为矢量。对所有图像执行此操作以获得特征向量P 定义一个新的布尔变量 使用ID3算法（决策树分类器）使用变量K查询每个子集 递归地将其应用于所有子集，直到其熵为零为止。 这样创建的决策树用于其他图像的快速检测。 非最大抑制在相邻位置检测多个兴趣点是另一个问题。通过使用非最大抑制来解决。 计算得分函数 考虑两个相邻的关键点并计算其 丢弃较低的 摘要它比其他现有的转角检测器快几倍。 但是它对于高水平的噪声不是很可靠。它取决于阈值。 OpenCV中的FAST功能检测器它被称为OpenCV中的任何其他特征检测器。如果需要，可以指定阈值，是否要应用非最大抑制，要使用的邻域等。 对于邻域，定义了三个标志，分别为cv.FAST_FEATURE_DETECTOR_TYPE_5_8，cv.FAST_FEATURE_DETECTOR_TYPE_7_12和cv.FAST_FEATURE_DETECTOR_TYPE_9_16。以下是有关如何检测和绘制FAST特征点的简单代码。 123456789101112131415161718192021import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('simple.jpg',0)# Initiate FAST object with default valuesfast = cv.FastFeatureDetector_create()# find and draw the keypointskp = fast.detect(img,None)img2 = cv.drawKeypoints(img, kp, None, color=(255,0,0))# Print all default paramsprint( "Threshold: &#123;&#125;".format(fast.getThreshold()) )print( "nonmaxSuppression:&#123;&#125;".format(fast.getNonmaxSuppression()) )print( "neighborhood: &#123;&#125;".format(fast.getType()) )print( "Total Keypoints with nonmaxSuppression: &#123;&#125;".format(len(kp)) )cv.imwrite('fast_true.png',img2)# Disable nonmaxSuppressionfast.setNonmaxSuppression(0)kp = fast.detect(img,None)print( "Total Keypoints without nonmaxSuppression: &#123;&#125;".format(len(kp)) )img3 = cv.drawKeypoints(img, kp, None, color=(255,0,0))cv.imwrite('fast_false.png',img3) 查看结果。第一张图片显示了带有nonmaxSuppression的FAST，第二张图片显示了没有nonmaxSuppression的FAST： BRIEF (Binary Robust Independent Elementary Features)目标在这一章当中 我们将看到Brief算法的基础 理论我们知道SIFT使用128维矢量作为描述符。由于它使用浮点数，因此基本上需要512个字节。同样，SURF最少也需要256个字节（用于64像素）。为数千个功能部件创建这样的向量会占用大量内存，这对于资源受限的应用程序尤其是嵌入式系统而言是不可行的。内存越大，匹配所需的时间越长。 但是实际匹配可能不需要所有这些尺寸。我们可以使用PCA，LDA等几种方法对其进行压缩。甚至使用LSH（局部敏感哈希）进行哈希的其他方法也可以将这些SIFT描述符中的浮点数转换为二进制字符串。这些二进制字符串用于使用汉明距离匹配要素。这提供了更好的速度，因为查找汉明距离仅是应用XOR和位数，这在具有SSE指令的现代CPU中非常快。但是在这里，我们需要先找到描述符，然后才可以应用散列，这不能解决我们最初的内存问题。 此刻简要介绍。它提供了一种直接查找二进制字符串而无需查找描述符的快捷方式。这需要平滑图像块，并选择一组Ñd（x，y）个位置对以独特的方式（用纸说明）。然后，在这些位置对上进行一些像素强度比较。 重要的一点是，BRIEF是特征描述符，它不提供任何查找特征的方法。因此，您将不得不使用任何其他特征检测器，例如SIFT，SURF等。本文建议使用CenSurE，它是一种快速检测器，并且BIM对于CenSurE点的工作原理甚至比对SURF点的工作要好一些。 简而言之，BRIEF是一种更快的方法特征描述符计算和匹配。除非平面内旋转较大，否则它还提供很高的识别率。 代码示例下面的代码显示了借助CenSurE检测器对Brief描述符的计算。（CenSurE检测器在OpenCV中称为STAR检测器） 1234567891011121314import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('simple.jpg',0)# Initiate FAST detectorstar = cv.xfeatures2d.StarDetector_create()# Initiate BRIEF extractorbrief = cv.xfeatures2d.BriefDescriptorExtractor_create()# find the keypoints with STARkp = star.detect(img,None)# compute the descriptors with BRIEFkp, des = brief.compute(img, kp)print( brief.descriptorSize() )print( des.shape ) ORB (Oriented FAST and Rotated BRIEF)目标在这一章当中， 我们将了解ORB的基础知识 理论作为OpenCV的狂热者，关于ORB的最重要的事情是它来自“ OpenCV Labs”。该算法由Ethan Rublee，Vincent Rabaud，Kurt Konolige和Gary R.Bradski在他们的论文ORB： 2011年SIFT或SURF的有效替代方案中提出。正如标题所述，它是计算中SIFT和SURF的良好替代方案成本，匹配性能以及主要是专利。是的，SIFT和SURF已获得专利，您应该为其使用付费。但是，ORB不是！ ORB基本上是FAST关键点检测器和Brief描述符的融合，并进行了许多修改以增强性能。首先，它使用FAST查找关键点，然后应用哈里斯角点度量在其中找到前N个点。它还使用金字塔生成多尺度特征。但是一个问题是，FAST无法计算方向。那么旋转不变性呢？作者提出以下修改。 它计算角点位于中心的贴片的强度加权质心。从此角点到质心的矢量方向确定了方向。为了改善旋转不变性，使用x和y计算矩，它们应该在半径r的圆形区域中 现在，对于描述符，ORB使用Brief描述符。但是我们已经看到，BRIEF的旋转性能很差。因此，ORB所做的就是根据关键点的方向“引导” BRIEF。 每个位特征具有较大的方差，且均值接近0.5。但是，一旦沿关键点方向定向，它就会失去此属性，变得更加分散。高方差使功能更具区分性，因为它对输入的响应不同。另一个理想的属性是使测试不相关，因为从那时起每个测试都会对结果有所贡献。为了解决所有这些问题，ORB在所有可能的二进制测试中进行贪婪搜索，以找到方差高且均值接近0.5且不相关的测试。结果称为rBRIEF。 对于描述符匹配，使用了对传统LSH进行改进的多探针LSH。该论文说，ORB比SURF快得多，SIFT和ORB描述符比SURF更好。对于全景拼接等低功耗设备，ORB是一个不错的选择。 OpenCV中的ORB与往常一样，我们必须使用函数cv.ORB()或使用feature2d通用接口来创建ORB对象。它具有许多可选参数。最有用的是nFeatures，它表示要保留的最大特征数（默认为500），scoreType表示是对特征进行排名的Harris分数还是FAST分数（默认为Harris分数）等。另一个参数WTA_K决定点数产生定向的BRIEF描述符的每个元素。默认情况下为两个，即一次选择两个点。在那种情况下，为了匹配，使用NORM_HAMMING距离。如果WTA_K为3或4，则需要3或4个点来生成Brief描述符，则匹配距离由NORM_HAMMING2定义。 以下是显示ORB用法的简单代码。 12345678910111213import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('simple.jpg',0)# Initiate ORB detectororb = cv.ORB_create()# find the keypoints with ORBkp = orb.detect(img,None)# compute the descriptors with ORBkp, des = orb.compute(img, kp)# draw only keypoints location,not size and orientationimg2 = cv.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)plt.imshow(img2), plt.show() 看到下面的结果： ORB的特征匹配，我们将在另一章中进行。 特征匹配目标在这一章当中 我们将看到如何将一幅图像中的特征与其他图像进行匹配。 我们将在OpenCV中使用蛮力匹配器和FLANN匹配器 蛮力匹配器的基础蛮力匹配器很简单。它采用第一组中一个特征的描述符，并使用一些距离计算将其与第二组中的所有其他特征匹配。并返回最接近的一个。 对于BF匹配器，首先我们必须使用cv.BFMatcher()创建BFMatcher对象。它需要两个可选参数。第一个是normType。它指定要使用的距离测量。默认情况下为cv.NORM_L2。对SIFT，SURF等（也有cv.NORM_L1）很有用。对于基于二进制字符串的描述符，例如ORB，BRIEF，BRISK等，应使用cv.NORM_HAMMING，该函数使用汉明距离作为度量。如果ORB使用WTA_K == 3或4，则应使用cv.NORM_HAMMING2。 第二个参数是布尔变量，即crossCheck，默认情况下为false。如果为true，则Matcher仅返回具有值（i，j）的那些匹配项，以使集合A中的第i个描述符具有集合B中的第j个描述符为最佳匹配，反之亦然。即，两组中的两个特征应彼此匹配。它提供了一致的结果，并且是D.Lowe在SIFT论文中提出的比率测试的良好替代方案。 创建之后，两个重要的方法是BFMatcher.match()和BFMatcher.knnMatch()。第一个返回最佳匹配。第二种方法返回k个最佳匹配，其中k由用户指定。当我们需要对此做其他工作时，它可能会很有用。 就像我们使用cv.drawKeypoints()绘制关键点一样，cv.drawMatches()可以帮助我们绘制匹配项。它水平堆叠两张图像，并绘制从第一张图像到第二张图像的线，以显示最佳匹配。还有cv.drawMatchesKnn绘制所有k个最佳匹配。如果k = 2，它将为每个关键点绘制两条匹配线。因此，如果要选择性地绘制遮罩，则必须通过遮罩。 让我们看一下SURF和ORB的一个示例（两者都使用不同的距离测量）。 使用ORB描述符进行蛮力匹配在这里，我们将看到一个有关如何在两个图像之间匹配特征的简单示例。在这种情况下，我有一个queryImage和trainImage。我们将尝试使用特征匹配在trainImage中找到queryImage。（图像为/samples/c/box.png和/samples/c/box_in_scene.png） 我们正在使用ORB描述符来匹配特征。因此，让我们从加载图像，查找描述符等开始。 12345678910import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg1 = cv.imread('box.png',cv.IMREAD_GRAYSCALE) # queryImageimg2 = cv.imread('box_in_scene.png',cv.IMREAD_GRAYSCALE) # trainImage# Initiate ORB detectororb = cv.ORB_create()# find the keypoints and descriptors with ORBkp1, des1 = orb.detectAndCompute(img1,None)kp2, des2 = orb.detectAndCompute(img2,None) 接下来，我们创建一个距离测量值为cv.NORM_HAMMING的BFMatcher对象（因为我们使用的是ORB），并且启用了crossCheck以获得更好的结果。然后，我们使用Matcher.match()方法来获取两个图像中的最佳匹配。我们按照距离的升序对它们进行排序，以使最佳匹配（低距离）排在最前面。然后我们只抽出前10场比赛（只是为了提高知名度。您可以根据需要增加它） 123456789# create BFMatcher objectbf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)# Match descriptors.matches = bf.match(des1,des2)# Sort them in the order of their distance.matches = sorted(matches, key = lambda x:x.distance)# Draw first 10 matches.img3 = cv.drawMatches(img1,kp1,img2,kp2,matches[:10],None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)plt.imshow(img3),plt.show() 以下是我得到的结果： 什么是Matcher对象？matchs = bf.match(des1,des2)行的结果是DMatch对象的列表。此DMatch对象具有以下属性： DMatch.distance-描述符之间的距离。越低越好。 DMatch.trainIdx-火车描述符中描述符的索引 DMatch.queryIdx-查询描述符中描述符的索引 DMatch.imgIdx-火车图像的索引。 具有SIFT描述符和比率测试的蛮力匹配这次，我们将使用BFMatcher.knnMatch()获得k个最佳匹配。在此示例中，我们将k = 2，以便可以应用D.Lowe在他的论文中解释的比率检验。 123456789101112131415161718192021import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg1 = cv.imread('box.png',cv.IMREAD_GRAYSCALE) # queryImageimg2 = cv.imread('box_in_scene.png',cv.IMREAD_GRAYSCALE) # trainImage# Initiate SIFT detectorsift = cv.xfeatures2d.SIFT_create()# find the keypoints and descriptors with SIFTkp1, des1 = sift.detectAndCompute(img1,None)kp2, des2 = sift.detectAndCompute(img2,None)# BFMatcher with default paramsbf = cv.BFMatcher()matches = bf.knnMatch(des1,des2,k=2)# Apply ratio testgood = []for m,n in matches: if m.distance &lt; 0.75*n.distance: good.append([m])# cv.drawMatchesKnn expects list of lists as matches.img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)plt.imshow(img3),plt.show() 看到下面的结果： 基于FLANN的匹配器FLANN代表大约最近邻居的快速库。它包含一组算法，这些算法针对大型数据集中的快速最近邻搜索和高维特征进行了优化。对于大型数据集，它的运行速度比BFMatcher快。我们将看到第二个基于FLANN的匹配器示例。 对于基于FLANN的匹配器，我们需要传递两个字典，这些字典指定要使用的算法及其相关参数等。第一个是IndexParams。对于各种算法，要传递的信息在FLANN文档中进行了说明。概括来说，对于SIFT，SURF等算法，您可以通过以下操作： 12FLANN_INDEX_KDTREE = 1index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5) 使用ORB时，您可以通过以下内容。根据文档建议使用带注释的值，但在某些情况下未提供必需的结果。其他值工作正常。： 12345FLANN_INDEX_LSH = 6index_params= dict(algorithm = FLANN_INDEX_LSH, table_number = 6, # 12 key_size = 12, # 20 multi_probe_level = 1) #2 第二个字典是SearchParams。它指定索引中的树应递归遍历的次数。较高的值可提供更好的精度，但也需要更多时间。如果要更改值，请传递search_params = dict（checks = 100）。 有了这些信息，我们就很高兴了。 12345678910111213141516171819202122232425262728import numpy as npimport cv2 as cvimport matplotlib.pyplot as pltimg1 = cv.imread('box.png',cv.IMREAD_GRAYSCALE) # queryImageimg2 = cv.imread('box_in_scene.png',cv.IMREAD_GRAYSCALE) # trainImage# Initiate SIFT detectorsift = cv.xfeatures2d.SIFT_create()# find the keypoints and descriptors with SIFTkp1, des1 = sift.detectAndCompute(img1,None)kp2, des2 = sift.detectAndCompute(img2,None)# FLANN parametersFLANN_INDEX_KDTREE = 1index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)search_params = dict(checks=50) # or pass empty dictionaryflann = cv.FlannBasedMatcher(index_params,search_params)matches = flann.knnMatch(des1,des2,k=2)# Need to draw only good matches, so create a maskmatchesMask = [[0,0] for i in range(len(matches))]# ratio test as per Lowe's paperfor i,(m,n) in enumerate(matches): if m.distance &lt; 0.7*n.distance: matchesMask[i]=[1,0]draw_params = dict(matchColor = (0,255,0), singlePointColor = (255,0,0), matchesMask = matchesMask, flags = cv.DrawMatchesFlags_DEFAULT)img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)plt.imshow(img3,),plt.show() 看到下面的结果： 特征匹配+单应性查找对象目标在这一章当中， 我们将calib3d模块中的特征匹配和findHomography混合在一起，以在复杂图像中找到已知对象。 基础那么我们在上一章上做了什么？我们使用了queryImage，找到了其中的一些特征点，我们使用了另一个trainImage，也找到了该图像中的特征，并且找到了其中的最佳匹配。简而言之，我们在另一个混乱的图像中找到了对象某些部分的位置。此信息足以在trainImage上准确找到对象。 为此，我们可以使用calib3d模块中的函数，即cv.findHomography()。如果我们从两个图像中传递点集，它将找到该对象的透视变换。然后，我们可以使用cv.perspectiveTransform()查找对象。找到转换至少需要四个正确的点。 我们已经看到，匹配时可能会出现一些可能影响结果的错误。为了解决这个问题，算法使用RANSAC或LEAST_MEDIAN（可以由标志决定）。因此，提供正确估计的良好匹配称为“异常值”，其余的称为“异常值”。cv.findHomography()返回指定内部和外部点的掩码。 因此，让我们开始吧！ 示例代码首先，像往常一样，让我们在图像中找到SIFT功能并应用比率测试以找到最佳匹配。 123456789101112131415161718192021import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltMIN_MATCH_COUNT = 10img1 = cv.imread('box.png',0) # queryImageimg2 = cv.imread('box_in_scene.png',0) # trainImage# Initiate SIFT detectorsift = cv.xfeatures2d.SIFT_create()# find the keypoints and descriptors with SIFTkp1, des1 = sift.detectAndCompute(img1,None)kp2, des2 = sift.detectAndCompute(img2,None)FLANN_INDEX_KDTREE = 1index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)search_params = dict(checks = 50)flann = cv.FlannBasedMatcher(index_params, search_params)matches = flann.knnMatch(des1,des2,k=2)# store all the good matches as per Lowe's ratio test.good = []for m,n in matches: if m.distance &lt; 0.7*n.distance: good.append(m) 现在我们设置一个条件，即至少有10个匹配项（由MIN_MATCH_COUNT定义）可以找到对象。否则，只需显示一条消息，说明没有足够的匹配项。 如果找到足够的匹配项，我们将在两个图像中提取匹配的关键点的位置。他们被传递以寻找预期的转变。一旦获得此3x3转换矩阵，就可以使用它将queryImage的角转换为trainImage中的相应点。然后我们画出来。 123456789101112if len(good)&gt;MIN_MATCH_COUNT: src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2) dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2) M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0) matchesMask = mask.ravel().tolist() h,w,d = img1.shape pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2) dst = cv.perspectiveTransform(pts,M) img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)else: print( "Not enough matches are found - &#123;&#125;/&#123;&#125;".format(len(good), MIN_MATCH_COUNT) ) matchesMask = None 最后，我们绘制轮廓线（如果成功找到对象）或匹配关键点（如果失败）。 123456draw_params = dict(matchColor = (0,255,0), # draw matches in green color singlePointColor = None, matchesMask = matchesMask, # draw only inliers flags = 2)img3 = cv.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)plt.imshow(img3, 'gray'),plt.show() 请参阅下面的结果。对象在混乱的图像中标记为白色：]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV中的图像处理]]></title>
    <url>%2Fopencv%2FOpenCV%E4%B8%AD%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[改变色彩空间目标 在本教程中，您将学习如何将图像从一种颜色空间转换为另一种颜色空间，例如BGR ↔ Gray，BGR ↔ HSV等。 除此之外，我们还将创建一个应用程序，以提取视频中的彩色对象 您将学习以下功能：cv.cvtColor()，cv.inRange()等 改变色彩空间OpenCV提供了150多种颜色空间转换方法。但是，我们将只研究两种使用最广泛的工具，即BGR ↔ Gray和BGR ↔ HSV。 对于颜色转换，我们使用函数cv.cvtColor（input_image，flag），其中flag确定转换的类型。 对于BGR → Gray转换，我们使用标志cv.COLOR_BGR2GRAY。同样，对于BGR → HSV，我们使用标志cv.COLOR_BGR2HSV。要获取其他标志，只需在Python终端中运行以下命令： 123import cv2 as cvflags = [i for i in dir(cv) if i.startswith('COLOR_')]print(flags) 1[&apos;COLOR_BAYER_BG2BGR&apos;, &apos;COLOR_BAYER_BG2BGRA&apos;, &apos;COLOR_BAYER_BG2BGR_EA&apos;, &apos;COLOR_BAYER_BG2BGR_VNG&apos;, &apos;COLOR_BAYER_BG2GRAY&apos;, &apos;COLOR_BAYER_BG2RGB&apos;, &apos;COLOR_BAYER_BG2RGBA&apos;, &apos;COLOR_BAYER_BG2RGB_EA&apos;, &apos;COLOR_BAYER_BG2RGB_VNG&apos;, &apos;COLOR_BAYER_GB2BGR&apos;, &apos;COLOR_BAYER_GB2BGRA&apos;, &apos;COLOR_BAYER_GB2BGR_EA&apos;, &apos;COLOR_BAYER_GB2BGR_VNG&apos;, &apos;COLOR_BAYER_GB2GRAY&apos;, &apos;COLOR_BAYER_GB2RGB&apos;, &apos;COLOR_BAYER_GB2RGBA&apos;, &apos;COLOR_BAYER_GB2RGB_EA&apos;, &apos;COLOR_BAYER_GB2RGB_VNG&apos;, &apos;COLOR_BAYER_GR2BGR&apos;, &apos;COLOR_BAYER_GR2BGRA&apos;, &apos;COLOR_BAYER_GR2BGR_EA&apos;, &apos;COLOR_BAYER_GR2BGR_VNG&apos;, &apos;COLOR_BAYER_GR2GRAY&apos;, &apos;COLOR_BAYER_GR2RGB&apos;, &apos;COLOR_BAYER_GR2RGBA&apos;, &apos;COLOR_BAYER_GR2RGB_EA&apos;, &apos;COLOR_BAYER_GR2RGB_VNG&apos;, &apos;COLOR_BAYER_RG2BGR&apos;, &apos;COLOR_BAYER_RG2BGRA&apos;, &apos;COLOR_BAYER_RG2BGR_EA&apos;, &apos;COLOR_BAYER_RG2BGR_VNG&apos;, &apos;COLOR_BAYER_RG2GRAY&apos;, &apos;COLOR_BAYER_RG2RGB&apos;, &apos;COLOR_BAYER_RG2RGBA&apos;, &apos;COLOR_BAYER_RG2RGB_EA&apos;, &apos;COLOR_BAYER_RG2RGB_VNG&apos;, &apos;COLOR_BGR2BGR555&apos;, &apos;COLOR_BGR2BGR565&apos;, &apos;COLOR_BGR2BGRA&apos;, &apos;COLOR_BGR2GRAY&apos;, &apos;COLOR_BGR2HLS&apos;, &apos;COLOR_BGR2HLS_FULL&apos;, &apos;COLOR_BGR2HSV&apos;, &apos;COLOR_BGR2HSV_FULL&apos;, &apos;COLOR_BGR2LAB&apos;, &apos;COLOR_BGR2LUV&apos;, &apos;COLOR_BGR2Lab&apos;, &apos;COLOR_BGR2Luv&apos;, &apos;COLOR_BGR2RGB&apos;, &apos;COLOR_BGR2RGBA&apos;, &apos;COLOR_BGR2XYZ&apos;, &apos;COLOR_BGR2YCR_CB&apos;, &apos;COLOR_BGR2YCrCb&apos;, &apos;COLOR_BGR2YUV&apos;, &apos;COLOR_BGR2YUV_I420&apos;, &apos;COLOR_BGR2YUV_IYUV&apos;, &apos;COLOR_BGR2YUV_YV12&apos;, &apos;COLOR_BGR5552BGR&apos;, &apos;COLOR_BGR5552BGRA&apos;, &apos;COLOR_BGR5552GRAY&apos;, &apos;COLOR_BGR5552RGB&apos;, &apos;COLOR_BGR5552RGBA&apos;, &apos;COLOR_BGR5652BGR&apos;, &apos;COLOR_BGR5652BGRA&apos;, &apos;COLOR_BGR5652GRAY&apos;, &apos;COLOR_BGR5652RGB&apos;, &apos;COLOR_BGR5652RGBA&apos;, &apos;COLOR_BGRA2BGR&apos;, &apos;COLOR_BGRA2BGR555&apos;, &apos;COLOR_BGRA2BGR565&apos;, &apos;COLOR_BGRA2GRAY&apos;, &apos;COLOR_BGRA2RGB&apos;, &apos;COLOR_BGRA2RGBA&apos;, &apos;COLOR_BGRA2YUV_I420&apos;, &apos;COLOR_BGRA2YUV_IYUV&apos;, &apos;COLOR_BGRA2YUV_YV12&apos;, &apos;COLOR_BayerBG2BGR&apos;, &apos;COLOR_BayerBG2BGRA&apos;, &apos;COLOR_BayerBG2BGR_EA&apos;, &apos;COLOR_BayerBG2BGR_VNG&apos;, &apos;COLOR_BayerBG2GRAY&apos;, &apos;COLOR_BayerBG2RGB&apos;, &apos;COLOR_BayerBG2RGBA&apos;, &apos;COLOR_BayerBG2RGB_EA&apos;, &apos;COLOR_BayerBG2RGB_VNG&apos;, &apos;COLOR_BayerGB2BGR&apos;, &apos;COLOR_BayerGB2BGRA&apos;, &apos;COLOR_BayerGB2BGR_EA&apos;, &apos;COLOR_BayerGB2BGR_VNG&apos;, &apos;COLOR_BayerGB2GRAY&apos;, &apos;COLOR_BayerGB2RGB&apos;, &apos;COLOR_BayerGB2RGBA&apos;, &apos;COLOR_BayerGB2RGB_EA&apos;, &apos;COLOR_BayerGB2RGB_VNG&apos;, &apos;COLOR_BayerGR2BGR&apos;, &apos;COLOR_BayerGR2BGRA&apos;, &apos;COLOR_BayerGR2BGR_EA&apos;, &apos;COLOR_BayerGR2BGR_VNG&apos;, &apos;COLOR_BayerGR2GRAY&apos;, &apos;COLOR_BayerGR2RGB&apos;, &apos;COLOR_BayerGR2RGBA&apos;, &apos;COLOR_BayerGR2RGB_EA&apos;, &apos;COLOR_BayerGR2RGB_VNG&apos;, &apos;COLOR_BayerRG2BGR&apos;, &apos;COLOR_BayerRG2BGRA&apos;, &apos;COLOR_BayerRG2BGR_EA&apos;, &apos;COLOR_BayerRG2BGR_VNG&apos;, &apos;COLOR_BayerRG2GRAY&apos;, &apos;COLOR_BayerRG2RGB&apos;, &apos;COLOR_BayerRG2RGBA&apos;, &apos;COLOR_BayerRG2RGB_EA&apos;, &apos;COLOR_BayerRG2RGB_VNG&apos;, &apos;COLOR_COLORCVT_MAX&apos;, &apos;COLOR_GRAY2BGR&apos;, &apos;COLOR_GRAY2BGR555&apos;, &apos;COLOR_GRAY2BGR565&apos;, &apos;COLOR_GRAY2BGRA&apos;, &apos;COLOR_GRAY2RGB&apos;, &apos;COLOR_GRAY2RGBA&apos;, &apos;COLOR_HLS2BGR&apos;, &apos;COLOR_HLS2BGR_FULL&apos;, &apos;COLOR_HLS2RGB&apos;, &apos;COLOR_HLS2RGB_FULL&apos;, &apos;COLOR_HSV2BGR&apos;, &apos;COLOR_HSV2BGR_FULL&apos;, &apos;COLOR_HSV2RGB&apos;, &apos;COLOR_HSV2RGB_FULL&apos;, &apos;COLOR_LAB2BGR&apos;, &apos;COLOR_LAB2LBGR&apos;, &apos;COLOR_LAB2LRGB&apos;, &apos;COLOR_LAB2RGB&apos;, &apos;COLOR_LBGR2LAB&apos;, &apos;COLOR_LBGR2LUV&apos;, &apos;COLOR_LBGR2Lab&apos;, &apos;COLOR_LBGR2Luv&apos;, &apos;COLOR_LRGB2LAB&apos;, &apos;COLOR_LRGB2LUV&apos;, &apos;COLOR_LRGB2Lab&apos;, &apos;COLOR_LRGB2Luv&apos;, &apos;COLOR_LUV2BGR&apos;, &apos;COLOR_LUV2LBGR&apos;, &apos;COLOR_LUV2LRGB&apos;, &apos;COLOR_LUV2RGB&apos;, &apos;COLOR_Lab2BGR&apos;, &apos;COLOR_Lab2LBGR&apos;, &apos;COLOR_Lab2LRGB&apos;, &apos;COLOR_Lab2RGB&apos;, &apos;COLOR_Luv2BGR&apos;, &apos;COLOR_Luv2LBGR&apos;, &apos;COLOR_Luv2LRGB&apos;, &apos;COLOR_Luv2RGB&apos;, &apos;COLOR_M_RGBA2RGBA&apos;, &apos;COLOR_RGB2BGR&apos;, &apos;COLOR_RGB2BGR555&apos;, &apos;COLOR_RGB2BGR565&apos;, &apos;COLOR_RGB2BGRA&apos;, &apos;COLOR_RGB2GRAY&apos;, &apos;COLOR_RGB2HLS&apos;, &apos;COLOR_RGB2HLS_FULL&apos;, &apos;COLOR_RGB2HSV&apos;, &apos;COLOR_RGB2HSV_FULL&apos;, &apos;COLOR_RGB2LAB&apos;, &apos;COLOR_RGB2LUV&apos;, &apos;COLOR_RGB2Lab&apos;, &apos;COLOR_RGB2Luv&apos;, &apos;COLOR_RGB2RGBA&apos;, &apos;COLOR_RGB2XYZ&apos;, &apos;COLOR_RGB2YCR_CB&apos;, &apos;COLOR_RGB2YCrCb&apos;, &apos;COLOR_RGB2YUV&apos;, &apos;COLOR_RGB2YUV_I420&apos;, &apos;COLOR_RGB2YUV_IYUV&apos;, &apos;COLOR_RGB2YUV_YV12&apos;, &apos;COLOR_RGBA2BGR&apos;, &apos;COLOR_RGBA2BGR555&apos;, &apos;COLOR_RGBA2BGR565&apos;, &apos;COLOR_RGBA2BGRA&apos;, &apos;COLOR_RGBA2GRAY&apos;, &apos;COLOR_RGBA2M_RGBA&apos;, &apos;COLOR_RGBA2RGB&apos;, &apos;COLOR_RGBA2YUV_I420&apos;, &apos;COLOR_RGBA2YUV_IYUV&apos;, &apos;COLOR_RGBA2YUV_YV12&apos;, &apos;COLOR_RGBA2mRGBA&apos;, &apos;COLOR_XYZ2BGR&apos;, &apos;COLOR_XYZ2RGB&apos;, &apos;COLOR_YCR_CB2BGR&apos;, &apos;COLOR_YCR_CB2RGB&apos;, &apos;COLOR_YCrCb2BGR&apos;, &apos;COLOR_YCrCb2RGB&apos;, &apos;COLOR_YUV2BGR&apos;, &apos;COLOR_YUV2BGRA_I420&apos;, &apos;COLOR_YUV2BGRA_IYUV&apos;, &apos;COLOR_YUV2BGRA_NV12&apos;, &apos;COLOR_YUV2BGRA_NV21&apos;, &apos;COLOR_YUV2BGRA_UYNV&apos;, &apos;COLOR_YUV2BGRA_UYVY&apos;, &apos;COLOR_YUV2BGRA_Y422&apos;, &apos;COLOR_YUV2BGRA_YUNV&apos;, &apos;COLOR_YUV2BGRA_YUY2&apos;, &apos;COLOR_YUV2BGRA_YUYV&apos;, &apos;COLOR_YUV2BGRA_YV12&apos;, &apos;COLOR_YUV2BGRA_YVYU&apos;, &apos;COLOR_YUV2BGR_I420&apos;, &apos;COLOR_YUV2BGR_IYUV&apos;, &apos;COLOR_YUV2BGR_NV12&apos;, &apos;COLOR_YUV2BGR_NV21&apos;, &apos;COLOR_YUV2BGR_UYNV&apos;, &apos;COLOR_YUV2BGR_UYVY&apos;, &apos;COLOR_YUV2BGR_Y422&apos;, &apos;COLOR_YUV2BGR_YUNV&apos;, &apos;COLOR_YUV2BGR_YUY2&apos;, &apos;COLOR_YUV2BGR_YUYV&apos;, &apos;COLOR_YUV2BGR_YV12&apos;, &apos;COLOR_YUV2BGR_YVYU&apos;, &apos;COLOR_YUV2GRAY_420&apos;, &apos;COLOR_YUV2GRAY_I420&apos;, &apos;COLOR_YUV2GRAY_IYUV&apos;, &apos;COLOR_YUV2GRAY_NV12&apos;, &apos;COLOR_YUV2GRAY_NV21&apos;, &apos;COLOR_YUV2GRAY_UYNV&apos;, &apos;COLOR_YUV2GRAY_UYVY&apos;, &apos;COLOR_YUV2GRAY_Y422&apos;, &apos;COLOR_YUV2GRAY_YUNV&apos;, &apos;COLOR_YUV2GRAY_YUY2&apos;, &apos;COLOR_YUV2GRAY_YUYV&apos;, &apos;COLOR_YUV2GRAY_YV12&apos;, &apos;COLOR_YUV2GRAY_YVYU&apos;, &apos;COLOR_YUV2RGB&apos;, &apos;COLOR_YUV2RGBA_I420&apos;, &apos;COLOR_YUV2RGBA_IYUV&apos;, &apos;COLOR_YUV2RGBA_NV12&apos;, &apos;COLOR_YUV2RGBA_NV21&apos;, &apos;COLOR_YUV2RGBA_UYNV&apos;, &apos;COLOR_YUV2RGBA_UYVY&apos;, &apos;COLOR_YUV2RGBA_Y422&apos;, &apos;COLOR_YUV2RGBA_YUNV&apos;, &apos;COLOR_YUV2RGBA_YUY2&apos;, &apos;COLOR_YUV2RGBA_YUYV&apos;, &apos;COLOR_YUV2RGBA_YV12&apos;, &apos;COLOR_YUV2RGBA_YVYU&apos;, &apos;COLOR_YUV2RGB_I420&apos;, &apos;COLOR_YUV2RGB_IYUV&apos;, &apos;COLOR_YUV2RGB_NV12&apos;, &apos;COLOR_YUV2RGB_NV21&apos;, &apos;COLOR_YUV2RGB_UYNV&apos;, &apos;COLOR_YUV2RGB_UYVY&apos;, &apos;COLOR_YUV2RGB_Y422&apos;, &apos;COLOR_YUV2RGB_YUNV&apos;, &apos;COLOR_YUV2RGB_YUY2&apos;, &apos;COLOR_YUV2RGB_YUYV&apos;, &apos;COLOR_YUV2RGB_YV12&apos;, &apos;COLOR_YUV2RGB_YVYU&apos;, &apos;COLOR_YUV420P2BGR&apos;, &apos;COLOR_YUV420P2BGRA&apos;, &apos;COLOR_YUV420P2GRAY&apos;, &apos;COLOR_YUV420P2RGB&apos;, &apos;COLOR_YUV420P2RGBA&apos;, &apos;COLOR_YUV420SP2BGR&apos;, &apos;COLOR_YUV420SP2BGRA&apos;, &apos;COLOR_YUV420SP2GRAY&apos;, &apos;COLOR_YUV420SP2RGB&apos;, &apos;COLOR_YUV420SP2RGBA&apos;, &apos;COLOR_YUV420p2BGR&apos;, &apos;COLOR_YUV420p2BGRA&apos;, &apos;COLOR_YUV420p2GRAY&apos;, &apos;COLOR_YUV420p2RGB&apos;, &apos;COLOR_YUV420p2RGBA&apos;, &apos;COLOR_YUV420sp2BGR&apos;, &apos;COLOR_YUV420sp2BGRA&apos;, &apos;COLOR_YUV420sp2GRAY&apos;, &apos;COLOR_YUV420sp2RGB&apos;, &apos;COLOR_YUV420sp2RGBA&apos;, &apos;COLOR_mRGBA2RGBA&apos;] 注意:对于HSV，色相范围为[0,179]，饱和度范围为[0,255]，值范围为[0,255]。不同的软件使用不同的比例。因此，如果将OpenCV值与它们进行比较，则需要将这些范围标准化。 对象追踪现在我们知道了如何将BGR图像转换为HSV，我们可以使用它来提取彩色对象。在HSV中，表示颜色比在BGR颜色空间中更容易。在我们的应用程序中，我们将尝试提取一个蓝色的对象。所以这是方法： 拍摄视频的每一帧 从BGR转换为HSV颜色空间 我们将HSV图片的阈值范围设为蓝色 现在，仅提取蓝色对象，我们就可以在所需图像上执行任何操作。以下是详细注释的代码： 1234567891011121314151617181920212223242526272829import cv2 as cvimport numpy as npcap = cv.VideoCapture(0)while(1): _,frame = cap.read() # Convert BGR to HSV hsv = cv.cvtColor(frame,cv.COLOR_BGR2HSV) # define range of blue color in HSV lower_blue = np.array([110,50,50]) upper_blue = np.array([130,255,255]) # Threshold the HSV image to get only blue colors mask = cv.inRange(hsv,lower_blue,upper_blue) # Bitwise-AND mask and original image res = cv.bitwise_and(frame,frame,mask=mask) cv.imshow('frame',frame) cv.imshow('mask',mask) cv.imshow('res',res) k = cv.waitKey(5) &amp; 0xFF if k == 27: breakcv.destroyAllWindows() 显示如下： 注意:图像中有一些噪点。我们将在后面的章节中看到如何删除它们。这是对象跟踪中最简单的方法。一旦学习了轮廓功能，您就可以做很多事情，例如找到该对象的质心并使用它来跟踪该对象，仅通过将手移到相机前面以及其他许多有趣的东西就可以绘制图表。 如何找到要追踪的HSV值？这是在stackoverflow.com中发现的常见问题。这非常简单，您可以使用相同的函数cv.cvtColor()。无需传递图像，只需传递所需的BGR值即可。例如，要查找Green的HSV值，请在Python终端中尝试以下命令： 123456import cv2 as cvimport numpy as npgreen = np.uint8([[[0,255,0]]])hsv_green = cv.cvtColor(green,cv.COLOR_BGR2HSV)print(hsv_green) 1[[[ 60 255 255]]] 图像的几何变换目标 学习将不同的几何变换应用于图像，例如平移，旋转，仿射变换等。 您将看到以下功能：cv.getPerspectiveTransform 转换OpenCV提供了两个转换函数cv.warpAffine和cv.warpPerspective，您可以使用它们进行各种转换。cv.warpAffine采用2x3转换矩阵，而cv.warpPerspective采用3x3转换矩阵作为输入。 缩放缩放只是调整图像的大小。为此，OpenCV带有一个函数cv.resize()。图像的大小可以手动指定，也可以指定缩放比例。使用了不同的插值方法。首选插值方法是cv.INTER_AREA用于缩小，cv.INTER_CUBIC（slow）和cv.INTER_LINEAR用于缩放。默认情况下，出于所有调整大小的目的，使用的插值方法为cv.INTER_LINEAR。您可以使用以下方法之一调整输入图像的大小： 1234567891011import numpy as npimport cv2 as cvimg = cv.imread('lena.jpg')res = cv.resize(img,None,fx=2, fy=2, interpolation = cv.INTER_CUBIC)#ORheight, width = img.shape[:2]res = cv.resize(img,(2*width, 2*height), interpolation = cv.INTER_CUBIC)cv.imshow('resize',res)cv.waitKey(0)cv.destroyAllWindows() 平移平移是物体位置的移动您可以将其放入np.float32类型的Numpy数组中，并将其传递给cv.warpAffine函数 123456789import numpy as npimport cv2 as cvimg = cv.imread('lena.jpg',0) # 改成1会报错rows,cols = img.shapeM = np.float32([[1,0,100],[0,1,50]])dst = cv.warpAffine(img,M,(cols,rows))cv.imshow('img',dst)cv.waitKey(0)cv.destroyAllWindows() 旋转OpenCV提供了可缩放的旋转以及可调整的旋转中心，因此您可以在自己喜欢的任何位置旋转。为了找到此转换矩阵，OpenCV提供了一个函数cv.getRotationMatrix2D。请检查以下示例，该示例将图像相对于中心旋转90度而没有任何缩放比例。 123456789img = cv.imread('lena.jpg',0) # 还是不能改成1rows,cols = img.shape# cols-1 and rows-1 are the coordinate limits.M = cv.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),90,1)dst = cv.warpAffine(img,M,(cols,rows))cv.imshow('rotation',dst)cv.waitKey(0)cv.destroyAllWindows() 仿射变换在仿射变换中，原始图像中的所有平行线在输出图像中仍将平行。为了找到变换矩阵，我们需要输入图像中的三个点及其在输出图像中的对应位置。然后cv.getAffineTransform将创建一个2x3矩阵，该矩阵将传递给cv.warpAffine 123456789101112import cv2 as cvimport matplotlib.pyplot as pltimg = cv.imread('lena.jpg')rows,cols,ch = img.shapepts1 = np.float32([[50,50],[200,50],[50,200]])pts2 = np.float32([[10,100],[200,50],[100,250]])M = cv.getAffineTransform(pts1,pts2)dst = cv.warpAffine(img,M,(cols,rows))plt.subplot(121),plt.imshow(img),plt.title('Input')plt.subplot(122),plt.imshow(dst),plt.title('Output')plt.show() 透视变换对于透视变换，您需要3x3变换矩阵。即使在转换后，直线也将保持直线。要找到此变换矩阵，您需要在输入图像上有4个点，在输出图像上需要相应的点。在这四个点中，其中三个不应共线。然后可以通过函数cv.getPerspectiveTransform找到变换矩阵。然后将cv.warpPerspective应用于此3x3转换矩阵。 123456789img = cv.imread('lena.jpg')rows,cols,ch = img.shapepts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])M = cv.getPerspectiveTransform(pts1,pts2)dst = cv.warpPerspective(img,M,(300,300))plt.subplot(121),plt.imshow(img),plt.title('Input')plt.subplot(122),plt.imshow(dst),plt.title('Output')plt.show() 图像阈值目标 在本教程中，您将学习简单阈值，自适应阈值和Otsu的阈值。 您将学习函数cv.threshold和cv.adaptiveThreshold。 简单阈值在这里，问题直截了当。对于每个像素，应用相同的阈值。如果像素值小于阈值，则将其设置为0，否则将其设置为最大值。函数cv.threshold用于应用阈值。第一个参数是源图像，它应该是灰度图像。第二个参数是阈值，用于对像素值进行分类。第三个参数是分配给超过阈值的像素值的最大值。OpenCV提供了不同类型的阈值，这由函数的第四个参数给出。通过使用类型cv.THRESH_BINARY完成上述基本阈值处理。所有简单的阈值类型为： cv.THRESH_BINARY cv.THRESH_BINARY_INV cv.THRESH_TRUNC cv.THRESH_TOZERO cv.THRESH_TOZERO_INV有关差异，请参见类型的文档。 该方法返回两个输出。第一个是使用的阈值，第二个输出是阈值图像。 此代码比较了不同的简单阈值类型： 12345678910111213141516import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltimg = cv.imread('lena.jpg',0)ret,thresh1 = cv.threshold(img,127,255,cv.THRESH_BINARY)ret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]for i in range(6): plt.subplot(2,3,i+1),plt.imshow(images[i],'gray') plt.title(titles[i]) plt.xticks([]),plt.yticks([])plt.show() 自适应阈值在上一节中，我们使用一个全局值作为阈值。但这可能并非在所有情况下都很好，例如，如果图像在不同区域具有不同的照明条件。在这种情况下，自适应阈值阈值化可以提供帮助。在此，算法基于像素周围的小区域确定像素的阈值。因此，对于同一图像的不同区域，我们获得了不同的阈值，这为光照度变化的图像提供了更好的结果。 除上述参数外，方法cv.adaptiveThreshold还包含三个输入参数： 该adaptiveMethod决定阈值是如何计算的： cv.ADAPTIVE_THRESH_MEAN_C：该阈值是该附近区域减去恒定的平均Ç。cv.ADAPTIVE_THRESH_GAUSSIAN_C：阈值是邻域值减去常数C的高斯加权和。该BLOCKSIZE确定附近区域的大小和Ç是从平均值或附近的像素的加权和中减去一个常数。 下面的代码比较了光照变化的图像的全局阈值和自适应阈值： 123456789101112131415161718import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltimg = cv.imread('lena.jpg',0)img = cv.medianBlur(img,5)ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\ cv.THRESH_BINARY,11,2)th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\ cv.THRESH_BINARY,11,2)titles = ['Original Image', 'Global Thresholding (v = 127)', 'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']images = [img, th1, th2, th3]for i in range(4): plt.subplot(2,2,i+1),plt.imshow(images[i],'gray') plt.title(titles[i]) plt.xticks([]),plt.yticks([])plt.show() Otsu’s Binarization在全局阈值化中，我们使用任意选择的值作为阈值。相反，Otsu的方法避免了必须选择一个值并自动确定它的情况。 考虑仅具有两个不同图像值的图像（双峰图像），其中直方图将仅包含两个峰。一个好的阈值应该在这两个值的中间。类似地，Otsu的方法从图像直方图中确定最佳全局阈值。 为此，使用了cv.threshold()函数，其中cv.THRESH_OTSU作为附加标志传递。阈值可以任意选择。然后，算法找到最佳阈值，该阈值作为第一输出返回。 查看以下示例。输入图像为噪点图像。在第一种情况下，将应用值127的全局阈值。在第二种情况下，将直接应用Otsu的阈值。在第三种情况下，首先使用5x5高斯核对图像进行滤波以去除噪声，然后应用Otsu阈值处理。了解噪声过滤如何改善结果。 1234567891011121314151617181920212223242526import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltimg = cv.imread('noisy2.png',0)# global thresholdingret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)# Otsu's thresholdingret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)# Otsu's thresholding after Gaussian filteringblur = cv.GaussianBlur(img,(5,5),0)ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)# plot all the images and their histogramsimages = [img, 0, th1, img, 0, th2, blur, 0, th3]titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)', 'Original Noisy Image','Histogram',"Otsu's Thresholding", 'Gaussian filtered Image','Histogram',"Otsu's Thresholding"]for i in range(3): plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray') plt.title(titles[i*3]), plt.xticks([]), plt.yticks([]) plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256) plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([]) plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray') plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])plt.show() 平滑图像目标 使用各种低通滤镜模糊图像 将定制的滤镜应用于图像（2D卷积） 2D卷积（图像过滤）与一维信号一样，还可以使用各种低通滤波器（LPF），高通滤波器（HPF）等对图像进行滤波。LPF有助于消除噪声，使图像模糊等。HPF滤波器有助于在图像中找到边缘。图片。 OpenCV提供了一个函数cv.filter2D()来将内核与映像进行卷积。例如，我们将尝试对图像进行平均滤波。操作如下：将内核保持在一个像素以上，将所有25个像素加到该内核以下，取其平均值，然后用新的平均值替换中心像素。对于图像中的所有像素，它将继续此操作。尝试以下代码并检查结果： 123456789101112import numpy as npimport cv2 as cv# %matplotlib inlinefrom matplotlib import pyplot as pltimg = cv.imread('original.jpg')kernel = np.ones((5,5),np.float32)/25dst = cv.filter2D(img,-1,kernel)plt.subplot(121),plt.imshow(img),plt.title('Original')plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(dst),plt.title('Averaging')plt.xticks([]), plt.yticks([])plt.show() 图像模糊（图像平滑）通过将图像与低通滤波器内核进行卷积来实现图像模糊。这对于消除噪音很有用。它实际上从图像中删除了高频内容（例如，噪声，边缘）。因此，在此操作中边缘有些模糊。（嗯，有一些模糊技术也不会模糊边缘）。OpenCV主要提供四种类型的模糊技术。 1.平均这是通过将图像与归一化框滤镜进行卷积来完成的。它仅获取内核区域下所有像素的平均值，并替换中心元素。这是通过功能cv.blur()或cv.boxFilter()完成的。检查文档以获取有关内核的更多详细信息。 注意:如果您不想使用标准化的框式过滤器，请使用cv.boxFilter（）。将参数normalize = False传递给函数。 12345678910import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltimg = cv.imread('original.jpg')blur = cv.blur(img,(5,5))plt.subplot(121),plt.imshow(img),plt.title('Original')plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(blur),plt.title('Blurred')plt.xticks([]), plt.yticks([])plt.show() 2.高斯模糊在这种情况下，代替盒式滤波器，使用了高斯核。这是通过功能cv.GaussianBlur()完成的。我们应指定内核的宽度和高度，该宽度和高度应为正数和奇数。我们还应指定X和Y方向的标准偏差，分别为sigmaX和sigmaY。如果仅指定sigmaX，则将sigmaY与sigmaX相同。如果两个都为零，则根据内核大小进行计算。高斯模糊对于从图像中去除高斯噪声非常有效。 如果需要，可以使用函数cv.getGaussianKernel()创建高斯内核。 可以针对高斯模糊修改以上代码： 1blur = cv.GaussianBlur（img，（5,5），0） 3.中位模糊在这里，函数cv.medianBlur()提取内核区域下所有像素的中值，并将中心元素替换为该中值。这对于消除图像中的椒盐噪声非常有效。有趣的是，在上述过滤器中，中心元素是新计算的值，该值可以是图像中的像素值或新值。但是在中值模糊中，中心元素总是被图像中的某些像素值代替。有效降低噪音。其内核大小应为正奇数整数。 1median = cv.medianBlur(img,5) 4.双边过滤cv.bilateralFilter()在去除噪声的同时保持边缘清晰锐利非常有效。但是，与其他过滤器相比，该操作速度较慢。我们已经看到，高斯滤波器采用像素周围的邻域并找到其高斯加权平均值。高斯滤波器仅是空间的函数，也就是说，滤波时会考虑附近的像素。它不考虑像素是否具有几乎相同的强度。它不考虑像素是否是边缘像素。因此它也模糊了边缘，这是我们不想做的。 双边滤波器在空间中也采用高斯滤波器，但是又有一个高斯滤波器，它是像素差的函数。空间的高斯函数确保仅考虑附近像素的模糊，而强度差的高斯函数确保仅考虑强度与中心像素相似的像素的模糊。由于边缘的像素强度变化较大，因此可以保留边缘。 以下示例显示了使用双边过滤器 1blur = cv.bilateralFilter(img,9,75,75) 形态转换目标 我们将学习不同的形态学操作，例如侵蚀，膨胀，打开，关闭等。 我们将看到不同的功能，例如：cv.erode()，cv.dilate()，cv.morphologyEx()等。 理论形态变换是基于图像形状的一些简单操作。通常在二进制图像上执行。它需要两个输入，一个是我们的原始图像，第二个是决定操作性质的结构元素或内核。两种基本的形态学算子是侵蚀和膨胀。然后，它的变体形式（如“打开”，“关闭”，“渐变”等）也开始起作用。在下图的帮助下，我们将一一看到它们： 1.侵蚀侵蚀的基本思想就像仅是土壤侵蚀一样，它侵蚀了前景物体的边界（始终尝试使前景保持白色）。那是什么呢？内核在图像中滑动（如2D卷积）。仅当内核下的所有像素均为1时，原始图像中的像素（1或0）才被视为1，否则它将被侵蚀（设为零）。 因此发生的是，将根据内核的大小丢弃边界附近的所有像素。因此，前景对象的厚度或大小会减小，或者图像中的白色区域只会减小。这对于消除小的白噪声（如我们在色彩空间一章中看到的），分离两个连接的对象等非常有用。 在这里，作为一个例子，我将使用一个全是5x5的内核。让我们看看它是如何工作的： 123456789import cv2 as cvimport numpy as npimg = cv.imread('j.png',0)kernel = np.ones((5,5),np.uint8)erosion = cv.erode(img,kernel,iterations = 1)cv.imshow('k',erosion)cv.waitKey(0)cv.destroyAllWindows() 结果： 2.膨胀它与侵蚀正好相反。如果内核下的至少一个像素为“ 1”，则像素元素为“ 1”。因此，它会增加图像中的白色区域或增加前景对象的大小。通常，在消除噪音的情况下，腐蚀后会膨胀。因为腐蚀会消除白噪声，但也会缩小物体。因此，我们对其进行了扩展。由于噪音消失了，它们不会回来，但是我们的目标区域增加了。在连接对象的损坏部分时也很有用。 1dilation = cv.dilate(img,kernel,iterations = 1) 3.开运算开运算只是侵蚀然后膨胀的另一个名称。如上文所述，它对于消除噪音很有用。这里我们使用函数cv.morphologyEx() 1opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel) 4.闭运算关闭与打开，膨胀接着是侵蚀相反。在闭运算前景对象内部的小孔或对象上的小黑点时很有用。 1closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel) 5.形态梯度这是图像的膨胀和腐蚀之间的区别。 结果将看起来像对象的轮廓。 1gradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel) 6.高帽这是输入图像和图像打开之间的区别。下面的示例针对9x9内核完成。 1tophat = cv.morphologyEx(img, cv.MORPH_TOPHAT, kernel) 7.黑帽这是输入图像和输入图像关闭之间的差异。 1blackhat = cv.morphologyEx(img, cv.MORPH_BLACKHAT, kernel) 结构元素在Numpy的帮助下，我们在前面的示例中手动创建了一个结构元素。它是矩形。但是在某些情况下，您可能需要椭圆形/圆形的内核。因此，为此，OpenCV具有一个函数cv.getStructuringElement()。您只需传递内核的形状和大小，即可获得所需的内核。 123456789101112131415161718192021# Rectangular Kernel&gt;&gt;&gt; cv.getStructuringElement(cv.MORPH_RECT,(5,5))array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], dtype=uint8)# Elliptical Kernel&gt;&gt;&gt; cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))array([[0, 0, 1, 0, 0], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [0, 0, 1, 0, 0]], dtype=uint8)# Cross-shaped Kernel&gt;&gt;&gt; cv.getStructuringElement(cv.MORPH_CROSS,(5,5))array([[0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 1, 1, 1, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]], dtype=uint8) 图像渐变目标 查找图像渐变，边缘等 我们将看到以下函数：cv.Sobel()，cv.Scharr()，cv.Laplacian()等 理论OpenCV提供了三种类型的梯度滤波器或高通滤波器，即Sobel，Scharr和Laplacian。 Sobel算子是高斯平滑加微分运算的联合运算，因此它更抗噪声。您可以指定要采用的导数方向，垂直或水平（分别通过参数yorder和xorder）。您还可以通过参数ksize指定内核的大小。如果ksize = -1，则使用3x3 Scharr滤波器，其效果要比3x3 Sobel滤波器更好。请参阅文档以了解所使用的内核。 12345678910111213141516import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('dave.jpg',0)laplacian = cv.Laplacian(img,cv.CV_64F)sobelx = cv.Sobel(img,cv.CV_64F,1,0,ksize=5)sobely = cv.Sobel(img,cv.CV_64F,0,1,ksize=5)plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')plt.title('Original'), plt.xticks([]), plt.yticks([])plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')plt.title('Laplacian'), plt.xticks([]), plt.yticks([])plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')plt.title('Sobel X'), plt.xticks([]), plt.yticks([])plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])plt.show() 一个重要的事情！在我们的最后一个示例中，输出数据类型为cv.CV_8U或np.uint8。但这有一个小问题。黑色到白色的过渡被视为正斜率（具有正值），而白色到黑色的过渡被视为负斜率（具有负值）。因此，当您将数据转换为np.uint8时，所有负斜率均设为零。如果要检测两个边缘，更好的选择是将输出数据类型保留为更高的形式，例如cv.CV_16S，cv.CV_64F等，取其绝对值，然后转换回cv.CV_8U。下面的代码演示了水平Sobel滤波器的处理过程以及结果的差异。 1234567891011121314151617import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('box.png',0)# Output dtype = cv.CV_8Usobelx8u = cv.Sobel(img,cv.CV_8U,1,0,ksize=5)# Output dtype = cv.CV_64F. Then take its absolute and convert to cv.CV_8Usobelx64f = cv.Sobel(img,cv.CV_64F,1,0,ksize=5)abs_sobel64f = np.absolute(sobelx64f)sobel_8u = np.uint8(abs_sobel64f)plt.subplot(1,3,1),plt.imshow(img,cmap = 'gray')plt.title('Original'), plt.xticks([]), plt.yticks([])plt.subplot(1,3,2),plt.imshow(sobelx8u,cmap = 'gray')plt.title('Sobel CV_8U'), plt.xticks([]), plt.yticks([])plt.subplot(1,3,3),plt.imshow(sobel_8u,cmap = 'gray')plt.title('Sobel abs(CV_64F)'), plt.xticks([]), plt.yticks([])plt.show() Canny边缘检测目标 边缘检测的概念 OpenCV函数：cv.Canny（） 理论Canny Edge Detection是一种流行的边缘检测算法。 1.这是一个多阶段算法，我们将经历每个阶段。2.降噪由于边缘检测容易受到图像中噪声的影响，因此第一步是使用5x5高斯滤波器消除图像中的噪声。我们已经在前面的章节中看到了这一点。3.查找图像的强度梯度然后使用Sobel核在水平和垂直方向上对平滑的图像进行滤波，以在水平方向()和垂直方向()上获得一阶导数。从这两个图像中，我们可以找到每个像素的边缘渐变和方向，渐变方向始终垂直于边缘。将其舍入为代表垂直，水平和两个对角线方向的四个角度之一4.非最大抑制在获得梯度大小和方向后，将对图像进行全面扫描，以去除可能不构成边缘的所有不需要的像素。为此，在每个像素处，检查像素是否是其在梯度方向上附近的局部最大值。查看下面的图片： 点A在边缘（垂直方向）上。渐变方向垂直于边缘。点B和C在梯度方向上。因此，将A点与B点和C点进行检查，看是否形成局部最大值。如果是这样，则考虑将其用于下一阶段，否则将其抑制（置为零）。 简而言之，您得到的结果是带有“细边”的二进制图像。 5.磁滞阈值 该阶段确定哪些边缘全部是真正的边缘，哪些不是。为此，我们需要两个阈值minVal和maxVal。强度梯度大于maxVal的任何边缘必定是边缘，而小于minVal的那些强度必定是非边缘，因此将其丢弃。介于这两个阈值之间的对象根据其连通性被分类为边缘或非边缘。如果它们连接到“保证边缘”像素，则将它们视为边缘的一部分。否则，它们也将被丢弃。见下图： 边缘A在maxVal之上，因此被视为“确定边缘”。尽管边C低于maxVal，但它连接到边A，因此也被视为有效边，我们得到了完整的曲线。但是边缘B尽管在minVal之上并且与边缘C处于同一区域，但是它没有连接到任何“确保边缘”，因此被丢弃。因此，非常重要的一点是我们必须相应地选择minVal和maxVal以获得正确的结果。 在边缘为长线的假设下，该阶段还消除了小像素噪声。 因此，我们最终得到的是图像中的强边缘。 OpenCV中的Canny Edge检测OpenCV将以上所有内容放在单个函数cv.Canny()中。我们将看到如何使用它。第一个参数是我们的输入图像。第二个和第三个参数分别是我们的minVal和maxVal。第三个参数是perture_size。它是用于查找图像渐变的Sobel内核的大小。默认情况下为3。最后一个参数是L2gradient，它指定用于查找梯度幅度的方程式。如果为True，则使用上面提到的更精确的方程式,否则使用下面的方程式：Edge_Gradient(G)=|Gx|+|Gy|，默认是Flase 1234567891011import numpy as npimport cv2 as cv%matplotlib inlinefrom matplotlib import pyplot as pltimg = cv.imread('lena.jpg',0)edges = cv.Canny(img,100,200)plt.subplot(121),plt.imshow(img,cmap = 'gray')plt.title('Original Image'), plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(edges,cmap = 'gray')plt.title('Edge Image'), plt.xticks([]), plt.yticks([])plt.show() 图像金字塔目标 我们将学习图像金字塔 我们将使用图像金字塔创建一个新的水果“ Orapple” 我们将看到以下功能：cv.pyrUp（），cv.pyrDown（） 理论通常，我们过去使用的是恒定大小的图像。但是在某些情况下，我们需要使用不同分辨率的（相同）图像。例如，当在图像中搜索诸如面部之类的东西时，我们不确定对象将以何种大小出现在所述图像中。在这种情况下，我们将需要创建一组具有不同分辨率的相同图像，并在所有图像中搜索对象。这些具有不同分辨率的图像集称为“ 图像金字塔”（因为当它们堆叠在底部时，最高分辨率的图像位于顶部，最低分辨率的图像位于顶部时，看起来像金字塔）。 有两种图像金字塔。1）高斯金字塔和2）拉普拉斯金字塔 高斯金字塔中的较高级别（低分辨率）是通过删除较低级别（较高分辨率）图像中的连续行和列而形成的。然后，较高级别的每个像素由基础级别的5个像素的贡献与高斯权重形成。这样，图像变成图像。因此面积减少到原始面积的四分之一。它称为八度。当我们在金字塔中越靠上时（即分辨率降低），这种模式就会继续。同样，在扩展时，每个级别的面积变为4倍。我们可以使用cv.pyrDown()和cv.pyrUp()函数找到高斯金字塔。 12img = cv.imread（'messi5.jpg'）lower_reso = cv.pyrDown（higher_reso） 以下是图像金字塔中的4个级别。 现在，您可以使用cv.pyrUp()函数查看图像金字塔。 upper_reso2 = cv.pyrUp（lower_reso） 请记住，higher_reso2不等于higher_reso，因为一旦降低分辨率，便会丢失信息。图像下方是在以前情况下从最小图像创建的金字塔下3级。将其与原始图像进行比较： 拉普拉斯金字塔由高斯金字塔形成。没有专用功能。拉普拉斯金字塔图像仅像边缘图像。它的大部分元素为零。它们用于图像压缩。拉普拉斯金字塔的层由高斯金字塔的层与高斯金字塔的上层的扩展版本之间的差形成。拉普拉斯水平的三个水平如下所示（调整了对比度以增强内容）： 使用金字塔进行图像融合金字塔的一种应用是图像融合。例如，在图像拼接中，您需要将两个图像堆叠在一起，但是由于图像之间的不连续性，可能看起来不太好。在这种情况下，使用金字塔混合图像可以无缝混合，而不会在图像中保留大量数据。一个经典的例子是将两种水果，橙和苹果混合在一起。现在查看结果本身，以了解我在说什么： 请检查其他资源中的第一个参考，它具有图像混合，拉普拉斯金字塔等的完整图解详细信息。只需完成以下步骤即可： 加载苹果和橙子的两个图像 查找苹果和橙子的高斯金字塔（在此示例中，级别数为6） 从高斯金字塔中找到他们的拉普拉斯金字塔 现在在每个拉普拉斯金字塔中加入苹果的左半部分和橙的右半部分 最后，从此联合图像金字塔中重建原始图像。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 这里用的是xrange，注意更改import cv2 as cvimport numpy as np,sysA = cv.imread('apple.jpg')B = cv.imread('orange.jpg')# generate Gaussian pyramid for AG = A.copy()gpA = [G]for i in xrange(6): G = cv.pyrDown(G) gpA.append(G)# generate Gaussian pyramid for BG = B.copy()gpB = [G]for i in xrange(6): G = cv.pyrDown(G) gpB.append(G)# generate Laplacian Pyramid for AlpA = [gpA[5]]for i in xrange(5,0,-1): GE = cv.pyrUp(gpA[i]) L = cv.subtract(gpA[i-1],GE) lpA.append(L)# generate Laplacian Pyramid for BlpB = [gpB[5]]for i in xrange(5,0,-1): GE = cv.pyrUp(gpB[i]) L = cv.subtract(gpB[i-1],GE) lpB.append(L)# Now add left and right halves of images in each levelLS = []for la,lb in zip(lpA,lpB): rows,cols,dpt = la.shape ls = np.hstack((la[:,0:cols/2], lb[:,cols/2:])) LS.append(ls)# now reconstructls_ = LS[0]for i in xrange(1,6): ls_ = cv.pyrUp(ls_) ls_ = cv.add(ls_, LS[i])# image with direct connecting each halfreal = np.hstack((A[:,:cols/2],B[:,cols/2:]))cv.imwrite('Pyramid_blending2.jpg',ls_)cv.imwrite('Direct_blending.jpg',real) OpenCV中的轮廓轮廓：入门目标 了解轮廓是什么。 学习寻找轮廓，绘制轮廓等 您将看到以下功能：cv.findContours()，cv.drawContours() 什么是轮廓？轮廓可以简单地解释为连接具有相同颜色或强度的所有连续点（沿边界）的曲线。轮廓是用于形状分析以及对象检测和识别的有用工具。 为了获得更高的准确性，请使用二进制图像。因此，在找到轮廓之前，请应用阈值或坎尼边缘检测。 从OpenCV 3.2开始，findContours()不再修改源图像。 在OpenCV中，找到轮廓就像从黑色背景中找到白色物体。因此请记住，要找到的对象应该是白色，背景应该是黑色。让我们看看如何找到二进制图像的轮廓： 123456import numpy as npimport cv2 as cvim = cv.imread('lena.jpg')imgray = cv.cvtColor(im, cv.COLOR_BGR2GRAY)ret, thresh = cv.threshold(imgray, 127, 255, 0)contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE) 在cv.findContours()函数中有三个参数，第一个是源图像，第二个是轮廓检索模式，第三个是轮廓逼近方法。并输出轮廓和层次。轮廓是图像中所有轮廓的Python列表。每个单独的轮廓都是对象边界点的（x，y）坐标的Numpy数组。 如何绘制轮廓？要绘制轮廓，请使用cv.drawContours函数。只要有边界点，它也可以用来绘制任何形状。它的第一个参数是源图像，第二个参数是应该作为Python列表传递的轮廓，第三个参数是轮廓的索引（在绘制单个轮廓时很有用。要绘制所有轮廓，请传递-1），其余参数是颜色，厚度等等 要在图像中绘制所有轮廓：cv.drawContours(img, contours, -1, (0,255,0), 3) 要绘制单个轮廓，请说第四个轮廓：cv.drawContours(img, contours, 3, (0,255,0), 3)但是在大多数情况下，以下方法会很有用：cnt = contours[4] cv.drawContours(img, [cnt], 0, (0,255,0), 3) 最后两种方法相同，但是前进时，您会发现最后一种更有用。 轮廓近似法这是cv.findContours函数中的第三个参数。它实际上表示什么？ 上面我们告诉我们轮廓是强度相同的形状的边界。它存储形状边界的（x，y）坐标。但是它存储所有坐标吗？这是通过这种轮廓近似方法指定的。 如果传递cv.CHAIN_APPROX_NONE，则会存储所有边界点。但是实际上我们需要所有这些要点吗？例如，您找到了一条直线的轮廓。您是否需要线上的所有点来代表该线？不，我们只需要该线的两个端点即可。这就是cv.CHAIN_APPROX_SIMPLE所做的。它删除所有冗余点并压缩轮廓，从而节省内存。 轮廓特征目标 查找轮廓的不同特征，例如面积，周长，质心，边界框等 您将看到大量与轮廓有关的功能。 1.时刻图像矩可帮助您计算某些特征，例如物体的重心，物体的面积等。请查看“ 图像矩”上的Wikipedia页面 函数cv.moments()提供了所有计算出的矩值的字典。见下文： 12345678import numpy as npimport cv2 as cvimg = cv.imread('star.jpg',0)ret,thresh = cv.threshold(img,127,255,0)contours,hierarchy = cv.findContours(thresh, 1, 2)cnt = contours[0]M = cv.moments(cnt)print( M ) 12cx = int(M['m10']/M['m00'])cy = int(M['m01']/M['m00']) 2.轮廓面积轮廓区域由函数cv.contourArea()或从力矩M [‘m00’]中给出。 1area = cv.contourArea(cnt) 3.轮廓周长也称为弧长。可以使用cv.arcLength()函数找到它。第二个参数指定形状是闭合轮廓（如果通过True）还是曲线。 1perimeter = cv.arcLength(cnt,True) 4.轮廓近似根据我们指定的精度，它可以将轮廓形状近似为顶点数量较少的其他形状。它是Douglas-Peucker算法的实现。 为了理解这一点，假设您试图在图像中找到一个正方形，但是由于图像中的某些问题，您没有得到一个完美的正方形，而是一个“坏形状”（如下图所示）。现在，您可以使用此功能来近似形状。在这种情况下，第二个参数称为epsilon，它是从轮廓到近似轮廓的最大距离。它是一个精度参数。需要正确选择epsilon才能获得正确的输出。 12epsilon = 0.1*cv.arcLength(cnt,True)approx = cv.approxPolyDP(cnt,epsilon,True) 下面，在第二张图片中，绿线显示了ε=弧长的10％时的近似曲线。第三幅图显示了ε=电弧长度的1％时的情况。第三个参数指定曲线是否闭合。 5.凸包凸包外观看起来与轮廓逼近相似，但并非如此（在某些情况下两者可能提供相同的结果）。在这里，cv.convexHull()函数检查曲线是否存在凸凹缺陷并对其进行校正。一般而言，凸曲线是始终凸出或至少平坦的曲线。如果在内部凸出，则称为凸度缺陷。例如，检查下面的手的图像。红线显示手的凸包。双向箭头标记显示凸度缺陷，这是船体与轮廓线之间的局部最大偏差。 关于它的语法，有一些事情需要讨论： 1hull = cv.convexHull(points[, hull[, clockwise[, returnPoints]] 参数详细信息： points就是我们传入的轮廓。 hull是输出，通常我们避免它。 clockwise方向标记。如果为True，则输出凸包为顺时针方向。否则，其方向为逆时针方向。 returnPoints：默认情况下为True。然后返回船体点的坐标。如果为False，则返回与船体点相对应的轮廓点的索引。因此，要获得如上图所示的凸包，以下内容就足够了： 1hull = cv.convexHull(cnt) 但是，如果要查找凸度缺陷，则需要传递returnPoints = False。为了理解它，我们将拍摄上面的矩形图像。首先，我发现它的轮廓为cnt。现在，我发现它的带有returnPoints = True的凸包，得到以下值：[[[234 202]]，[[51 202]]，[[51 79]]，[[234 79]]]，它们是四个角矩形的点。现在，如果对returnPoints = False执行相同的操作，则会得到以下结果：[[129]，[67]，[0]，[142]]。这些是轮廓中相应点的索引。例如，检查第一个值：cnt [129] = [[234，202]]与第一个结果相同（对于其他结果依此类推）。 6.检查凸度cv.isContourConvex()有一个函数可以检查曲线是否为凸形。它只是返回True还是False。没有大碍。 7.边界矩形有两种类型的边界矩形。 7.A. 直角矩形它是一个直角矩形，不考虑对象的旋转。因此，边界矩形的面积将不会最小。它可以通过函数cv.boundingRect()找到。 令（x，y）为矩形的左上角坐标，而（w，h）为矩形的宽度和高度。 12x,y,w,h = cv.boundingRect(cnt)cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) 7.b. 旋转矩形在这里，边界矩形是用最小面积绘制的，因此它也考虑了旋转。使用的函数是cv.minAreaRect()。它返回一个Box2D结构，其中包含以下细节-（中心（x，y），（宽度，高度），旋转角度）。但是要绘制此矩形，我们需要矩形的4个角。它是通过函数cv.boxPoints()获得的 1234rect = cv.minAreaRect(cnt)box = cv.boxPoints(rect)box = np.int0(box)cv.drawContours(img,[box],0,(0,0,255),2) 8.最小外接圆接下来，我们使用函数cv.minEnclosingCircle()找到对象的外接圆。它是一个以最小面积完全覆盖对象的圆圈。 1234(x,y),radius = cv.minEnclosingCircle(cnt)center = (int(x),int(y))radius = int(radius)cv.circle(img,center,radius,(0,255,0),2) 9.拟合椭圆下一步是使椭圆适合对象。它返回椭圆所在的旋转形状。 12ellipse = cv.fitEllipse(cnt)cv.ellipse(img,ellipse,(0,255,0),2) 10.拟合线同样，我们可以将一条直线拟合到一组点。下图包含一组白点。我们可以近似一条直线。 12345rows,cols = img.shape[:2][vx,vy,x,y] = cv.fitLine(cnt, cv.DIST_L2,0,0.01,0.01)lefty = int((-x*vy/vx) + y)righty = int(((cols-x)*vy/vx)+y)cv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2) 轮廓属性在这里，我们将学习提取对象的一些常用属性，例如实体，等效直径，蒙版图像，平均强度等 1.长宽比它是对象边界矩形的宽度与高度的比率。 12x,y,w,h = cv.boundingRect(cnt)aspect_ratio = float(w)/h 2.范围范围是轮廓区域与边界矩形区域的比率。 1234area = cv.contourArea(cnt)x,y,w,h = cv.boundingRect(cnt)rect_area = w*hextent = float(area)/rect_area 3.坚固性坚固度是轮廓面积与其凸包面积的比率。 1234area = cv.contourArea(cnt)hull = cv.convexHull(cnt)hull_area = cv.contourArea(hull)solidity = float(area)/hull_area 4.等效直径当量直径是面积与轮廓面积相同的圆的直径。 12area = cv.contourArea(cnt)equi_diameter = np.sqrt(4*area/np.pi) 5.方向方向是物体指向的角度。以下方法还给出了主轴和副轴的长度。 1(x,y),(MA,ma),angle = cv.fitEllipse(cnt) 6.遮罩和像素点在某些情况下，我们可能需要构成该对象的所有点。可以按照以下步骤完成： 1234mask = np.zeros(imgray.shape,np.uint8)cv.drawContours(mask,[cnt],0,255,-1)pixelpoints = np.transpose(np.nonzero(mask))#pixelpoints = cv.findNonZero(mask) 7.最大值，最小值及其位置我们可以使用遮罩图像找到这些参数。 1min_val, max_val, min_loc, max_loc = cv.minMaxLoc(imgray,mask = mask) 8.平均颜色或平均强度在这里，我们可以找到对象的平均颜色。或者可以是灰度模式下物体的平均强度。我们再次使用相同的蒙版进行此操作。 1mean_val = cv.mean(im,mask = mask) 9.极端点极点是指对象的最顶部，最底部，最右侧和最左侧的点。 1234leftmost = tuple(cnt[cnt[:,:,0].argmin()][0])rightmost = tuple(cnt[cnt[:,:,0].argmax()][0])topmost = tuple(cnt[cnt[:,:,1].argmin()][0])bottommost = tuple(cnt[cnt[:,:,1].argmax()][0]) 轮廓：更多功能目标 凸性缺陷以及如何找到它们。 查找点到多边形的最短距离 匹配不同的形状 理论与规范1.凸性缺陷我们看到了关于轮廓的凸包。物体与该船体的任何偏离都可以视为凸度缺陷。 OpenCV带有一个现成的函数cv.convexityDefects()来查找该函数。基本的函数调用如下所示： 12hull = cv.convexHull(cnt,returnPoints = False)defects = cv.convexityDefects(cnt,hull) 注意:请记住，在寻找凸包时，我们必须传递returnPoints = False，以便寻找凸缺陷。 它返回一个数组，其中每行包含这些值- [起点，终点，最远点，到最远点的近似距离]。我们可以使用图像对其进行可视化。我们画一条连接起点和终点的线，然后在最远的点画一个圆。请记住，返回的前三个值是cnt的索引。因此，我们必须从cnt带来这些价值。 12345678910111213141516171819import cv2 as cvimport numpy as npimg = cv.imread('star.jpg')img_gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)ret,thresh = cv.threshold(img_gray, 127, 255,0)contours,hierarchy = cv.findContours(thresh,2,1)cnt = contours[0]hull = cv.convexHull(cnt,returnPoints = False)defects = cv.convexityDefects(cnt,hull)for i in range(defects.shape[0]): s,e,f,d = defects[i,0] start = tuple(cnt[s][0]) end = tuple(cnt[e][0]) far = tuple(cnt[f][0]) cv.line(img,start,end,[0,255,0],2) cv.circle(img,far,5,[0,0,255],-1)cv.imshow('img',img)cv.waitKey(0)cv.destroyAllWindows() 2.点多边形测试此功能查找图像中的点与轮廓之间的最短距离。它返回的距离为：当点在轮廓外时为负；当点在轮廓内时为正；如果点在轮廓上，则返回零。 例如，我们可以如下检查点（50,50）： 1dist = cv.pointPolygonTest（cnt，（50,50），True） 在函数中，第三个参数是measureDist。如果为True，则找到带符号的距离。如果为False，它将查找该点是在轮廓内部还是外部或轮廓上（它分别返回+ 1，-1、0）。 注意:如果您不想查找距离，请确保第三个参数为False，因为这是一个耗时的过程。因此，将其设置为False可使速度提高2-3倍。 3.比较形状OpenCV带有函数cv.matchShapes()，使我们能够比较两个形状或两个轮廓，并返回显示相似性的度量。结果越低，匹配越好。它是基于hu-moment值计算的。文档中介绍了不同的测量方法。 123456789101112import cv2 as cvimport numpy as npimg1 = cv.imread('star.jpg',0)img2 = cv.imread('star2.jpg',0)ret, thresh = cv.threshold(img1, 127, 255,0)ret, thresh2 = cv.threshold(img2, 127, 255,0)contours,hierarchy = cv.findContours(thresh,2,1)cnt1 = contours[0]contours,hierarchy = cv.findContours(thresh2,2,1)cnt2 = contours[0]ret = cv.matchShapes(cnt1,cnt2,1,0.0)print( ret ) 我尝试匹配以下给出的不同形状的形状： 我得到以下结果： 匹配图像A本身= 0.0 将图像A与图像B匹配= 0.001946 将图像A与图像C匹配= 0.326911看，即使图像旋转也不会对该比较产生太大影响。 也可以看看Hu-Moments是平移，旋转和缩放不变的七个时刻。第七个是偏斜不变的。这些值可以使用cv.HuMoments()函数找到。 轮廓层次目标这次，我们了解轮廓的层次结构，即Contours中的父子关系。 理论在有关轮廓的最后几篇文章中，我们使用了与OpenCV提供的轮廓相关的一些功能。但是，当我们使用cv.findContours()函数在图像中找到轮廓时，我们传递了一个参数，即Contour Retrieval Mode。我们通常通过cv.RETR_LIST或cv.RETR_TREE，效果很好。但这实际上是什么意思？ 另外，在输出中，我们得到了三个数组，第一个是图像，第二个是轮廓，另一个是我们命名为层次结构的输出（请检查上一篇文章中的代码）。但是，我们从未在任何地方使用此层次结构。那么，这个层次结构是什么呢？它与前面提到的函数参数有什么关系？ 这就是本文要处理的内容。 什么是层次结构？通常我们使用cv.findContours()函数来检测图像中的对象，对吗？有时对象位于不同的位置。但是在某些情况下，某些形状位于其他形状内。就像嵌套的数字一样。在这种情况下，我们将外部的一个称为父级，将内部的一个称为子级。这样，图像中的轮廓彼此之间就具有某种关系。并且我们可以指定一个轮廓如何相互连接，例如是其他轮廓的子轮廓，还是父轮廓等。这种关系的表示称为层次结构。 考虑下面的示例图像： 在此图像中，我从0-5编号了一些形状。2和2a表示最外面的盒子的外部和内部轮廓。 在此，轮廓0,1,2在外部或最外部。我们可以说，它们处于0层次结构中，或者只是处于相同的层次结构级别中。 接下来是轮廓2a。可以将其视为轮廓2的子级（或者相反，轮廓2是轮廓2a的父级）。因此，将其设置为hierarchy-1。同样，contour-3是contour-2的子级，位于下一个层次结构中。最后，轮廓4,5是轮廓3a的子级，它们位于最后的层次结构级别。从编号方式上来说，轮廓4是轮廓3a的第一个子元素（也可以是轮廓5）。 我提到这些东西是为了理解诸如相同的层次结构级别，外部轮廓，子轮廓，父轮廓，第一个孩子等术语。现在让我们进入OpenCV。 OpenCV中的层次结构表示因此，每个轮廓都有关于其层次结构，其子级，其父级等的信息。OpenCV将其表示为四个值的数组：[Next，Previous，First_Child，Parent] *“下一个表示相同等级的下一个轮廓。” *例如，在我们的图片中选择轮廓0。谁是同一级别的下一个轮廓？它是轮廓1。因此，只需将Next = 1放进去。同样对于Contour-1，下一个就是轮廓线2。所以下一个= 2。 那轮廓2呢？在同一层中没有下一个轮廓。简而言之，将Next = -1。那轮廓4呢？与轮廓5处于同一水平。所以它的下一个轮廓是轮廓5，所以Next = 5。 *“上一个表示相同轮廓级别的上一个轮廓。” *和上面一样。轮廓1的先前轮廓是同一级别的轮廓0。同样对于轮廓2，它是轮廓1。对于轮廓0，没有先前值，因此将其设为-1。 *“ First_Child表示其第一个子轮廓。” *无需任何解释。对于轮廓2，子级是轮廓2a。这样就得到了轮廓2a的相应索引值。那轮廓3a呢？它有两个孩子。但是我们只带第一个孩子。它是轮廓4。因此，轮廓3a的First_Child = 4。 *“父代表示其父代轮廓的索引。” *它与First_Child相反。轮廓4和轮廓5的父轮廓均为轮廓3a。对于轮廓3a，它是轮廓3，依此类推。 注意:如果没有孩子或父母，则该字段为-1 因此，现在我们知道了OpenCV中使用的层次结构样式，我们可以借助上面给出的相同图像来检查OpenCV中的轮廓检索模式。即，像cv.RETR_LIST，cv.RETR_TREE，cv.RETR_CCOMP，cv.RETR_EXTERNAL等标志是什么意思？ 轮廓检索模式1. RETR_LIST这是四个标志中最简单的一个（从解释的角度来看）。它仅检索所有轮廓，但不创建任何父子关系。在这个规则下，父母和孩子是平等的，他们只是轮廓。即它们都属于同一层次结构级别。 因此，在这里，层次结构数组中的第3和第4项始终为-1。但是很明显，下一个和上一个术语将具有其相应的值。只需自己检查并验证即可。 以下是我得到的结果，每行是相应轮廓的层次结构详细信息。例如，第一行对应于轮廓0。下一个轮廓为轮廓1。因此Next =1。没有先前的轮廓，因此Previous = -1。如前所述，其余两个为-1。 123456789&gt;&gt;&gt; hierarchyarray([[[ 1, -1, -1, -1], [ 2, 0, -1, -1], [ 3, 1, -1, -1], [ 4, 2, -1, -1], [ 5, 3, -1, -1], [ 6, 4, -1, -1], [ 7, 5, -1, -1], [-1, 6, -1, -1]]]) 如果不使用任何层次结构功能，这是在代码中使用的不错选择。 2. RETR_EXTERNAL如果使用此标志，则仅返回极端的外部标志。保留所有子轮廓。可以说，根据这项法律，只有每个家庭中的老大才能得到照顾。它不在乎家庭其他成员: 那么，在我们的图像中，有多少个极端的外部轮廓？即在等级0级别？只有3个，即轮廓0,1,2，对吗？现在尝试使用该标志查找轮廓。在此，赋予每个元素的值也与上述相同。与上面的结果进行比较。以下是我得到的： 1234&gt;&gt;&gt; hierarchyarray([[[ 1, -1, -1, -1], [ 2, 0, -1, -1], [-1, 1, -1, -1]]]) 如果只想提取外部轮廓，则可以使用此标志。在某些情况下可能有用。 3. RETR_CCOMP该标志检索所有轮廓并将它们排列为2级层次结构。即，对象的外部轮廓（即其边界）位于层次1中。然后，将对象（如果有）中的孔的轮廓放置在层次2中。如果其中有任何对象，则其轮廓将仅再次放置在等级1中。以及它在等级2中的漏洞等等。 只需考虑黑色背景上的“白色大零”图像即可。零外圈属于第一层级，零内圈属于第二层级。 我们可以用一个简单的图像来解释它。在这里，我用红色标记了轮廓的顺序，并用绿色（1或2）标记了它们所属的层次。该顺序与OpenCV检测轮廓的顺序相同。 因此考虑第一个轮廓，即轮廓0。它是等级1。它有两个孔，轮廓1和2，它们属于层次2。因此，对于轮廓0，相同层次结构级别中的下一个轮廓为轮廓3。而且没有以前的。它的第一个子对象是层次结构2中的轮廓1。它没有父级，因为它位于1层级中。因此其层次结构数组为[3，-1,1，-1] 现在取轮廓1。它在等级2中。在同一层次结构中（轮廓1的父项下）下一个是轮廓2。没有上一个。没有孩子，但父母的轮廓为0。因此数组为[2，-1，-1,0]。 同样的轮廓2：它位于层次2中。在轮廓-0下的相同层次结构中没有下一个轮廓。所以没有下一步。上一个是轮廓1。没有孩子，父母的轮廓为0。因此数组为[-1,1，-1,0]。 轮廓-3：等级1中的下一个是轮廓5。上一个是轮廓0。孩子是轮廓4，没有父母。因此数组为[5,0,4，-1]。 轮廓-4：位于轮廓3下的层次2中，并且没有同级。所以没有下一个，没有以前的，没有孩子，父母是轮廓3。因此数组为[-1，-1，-1,3]。 剩下的可以填满。这是我得到的最终答案： 12345678910&gt;&gt;&gt; hierarchyarray([[[ 3, -1, 1, -1], [ 2, -1, -1, 0], [-1, 1, -1, 0], [ 5, 0, 4, -1], [-1, -1, -1, 3], [ 7, 3, 6, -1], [-1, -1, -1, 5], [ 8, 5, -1, -1], [-1, 7, -1, -1]]]) 4. RETR_TREE这是最后一个家伙，Perfect先生。它检索所有轮廓并创建完整的族层次列表。它甚至告诉，谁是爷爷，父亲，儿子，孙子甚至更远… :)。 例如，我拍摄了上面的图片，重写了cv.RETR_TREE的代码，根据OpenCV给定的结果对轮廓进行重新排序并对其进行分析。同样，红色字母表示轮廓编号，绿色字母表示层次结构顺序。 取轮廓0：在层次0中。同一层次结构中的下一个轮廓是轮廓7。没有先前的轮廓。孩子是轮廓1。而且没有父母。因此数组为[7，-1,1，-1]。 取轮廓2：在等级1中。同一级别无轮廓。没有上一个。孩子是轮廓3。父级是轮廓1。因此数组为[-1，-1,3,1]。 还有，尝试一下。以下是完整答案： 12345678910&gt;&gt;&gt; hierarchyarray([[[ 7, -1, 1, -1], [-1, -1, 2, 0], [-1, -1, 3, 1], [-1, -1, 4, 2], [-1, -1, 5, 3], [ 6, -1, -1, 4], [-1, 5, -1, 4], [ 8, 0, -1, -1], [-1, 7, -1, -1]]]) OpenCV中的直方图直方图-1：查找，绘制，分析!!!目标 使用OpenCV和Numpy函数查找直方图 使用OpenCV和Matplotlib函数绘制直方图 您将看到以下功能：cv.calcHist()，np.histogram()等。 理论那么直方图是什么？您可以将直方图视为图形或曲线图，从而使您对图像的强度分布有一个整体的了解。它是在X轴上具有像素值（不总是从0到255的范围），在Y轴上具有图像中相应像素数的图。 这只是理解图像的另一种方式。通过查看图像的直方图，您可以直观地了解该图像的对比度，亮度，强度分布等。当今几乎所有图像处理工具都提供直方图功能。以下是剑桥彩色网站上的图片，建议您访问该网站以获取更多详细信息。 您可以看到图像及其直方图。（请记住，此直方图是针对灰度图像而非彩色图像绘制的）。直方图的左侧区域显示图像中较暗像素的数量，而右侧区域则显示较亮像素的数量。从直方图中，您可以看到暗区域多于亮区域，中间调的数量（中间值的像素值，例如127附近）非常少。 查找直方图现在我们有了一个关于直方图的想法，我们可以研究如何找到它。OpenCV和Numpy都为此内置了功能。在使用这些功能之前，我们需要了解一些与直方图有关的术语。 BINS：上面的直方图显示每个像素值的像素数，即从0到255。即，您需要256个值来显示上面的直方图。但是考虑一下，如果您不需要分别找到所有像素值的像素数，而是找到像素值间隔中的像素数怎么办？例如，您需要找到介于0到15之间，然后16到31之间，…，240到255之间的像素数。您只需要16个值即可表示直方图。这就是在OpenCV直方图教程中给出的示例中所显示的内容。 因此，您要做的就是将整个直方图分成16个子部分，每个子部分的值就是其中所有像素数的总和。每个子部分都称为“ BIN”。在第一种情况下，bin的数量为256个（每个像素一个），而在第二种情况下，bin的数量仅为16个。BINS由OpenCV文档中的histSize术语表示。 DIMS：这是我们为其收集数据的参数的数量。在这种情况下，我们仅收集关于强度值的一件事的数据。所以这里是1。 范围：这是您要测量的强度值的范围。通常，它是[0,256]，即所有强度值。 1. OpenCV中的直方图计算因此，现在我们使用cv.calcHist()函数查找直方图。让我们熟悉一下函数及其参数： 1cv.calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) images：它是uint8或float32类型的源图像。它应该放在方括号中，即“ [img]”。 channels：也以方括号给出。它是我们计算直方图的通道的索引。例如，如果输入为灰度图像，则其值为[0]。对于彩色图像，您可以传递[0]，[1]或[2]分别计算蓝色，绿色或红色通道的直方图。 mask：遮罩图像。为了找到完整图像的直方图，将其指定为“无”。但是，如果要查找图像特定区域的直方图，则必须为此创建一个遮罩图像并将其作为遮罩。（我将在后面显示一个示例。） histSize：这表示我们的BIN计数。需要放在方括号中。对于全尺寸，我们通过[256]。 ranges：这是我们的RANGE。通常为[0,256]。因此，让我们从示例图像开始。只需在灰度模式下加载图像并找到其完整的直方图即可。 12img = cv.imread(&apos;home.jpg&apos;,0)hist = cv.calcHist([img],[0],None,[256],[0,256]) hist是256x1的数组，每个值对应于该图像中具有相应像素值的像素数。 2. Numpy中的直方图计算Numpy还为您提供了一个函数np.histogram()。因此，您可以在下面的行尝试代替calcHist()函数： 1hist,bins = np.histogram(img.ravel(),256,[0,256]) hist与我们之前计算的相同。但是bin将具有257个元素，因为Numpy计算出bin的范围为0-0.99、1-1.99、2-2.99等。因此最终范围为255-255.99。为了表示这一点，他们还在料箱末端添加了256。但是我们不需要256。最多255就足够了。 也可以看看 Numpy还有另一个函数np.bincount()，它比np.histogram()快10倍左右。因此，对于一维直方图，您可以更好地尝试一下。不要忘记在np.bincount中设置minlength = 256。例如，hist = np.bincount(img.ravel()，minlength = 256) 注意:OpenCV函数比np.histogram()快（大约40倍）。因此，请坚持使用OpenCV功能。现在我们应该绘制直方图，但是如何绘制？ 绘制直方图有两种方法， 简单方法：使用Matplotlib绘图功能 复杂方法：使用OpenCV绘图功能 1.使用MatplotlibMatplotlib带有直方图绘图功能：matplotlib.pyplot.hist() 它直接找到直方图并将其绘制。您无需使用calcHist()或np.histogram()函数来查找直方图。请参见下面的代码： 123456import numpy as npimport cv2 as cv%matplotlib inlinefrom matplotlib import pyplot as pltimg = cv.imread('home.jpg',0)plt.hist(img.ravel(),256,[0,256]); plt.show() 或者，您可以使用matplotlib的法线图，这对于BGR图是很好的。为此，您需要首先找到直方图数据。试试下面的代码： 12345678910import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('home.jpg')color = ('b','g','r')for i,col in enumerate(color): histr = cv.calcHist([img],[i],None,[256],[0,256]) plt.plot(histr,color = col) plt.xlim([0,256])plt.show() 您可以从上图中得出，蓝色在图像中具有一些高价值区域（显然这应该是由于天空） 2.使用OpenCV好吧，在这里您可以调整直方图的值及其bin值，使其看起来像x，y坐标，以便可以使用cv.line()或cv.polyline()函数绘制它以生成与上述相同的图像。OpenCV-Python2官方示例已经提供了此功能。检查示例/python/hist.py中的代码。 掩膜的应用我们使用cv.calcHist()查找完整图像的直方图。如果要查找图像某些区域的直方图怎么办？只需在要查找直方图的区域上创建白色的蒙版图像，否则创建黑色。然后通过这个作为面具。 123456789101112131415img = cv.imread('home.jpg',0)# create a maskmask = np.zeros(img.shape[:2], np.uint8)mask[100:300, 100:400] = 255masked_img = cv.bitwise_and(img,img,mask = mask)# Calculate histogram with mask and without mask# Check third argument for maskhist_full = cv.calcHist([img],[0],None,[256],[0,256])hist_mask = cv.calcHist([img],[0],mask,[256],[0,256])plt.subplot(221), plt.imshow(img, 'gray')plt.subplot(222), plt.imshow(mask,'gray')plt.subplot(223), plt.imshow(masked_img, 'gray')plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)plt.xlim([0,256])plt.show() 查看结果。在直方图中，蓝线表示完整图像的直方图，绿线表示遮蔽区域的直方图。 直方图-2：直方图均衡目标 我们将学习直方图均衡化的概念，并将其用于改善图像的对比度。 理论考虑一个图像，其像素值仅限于特定的值范围。例如，较亮的图像会将所有像素限制在较高的值。但是，好的图像将具有来自图像所有区域的像素。因此，您需要将此直方图拉伸到两端（如下图所示，来自维基百科），这就是直方图均衡化的作用（简单来说）。通常，这可以提高图像的对比度。 在这里我们将看到其Numpy实现。之后，我们将看到OpenCV功能 您可以看到直方图位于较亮的区域。我们需要全方位的服务。为此，我们需要一个转换函数，该函数将较亮区域中的输入像素映射到整个区域中的输出像素。这就是直方图均衡化的作用。 现在，我们找到最小的直方图值（不包括0）并应用Wiki页面中给出的直方图均衡方程。但是我在这里使用了Numpy的masked array概念数组。对于掩码数组，所有操作都在非掩码元素上执行。您可以从有关屏蔽数组的Numpy文档中了解有关此内容的更多信息。 123cdf_m = np.ma.masked_equal(cdf,0)cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())cdf = np.ma.filled(cdf_m,0).astype('uint8') 现在我们有了查找表，该表为我们提供了有关每个输入像素值的输出像素值是什么的信息。因此，我们仅应用变换。 1img2 = cdf[img] 现在我们像以前一样计算它的直方图和cdf（您这样做），结果如下所示： 另一个重要特征是，即使图像是较暗的图像（而不是我们使用的较亮的图像），在均衡后，我们将获得与获得的图像几乎相同的图像。结果，它被用作“参考工具”，以使所有图像具有相同的照明条件。在许多情况下这很有用。例如，在人脸识别中，在训练人脸数据之前，将人脸图像进行直方图均衡，以使它们全部具有相同的光照条件。 OpenCV中的直方图均衡OpenCV具有执行此操作的功能cv.equalizeHist()。它的输入只是灰度图像，输出是我们的直方图均衡图像。 下面是一个简单的代码片段，显示了它与我们使用的同一图像的用法： 1234img = cv.imread('wiki.jpg',0)equ = cv.equalizeHist(img)res = np.hstack((img,equ)) #stacking images side-by-sidecv.imwrite('res.png',res) 因此，现在您可以在不同的光照条件下拍摄不同的图像，对其进行均衡并检查结果。 当图像的直方图限制在特定区域时，直方图均衡化效果很好。在直方图覆盖较大区域（即同时存在亮像素和暗像素）的强度变化较大的地方，效果不好。请检查其他资源中的SOF链接。 CLAHE（对比度受限的自适应直方图均衡）我们刚刚看到的第一个直方图均衡化考虑了图像的整体对比度。在许多情况下，这不是一个好主意。例如，下图显示了输入图像及其在全局直方图均衡后的结果。 直方图均衡后，背景对比度确实得到了改善。但是在两个图像中比较雕像的脸。由于亮度过高，我们在那里丢失了大多数信息。这是因为它的直方图不像我们在前面的案例中所看到的那样局限于特定区域（尝试绘制输入图像的直方图，您将获得更多的直觉）。 因此，为了解决这个问题，使用了自适应直方图均衡。在这种情况下，图像被分成称为“tiles”的小块（在OpenCV中，tileSize默认为8x8）。然后，像往常一样对这些块中的每一个进行直方图均衡。因此，在较小的区域中，直方图将局限于一个较小的区域（除非有噪声）。如果有噪音，它将被放大。为了避免这种情况，应用了对比度限制。如果任何直方图bin超过指定的对比度限制（在OpenCV中默认为40），则在应用直方图均衡之前，将这些像素裁剪并均匀地分布到其他bin。均衡后，要消除图块边界中的伪影，请应用双线性插值。 下面的代码片段显示了如何在OpenCV中应用CLAHE： 1234567import numpy as npimport cv2 as cvimg = cv.imread('tsukuba_l.png',0)# create a CLAHE object (Arguments are optional).clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))cl1 = clahe.apply(img)cv.imwrite('clahe_2.jpg',cl1) 查看下面的结果，并将其与上面的结果进行比较，尤其是雕像区域： 直方图-3：2D直方图目标 在本章中，我们将学习查找和绘制2D直方图。这将在以后的章节中有所帮助。 介绍在第一篇文章中，我们计算并绘制了一维直方图。之所以称为一维，是因为我们仅考虑一个特征，即像素的灰度强度值。但是在二维直方图中，您要考虑两个特征。通常，它用于查找颜色直方图，其中两个特征是每个像素的色相和饱和度值。 已经有一个python样本（samples/python/color_histogram.py）用于查找颜色直方图。我们将尝试了解如何创建这种颜色直方图，这对于理解诸如直方图反投影之类的更多主题将很有用。 OpenCV中的2D直方图它非常简单，并且使用相同的函数cv.calcHist()进行计算。对于颜色直方图，我们需要将图像从BGR转换为HSV。（请记住，对于一维直方图，我们从BGR转换为灰度）。对于2D直方图，其参数将进行如下修改： channels = [0,1]， 因为我们需要同时处理H和S平面。 bins = [180,256] 对于H平面为180，对于S平面为256。 ranges = [0,180,0,256] 色相值介于0和180之间，饱和度介于0和256之间。 现在检查以下代码： 12345import numpy as npimport cv2 as cvimg = cv.imread('home.jpg')hsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)hist = cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256]) numpy中的2D直方图Numpy还为此提供了一个特定功能：np.histogram2d()。（请记住，对于一维直方图，我们使用了np.histogram()）。 123456import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('home.jpg')hsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)hist, xbins, ybins = np.histogram2d(h.ravel(),s.ravel(),[180,256],[[0,180],[0,256]]) 绘制2D直方图方法-1：使用cv.imshow()我们得到的结果是尺寸为180x256的二维数组。因此，可以使用cv.imshow()函数像平常一样显示它们。它将是一幅灰度图像，除非您知道不同颜色的色相值，否则不会对其中的颜色有太多了解。 方法-2：使用Matplotlib我们可以使用matplotlib.pyplot.imshow()函数绘制具有不同颜色图的2D直方图。它使我们对不同的像素密度有了更好的了解。但是，这也并不能使我们一眼就能知道是什么颜色，除非您知道不同颜色的色相值。我还是更喜欢这种方法。它简单而更好。 注意:使用此功能时，请记住，插值标记应最接近以获得更好的结果。 代码： 12345678import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('home.jpg')hsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)hist = cv.calcHist( [hsv], [0, 1], None, [180, 256], [0, 180, 0, 256] )plt.imshow(hist,interpolation = 'nearest')plt.show() 在直方图中，您可以在H = 100和S = 200附近看到一些较高的值。它对应于天空的蓝色。同样，在H = 25和S = 100附近可以看到另一个峰值。它对应于宫殿的黄色。您可以使用GIMP等任何图像编辑工具进行验证。 方法3：OpenCV示例样式！OpenCV-Python2示例中有一个颜色直方图的示例代码（samples/python/color_histogram.py）。如果运行代码，则可以看到直方图也显示了相应的颜色。或者简单地，它输出颜色编码的直方图。其结果非常好（尽管您需要添加额外的线束）。 在该代码中，作者在HSV中创建了一个颜色图。然后将其转换为BGR。将所得的直方图图像与此颜色图相乘。他还使用一些预处理步骤来删除小的孤立像素，从而获得良好的直方图。 我将它留给读者来运行代码，对其进行分析并拥有自己的解决方法。下面是与上面相同的图像的代码输出： 您可以在直方图中清楚地看到存在什么颜色，那里是蓝色，那里是黄色，并且由于棋盘而有些白色。不错！ 直方图-4：直方图反投影目标 在本章中，我们将学习直方图反投影。 理论它是由Michael J.Swain和Dana H.Ballard在他们的论文“ 通过颜色直方图索引”中提出的。 简单来说到底是什么？它用于图像分割或在图像中查找感兴趣的对象。简而言之，它创建的图像大小与输入图像相同（但只有一个通道），其中每个像素对应于该像素属于我们物体的概率。用更简单的话来说，与其余部分相比，输出图像将使我们感兴趣的对象具有更多的白色。好吧，这是一个直观的解释。（我无法使其更简单）。直方图反投影与camshift算法等配合使用。 我们该怎么做呢 ？我们创建一个图像的直方图，其中包含我们感兴趣的对象（在我们的示例中是地面，离开播放器等）。对象应尽可能填充图像以获得更好的效果。而且颜色直方图比灰度直方图更可取，因为对象的颜色比其灰度强度是定义对象的更好方法。然后，我们将该直方图“反向投影”到需要找到对象的测试图像上，换句话说，我们计算出属于地面的每个像素的概率并将其显示出来。在适当的阈值下产生的输出仅使我们有基础。 Numpy中的算法1.首先，我们需要计算我们要查找的对象（使其为“ M”）和要搜索的图像（使其为“ I”）的颜色直方图。 1234567891011import numpy as npimport cv2 as cvfrom matplotlib import pyplot as plt#roi is the object or region of object we need to findroi = cv.imread('rose_red.png')hsv = cv.cvtColor(roi,cv.COLOR_BGR2HSV)#target is the image we search intarget = cv.imread('rose.png')hsvt = cv.cvtColor(target,cv.COLOR_BGR2HSV)# Find the histograms using calcHist. Can be done with np.histogram2d alsoM = cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )I = cv.calcHist([hsvt],[0, 1], None, [180, 256], [0, 180, 0, 256] ) 2.求出比率 1234h,s,v = cv.split(hsvt)B = R[h.ravel(),s.ravel()]B = np.minimum(B,1)B = B.reshape(hsvt.shape[:2]) 3.在对圆盘应用卷积 1234disc = cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))cv.filter2D(B,-1,disc,B)B = np.uint8(B)cv.normalize(B,B,0,255,cv.NORM_MINMAX) 现在最大强度的位置给了我们物体的位置。如果我们期望图像中有一个区域，则对合适的值进行阈值处理会得到不错的结果。 1ret,thresh = cv.threshold(B,50,255,0) OpenCV中的反投影OpenCV提供了一个内置函数cv.calcBackProject()。它的参数与cv.calcHist()函数几乎相同。它的参数之一是直方图，它是对象的直方图，我们必须找到它。另外，在传递给backproject函数之前，应对对象直方图进行标准化。它返回概率图像。然后，我们将图像与磁盘内核卷积并应用阈值。下面是我的代码和输出： 1234567891011121314151617181920import numpy as npimport cv2 as cvroi = cv.imread('rose_red.png')hsv = cv.cvtColor(roi,cv.COLOR_BGR2HSV)target = cv.imread('rose.png')hsvt = cv.cvtColor(target,cv.COLOR_BGR2HSV)# calculating object histogramroihist = cv.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )# normalize histogram and apply backprojectioncv.normalize(roihist,roihist,0,255,cv.NORM_MINMAX)dst = cv.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)# Now convolute with circular discdisc = cv.getStructuringElement(cv.MORPH_ELLIPSE,(5,5))cv.filter2D(dst,-1,disc,dst)# threshold and binary ANDret,thresh = cv.threshold(dst,50,255,0)thresh = cv.merge((thresh,thresh,thresh))res = cv.bitwise_and(target,thresh)res = np.vstack((target,thresh,res))cv.imwrite('res.jpg',res) 以下是我处理过的一个示例。我将蓝色矩形内的区域用作示例对象，我想提取整个地面。 OpenCV中的图像转换傅立叶变换目标 使用OpenCV查找图像的傅立叶变换 利用Numpy中可用的FFT功能 傅立叶变换的一些应用 我们将看到以下函数：cv.dft()，cv.idft()等 理论傅立叶变换用于分析各种滤波器的频率特性。对于图像，使用2D离散傅里叶变换（DFT）查找频域。快速算法称为快速傅立叶变换（FFT）用于计算DFT。关于这些的详细信息可以在任何图像处理或信号处理教科书中找到。请参阅其他资源_部分。 对于正弦信号$x(t) = Asin(2{\pi}ft)$，我们可以说F是信号的频率，如果采用其频域，则可以看到的尖峰F。如果对信号进行采样以形成离散信号，我们将获得相同的频域，但在$[-{\pi},{\pi}]$或$[0,2{\pi}]$（或对于N点DFT为是周期性的$[0,N]$）。您可以将图像视为在两个方向上采样的信号。因此，在X和Y方向都进行傅立叶变换，可以得到图像的频率表示。 更直观地说，对于正弦信号，如果振幅在短时间内变化如此之快，则可以说它是高频信号。如果变化缓慢，则为低频信号。您可以将相同的想法扩展到图像。图像中的振幅在哪里急剧变化？在边缘点或噪音。因此，可以说边缘和噪声是图像中的高频内容。如果幅度没有太大变化，则它是低频分量。（一些链接已添加到“其他资源”，其中通过示例直观地说明了频率变换）。 现在，我们将看到如何找到傅立叶变换。 numpy中的傅立叶变换首先，我们将看到如何使用Numpy查找傅立叶变换。Numpy具有FFT软件包来执行此操作。np.fft.fft2()为我们提供了频率转换，它将是一个复杂的数组。它的第一个参数是输入图像，即灰度图像。第二个参数是可选的，它决定输出数组的大小。如果它大于输入图像的大小，则在计算FFT之前用零填充输入图像。如果小于输入图像，将裁切输入图像。如果未传递任何参数，则输出数组的大小将与输入的大小相同。 现在，一旦获得结果，零频率分量（DC分量）将位于左上角。如果要将其居中，则需要在两个方向结果移动。只需通过函数np.fft.fftshift()即可完成。（它更容易分析）。找到频率变换后，就可以找到幅度谱。结果如下： 看，您可以在中心看到更多白色区域，这表明低频内容更多。 因此，您找到了频率变换现在，您可以在频域中执行一些操作，例如高通滤波和重建图像，即找到逆DFT。为此，您只需用尺寸为60x60的矩形窗口遮罩即可消除低频。然后，使用np.fft.ifftshift（）应用反向移位，以使DC分量再次出现在左上角。然后使用np.ifft2（）函数找到逆FFT 。同样，结果将是一个复数。您可以采用其绝对值。 12345678910111213rows, cols = img.shapecrow,ccol = rows//2 , cols//2fshift[crow-30:crow+31, ccol-30:ccol+31] = 0f_ishift = np.fft.ifftshift(fshift)img_back = np.fft.ifft2(f_ishift)img_back = np.real(img_back)plt.subplot(131),plt.imshow(img, cmap = 'gray')plt.title('Input Image'), plt.xticks([]), plt.yticks([])plt.subplot(132),plt.imshow(img_back, cmap = 'gray')plt.title('Image after HPF'), plt.xticks([]), plt.yticks([])plt.subplot(133),plt.imshow(img_back)plt.title('Result in JET'), plt.xticks([]), plt.yticks([])plt.show() Result look like below: 结果表明高通滤波是边缘检测操作。这就是我们在“图像渐变”一章中看到的。这也表明大多数图像数据都存在于频谱的低频区域。无论如何，我们已经看到了如何在Numpy中找到DFT，IDFT等。现在，让我们看看如何在OpenCV中进行操作。 如果您仔细观察结果，尤其是最后一张JET颜色的图像，您会看到一些伪像（我用红色箭头标记的一个实例）。它在那里显示出一些波纹状结构，称为振铃效应。这是由我们用于遮罩的矩形窗口引起的。此蒙版转换为正弦形状，从而导致此问题。因此，矩形窗口不用于过滤。更好的选择是高斯Windows。 OpenCV中的傅立叶变换OpenCV 为此提供了功能cv.dft()和cv.idft()。它返回与以前相同的结果，但是有两个通道。第一个通道将具有结果的实部，第二个通道将具有结果的虚部。输入的图像应首先转换为np.float32。我们将看到如何做。 123456789101112import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('messi5.jpg',0)dft = cv.dft(np.float32(img),flags = cv.DFT_COMPLEX_OUTPUT)dft_shift = np.fft.fftshift(dft)magnitude_spectrum = 20*np.log(cv.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))plt.subplot(121),plt.imshow(img, cmap = 'gray')plt.title('Input Image'), plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])plt.show() 注意:您还可以使用cv.cartToPolar（）一次返回大小和相位 因此，现在我们必须进行逆DFT。在上面，我们创建了一个HPF，这次我们将看到如何去除图像中的高频内容，即我们将LPF应用于图像。实际上会使图像模糊。为此，我们首先创建一个在低频时具有高值（1）的蒙版，即，我们传递LF含量，并在HF区域传递0。 123456789101112131415rows, cols = img.shapecrow,ccol = rows/2 , cols/2# create a mask first, center square is 1, remaining all zerosmask = np.zeros((rows,cols,2),np.uint8)mask[crow-30:crow+30, ccol-30:ccol+30] = 1# apply mask and inverse DFTfshift = dft_shift*maskf_ishift = np.fft.ifftshift(fshift)img_back = cv.idft(f_ishift)img_back = cv.magnitude(img_back[:,:,0],img_back[:,:,1])plt.subplot(121),plt.imshow(img, cmap = 'gray')plt.title('Input Image'), plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(img_back, cmap = 'gray')plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])plt.show() 注意:像往常一样，OpenCV函数cv.dft()和cv.idft()比Numpy对应函数要快。但是Numpy功能更加人性化。有关性能问题的更多详细信息，请参阅以下部分。 DFT的性能优化对于某些阵列大小，DFT计算的性能更好。当阵列大小为2的幂时，它是最快的。大小为2、3和5的乘积的数组也得到了有效处理。因此，如果您担心代码的性能，可以在找到DFT之前将数组的大小修改为任何最佳大小（通过填充零）。对于OpenCV，您必须手动填充零。但是对于Numpy，您可以指定FFT计算的新大小，它将自动为您填充零。 那么我们如何找到这个最佳尺寸呢？OpenCV 为此提供了一个函数cv.getOptimalDFTSize()。它适用于cv.dft()和np.fft.fft2()。让我们使用IPython magic命令timeit检查它们的性能。 12345678In [16]: img = cv.imread('messi5.jpg',0)In [17]: rows,cols = img.shapeIn [18]: print("&#123;&#125; &#123;&#125;".format(rows,cols))342 548In [19]: nrows = cv.getOptimalDFTSize(rows)In [20]: ncols = cv.getOptimalDFTSize(cols)In [21]: print("&#123;&#125; &#123;&#125;".format(nrows,ncols))360 576 参见，将大小（342,548）修改为（360，576）。现在让我们用零填充（对于OpenCV），并找到其DFT计算性能。您可以通过创建一个新的大零数组并将数据复制到其中来完成此操作，或者使用cv.copyMakeBorder()。 12nimg = np.zeros((nrows,ncols))nimg[:rows,:cols] = img 或者： 1234right = ncols - colsbottom = nrows - rowsbordertype = cv.BORDER_CONSTANT #just to avoid line breakup in PDF filenimg = cv.copyMakeBorder(img,0,bottom,0,right,bordertype, value = 0) 现在，我们计算Numpy函数的DFT性能比较： 1234In [22]: %timeit fft1 = np.fft.fft2(img)10 loops, best of 3: 40.9 ms per loopIn [23]: %timeit fft2 = np.fft.fft2(img,[nrows,ncols])100 loops, best of 3: 10.4 ms per loop 它显示了4倍的加速。现在，我们将尝试使用OpenCV函数。 1234In [24]: %timeit dft1= cv.dft(np.float32(img),flags=cv.DFT_COMPLEX_OUTPUT)100 loops, best of 3: 13.5 ms per loopIn [27]: %timeit dft2= cv.dft(np.float32(nimg),flags=cv.DFT_COMPLEX_OUTPUT)100 loops, best of 3: 3.11 ms per loop 它还显示了4倍的加速。您还可以看到OpenCV函数比Numpy函数快3倍左右。也可以对逆FFT进行测试，这留给您练习。 为什么拉普拉斯算子是高通滤波器？在论坛上提出了类似的问题。问题是，为什么拉普拉斯算子是高通滤波器？为什么Sobel是HPF？等等。第一个得到的答案是傅里叶变换。只需对Laplacian进行傅立叶变换，以获得更大的FFT大小。分析一下： 1234567891011121314151617181920212223242526272829303132333435import cv2 as cvimport numpy as npfrom matplotlib import pyplot as plt# simple averaging filter without scaling parametermean_filter = np.ones((3,3))# creating a gaussian filterx = cv.getGaussianKernel(5,10)gaussian = x*x.T# different edge detecting filters# scharr in x-directionscharr = np.array([[-3, 0, 3], [-10,0,10], [-3, 0, 3]])# sobel in x directionsobel_x= np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])# sobel in y directionsobel_y= np.array([[-1,-2,-1], [0, 0, 0], [1, 2, 1]])# laplacianlaplacian=np.array([[0, 1, 0], [1,-4, 1], [0, 1, 0]])filters = [mean_filter, gaussian, laplacian, sobel_x, sobel_y, scharr]filter_name = ['mean_filter', 'gaussian','laplacian', 'sobel_x', \ 'sobel_y', 'scharr_x']fft_filters = [np.fft.fft2(x) for x in filters]fft_shift = [np.fft.fftshift(y) for y in fft_filters]mag_spectrum = [np.log(np.abs(z)+1) for z in fft_shift]for i in xrange(6): plt.subplot(2,3,i+1),plt.imshow(mag_spectrum[i],cmap = 'gray') plt.title(filter_name[i]), plt.xticks([]), plt.yticks([])plt.show() 从图像中，您可以看到每个内核阻止的频率区域以及它经过的区域。从这些信息中，我们可以说出为什么每个内核都是HPF或LPF 模板匹配目标 使用模板匹配在图像中查找对象 您将看到以下功能：cv.matchTemplate()，cv.minMaxLoc() 理论模板匹配是一种用于在较大图像中搜索和查找模板图像位置的方法。为此，OpenCV带有一个函数cv.matchTemplate()。它只是在输入图像上滑动模板图像（如2D卷积），然后在模板图像下比较模板和输入图像的补丁。OpenCV中实现了几种比较方法。（您可以检查文档以了解更多详细信息）。它返回一个灰度图像，其中每个像素表示该像素的邻域与模板匹配多少。 如果输入图像的尺寸为（WxH），模板图像的尺寸为（wxh），则输出图像的尺寸将为（W-w + 1，H-h + 1）。获得结果后，可以使用cv.minMaxLoc()函数查找最大/最小值在哪里。将其作为矩形的左上角，并以（w，h）作为矩形的宽度和高度。该矩形是您模板的区域。 注意:如果使用cv.TM_SQDIFF作为比较方法，则最小值提供最佳匹配。 OpenCV中的模板匹配作为示例，我们将在梅西的照片中搜索他的脸。所以我创建了一个模板，如下所示：我们将尝试所有比较方法，以便我们可以看到它们的结果如何： 1234567891011121314151617181920212223242526272829import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltimg = cv.imread('messi5.jpg',0)img2 = img.copy()template = cv.imread('template.jpg',0)w, h = template.shape[::-1]# All the 6 methods for comparison in a listmethods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR', 'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']for meth in methods: img = img2.copy() method = eval(meth) # Apply template Matching res = cv.matchTemplate(img,template,method) min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res) # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]: top_left = min_loc else: top_left = max_loc bottom_right = (top_left[0] + w, top_left[1] + h) cv.rectangle(img,top_left, bottom_right, 255, 2) plt.subplot(121),plt.imshow(res,cmap = 'gray') plt.title('Matching Result'), plt.xticks([]), plt.yticks([]) plt.subplot(122),plt.imshow(img,cmap = 'gray') plt.title('Detected Point'), plt.xticks([]), plt.yticks([]) plt.suptitle(meth) plt.show() 结果如下： cv.TM_CCOEFF cv.TM_CCOEFF_NORMED cv.TM_CCORR cv.TM_CCORR_NORMED cv.TM_SQDIFF cv.TM_SQDIFF_NORMED 模板与多个对象匹配在上一节中，我们在图像中搜索了梅西的脸，该脸在图像中仅出现一次。假设您正在搜索具有多次出现的对象，则cv.minMaxLoc()不会为您提供所有位置。在这种情况下，我们将使用阈值。因此，在此示例中，我们将使用著名游戏Mario的屏幕截图，并在其中找到硬币。 12345678910111213import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltimg_rgb = cv.imread('mario.png')img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)template = cv.imread('mario_coin.png',0)w, h = template.shape[::-1]res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)threshold = 0.8loc = np.where( res &gt;= threshold)for pt in zip(*loc[::-1]): cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)cv.imwrite('res.png',img_rgb) 霍夫线变换目标 我们将了解霍夫变换的概念。 我们将看到如何使用它来检测图像中的线条。 我们将看到以下函数：cv.HoughLines()，cv.HoughLinesP() 理论如果可以用数学形式表示形状，则霍夫变换是一种检测任何形状的流行技术。即使形状有些破损或变形，也可以检测出形状。 OpenCV中的霍夫变换上面解释的所有内容都封装在OpenCV函数cv.HoughLines()中。第一个参数，输入图像应该是二进制图像，因此在应用霍夫变换之前，请应用阈值或使用Canny边缘检测。第二和第三参数是ρ和θ准确度。第四个参数是阈值，这意味着应该将其视为行的最低投票。请记住，票数取决于线上的点数。因此，它表示应检测到的最小线长。 123456789101112131415161718import cv2 as cvimport numpy as npimg = cv.imread(cv.samples.findFile('sudoku.png'))gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)edges = cv.Canny(gray,50,150,apertureSize = 3)lines = cv.HoughLines(edges,1,np.pi/180,200)for line in lines: rho,theta = line[0] a = np.cos(theta) b = np.sin(theta) x0 = a*rho y0 = b*rho x1 = int(x0 + 1000*(-b)) y1 = int(y0 + 1000*(a)) x2 = int(x0 - 1000*(-b)) y2 = int(y0 - 1000*(a)) cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)cv.imwrite('houghlines3.jpg',img) 概率霍夫变换在霍夫变换中，您可以看到即使对于带有两个参数的行，也需要大量的计算。概率霍夫变换是我们看到的霍夫变换的优化。它没有考虑所有要点。取而代之的是，它仅采用随机的点子集，足以进行线检测。只是我们必须降低阈值。参见下图，比较了霍夫空间中的霍夫变换和概率霍夫变换。（图片提供：Franck Bettinger的主页） OpenCV的实现基于Matas，J.和Galambos，C.和Kittler，JV使用渐进式概率霍夫变换对行进行的稳健检测。使用的函数是cv.HoughLinesP()。它有两个新的论点。 minLineLength-最小长度。小于此长度的线段将被拒绝。 maxLineGap-线段之间允许将它们视为一条线的最大间隙。最好的是，它直接返回行的两个端点。在以前的情况下，您仅获得线的参数，并且必须找到所有点。在这里，一切都是直接而简单的。 霍夫圆变换目标在这一章当中， 我们将学习使用霍夫变换在图像中查找圆。 我们将看到以下函数：cv.HoughCircles（） 示例代码12345678910111213141516import numpy as npimport cv2 as cvimg = cv.imread('opencv-logo-white.png',0)img = cv.medianBlur(img,5)cimg = cv.cvtColor(img,cv.COLOR_GRAY2BGR)circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,20, param1=50,param2=30,minRadius=0,maxRadius=0)circles = np.uint16(np.around(circles))for i in circles[0,:]: # draw the outer circle cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2) # draw the center of the circle cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)cv.imshow('detected circles',cimg)cv.waitKey(0)cv.destroyAllWindows() 分水岭算法的图像分割目标 我们将学习使用分水岭算法使用基于标记的图像分割 我们将看到：cv.watershed() 理论任何灰度图像都可以视为地形图表面，其中高强度表示山峰和丘陵，而低强度表示山谷。您开始用不同颜色的水（标签）填充每个孤立的山谷（局部最小值）。随着水的上升，取决于附近的峰（梯度），来自不同山谷（显然具有不同颜色）的水将开始合并。为了避免这种情况，您可以在水汇合的位置建造障碍。您将继续填充水和建造障碍物的工作，直到所有山峰都在水下。然后，您创建的障碍将为您提供细分结果。这就是分水岭背后的“哲学”。您可以访问分水岭上的CMM网页，借助一些动画来了解它。 但是，这种方法会由于噪声或图像中的任何其他不规则性而给您造成过分分割的结果。因此，OpenCV实施了基于标记的分水岭算法，您可以在其中指定要合并的所有山谷点，哪些不是。这是一个交互式图像分割。我们要做的是为我们知道的对象提供不同的标签。用一种颜色（或强度）标记我们确定为前景或对象的区域，用另一种颜色标记我们确定为背景或非对象的区域，最后标记为我们不确定的区域，将其标记为0。这就是我们的标记。然后应用分水岭算法。然后，我们的标记将使用给定的标签进行更新，并且对象的边界的值为-1。 代码示例：下面我们将看到一个有关如何使用距离变换和分水岭分割相互接触的对象的示例。 考虑下面的硬币图像，硬币彼此接触。即使您设置阈值，它也会彼此接触。 我们从找到硬币的近似估计开始。为此，我们可以使用Otsu的二值化。 123456import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('coins.png')gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)ret, thresh = cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU) 现在我们需要去除图像中的任何小白噪声。为此，我们可以使用形态学开放。要去除对象中的任何小孔，我们可以使用形态学封闭。因此，现在我们可以确定，靠近对象中心的区域是前景，而离对象中心很远的区域是背景。我们不确定的唯一区域是硬币的边界区域。 因此，我们需要提取我们确定它们是硬币的区域。侵蚀会去除边界像素。因此，无论剩余多少，我们都可以肯定它是硬币。如果物体彼此不接触，那将起作用。但是，由于它们彼此接触，因此另一个好选择是找到距离变换并应用适当的阈值。接下来，我们需要找到我们确定它们不是硬币的区域。为此，我们扩大了结果。膨胀将对象边界增加到背景。这样，由于边界区域已删除，因此我们可以确保结果中背景中的任何区域实际上都是背景。参见下图。 其余的区域是我们不知道的区域，无论是硬币还是背景。分水岭算法应该找到它。这些区域通常位于前景和背景相遇（甚至两个不同的硬币相遇）的硬币边界附近。我们称之为边界。可以通过从sure_bg区域中减去sure_fg区域来获得。 1234567891011# noise removalkernel = np.ones((3,3),np.uint8)opening = cv.morphologyEx(thresh,cv.MORPH_OPEN,kernel, iterations = 2)# sure background areasure_bg = cv.dilate(opening,kernel,iterations=3)# Finding sure foreground areadist_transform = cv.distanceTransform(opening,cv.DIST_L2,5)ret, sure_fg = cv.threshold(dist_transform,0.7*dist_transform.max(),255,0)# Finding unknown regionsure_fg = np.uint8(sure_fg)unknown = cv.subtract(sure_bg,sure_fg) 查看结果。在阈值图像中，我们得到了一些硬币区域，我们确定这些区域是硬币，现在已经分离了。（在某些情况下，您可能只对前景分割感兴趣，而不对分离相互接触的对象感兴趣。在那种情况下，您无需使用距离变换，只需腐蚀就足够了。腐蚀只是另一种提取确定前景区域的方法，那就是所有。） 现在我们可以确定哪些是硬币区域，哪些是背景硬币以及所有硬币。因此，我们创建标记（它的大小与原始图像的大小相同，但具有int32数据类型），并标记其中的区域。我们肯定知道的区域（无论是前景还是背景）都标有任何正整数，但标记为不同的整数，而我们不确定的区域则保留为零。为此，我们使用cv.connectedComponents()。它用0标记图像的背景，然后其他对象用从1开始的整数标记。 但是我们知道，如果背景标记为0，则分水岭会将其视为未知区域。所以我们想用不同的整数来标记它。相反，我们将未知定义的未知区域标记为0。 123456# Marker labellingret, markers = cv.connectedComponents(sure_fg)# Add one to all labels so that sure background is not 0, but 1markers = markers+1# Now, mark the region of unknown with zeromarkers[unknown==255] = 0 请参见JET颜色图中显示的结果。深蓝色区域显示未知区域。当然，硬币的颜色会不同。与未知区域相比，可以确定背景的其余区域以浅蓝色显示。 现在我们的标记器已准备就绪。现在是最后一步的时候了，申请分水岭。然后标记图像将被修改。边界区域将标记为-1。 12markers = cv.watershed(img,markers)img[markers == -1] = [255,0,0] 请参阅下面的结果。对于某些硬币，它们接触的区域被正确地分割，而对于某些硬币，则不是。 使用GrabCut算法进行交互式前景提取目标 我们将看到GrabCut算法提取图像中的前景 我们将为此创建一个交互式应用程序。 理论GrabCut算法由英国微软研究院的Carsten Rother，Vladimir Kolmogorov和Andrew Blake设计。在他们的论文“GrabCut”中：使用迭代图割的交互式前景提取。需要用最少的用户交互进行前景提取的算法，结果是GrabCut。 从用户角度来看，它是如何工作的？最初，用户在前景区域周围绘制一个矩形（前景区域应完全位于矩形内部）。然后，算法会对其进行迭代分割，以获得最佳结果。完成。但在某些情况下，分割可能不会很好，例如，可能已将某些前景区域标记为背景，反之亦然。在这种情况下，用户需要进行精细修饰。只需在图像上画些笔画，那里就会有一些错误的结果。笔画基本上说*“嘿，该区域应该是前景，您将其标记为背景，在下一次迭代中对其进行校正” *或与背景相反。然后在下一次迭代中，您将获得更好的结果。 参见下图。第一名球员和球被封闭在一个蓝色矩形中。然后用白色笔划（表示前景）和黑色笔划（表示背景）进行最后的修饰。而且我们得到了不错的结果。 那么背景发生了什么呢？ 用户输入矩形。此矩形之外的所有内容都将用作背景（这是在矩形应包含所有对象之前提到的原因）。矩形内的所有内容都是未知的。同样，任何指定前景和背景的用户输入都被视为硬标签，这意味着它们在此过程中不会更改。 计算机根据我们提供的数据进行初始标记。它标记前景和背景像素（或对其进行硬标记） 现在，使用高斯混合模型（GMM）对前景和背景进行建模。 根据我们提供的数据，GMM可以学习并创建新的像素分布。也就是说，未知像素根据颜色统计上与其他硬标记像素的关系而被标记为可能的前景或可能的背景（就像聚类一样）。 根据此像素分布构建图形。图中的节点为像素。添加了另外两个节点，即Source节点和Sink节点。每个前景像素都连接到源节点，每个背景像素都连接到接收器节点。 将像素连接到源节点/末端节点的边缘的权重由像素是前景/背景的概率定义。像素之间的权重由边缘信息或像素相似度定义。如果像素颜色差异很大，则它们之间的边缘将变低。 然后使用mincut算法对图进行分段。它将图切成具有最小成本函数的两个分离的源节点和宿节点。成本函数是被切割边缘的所有权重的总和。剪切后，连接到“源”节点的所有像素都变为前景，而连接到“接收器”节点的像素都变为背景。 继续该过程，直到分类收敛为止。下图对此进行了说明（图片提供：http : //www.cs.ru.ac.za/research/g02m1682/） 演示版现在我们使用OpenCV进行抓取算法。OpenCV 为此具有功能cv.grabCut()。我们将首先看到其参数： img-输入图片 mask-这是一个蒙版图像，其中我们指定哪些区域是背景，前景或可能的背景/前景等。它是通过以下标志cv.GC_BGD，cv.GC_FGD，cv.GC_PR_BGD，cv.GC_PR_FGD来完成的，或者只是通过0,1,2,3到图像。 rect-它是矩形的坐标，其中包括格式为（x，y，w，h）的前景对象 bdgModel，fgdModel-这些是算法内部使用的数组。您只需创建两个大小为（1,65）的np.float64类型零数组。 iterCount-算法应运行的迭代次数。 mode-应该为cv.GC_INIT_WITH_RECT或cv.GC_INIT_WITH_MASK或两者结合，决定我们要绘制矩形还是最终的修饰笔触。首先让我们看看矩形模式。我们加载图像，创建类似的蒙版图像。我们创建fgdModel和bgdModel。我们给出矩形参数。一切都是直截了当的。让算法运行5次迭代。模式应为cv.GC_INIT_WITH_RECT，因为我们使用的是矩形。然后运行抓取。修改蒙版图像。在新的蒙版图像中，像素将被标记有四个标记，分别表示上面指定的背景/前景。因此，我们修改蒙版，使所有0像素和2像素都置为0（即背景），而所有1像素和3像素均置为1（即前景像素）。现在，我们的最终蒙版已经准备就绪。只需将其与输入图像相乘即可得到分割的图像。 123456789101112import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltimg = cv.imread('messi5.jpg')mask = np.zeros(img.shape[:2],np.uint8)bgdModel = np.zeros((1,65),np.float64)fgdModel = np.zeros((1,65),np.float64)rect = (50,50,450,290)cv.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv.GC_INIT_WITH_RECT)mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')img = img*mask2[:,:,np.newaxis]plt.imshow(img),plt.colorbar(),plt.show() 糟糕，梅西的头发不见了。谁会喜欢没有头发的梅西？我们需要把它带回来。因此，我们将使用1像素（确保前景）进行精细修饰。同时，一些我们不需要的地面图片和一些徽标也出现了。我们需要删除它们。在那里，我们给出了一些0像素的修饰（确保背景）。因此，如现在所说，我们在以前的情况下修改了生成的蒙版。 我实际上所做的是，我在paint应用程序中打开了输入图像，并在图像中添加了另一层。使用画笔中的画笔工具，我在新图层上用白色标记了错过的前景（头发，鞋子，球等），而用白色标记了不需要的背景（例如徽标，地面等）。然后用灰色填充剩余的背景。然后将该蒙版图像加载到OpenCV中，编辑我们在新添加的蒙版图像中具有相应值的原始蒙版图像。检查以下代码： 12345678910# newmask is the mask image I manually labellednewmask = cv.imread('newmask.png',0)# wherever it is marked white (sure foreground), change mask=1# wherever it is marked black (sure background), change mask=0mask[newmask == 0] = 0mask[newmask == 255] = 1mask, bgdModel, fgdModel = cv.grabCut(img,mask,None,bgdModel,fgdModel,5,cv.GC_INIT_WITH_MASK)mask = np.where((mask==2)|(mask==0),0,1).astype('uint8')img = img*mask[:,:,np.newaxis]plt.imshow(img),plt.colorbar(),plt.show() 就是这样了。在这里，您无需直接在rect模式下初始化，而可以直接进入mask模式。只需用2像素或3像素（可能的背景/前景）标记蒙版图像中的矩形区域。然后像在第二个示例中一样，将我们的sure_foreground标记为1像素。然后直接在遮罩模式下应用grabCut功能。]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV的核心操作]]></title>
    <url>%2Fopencv%2FOpenCV%E7%9A%84%E6%A0%B8%E5%BF%83%E6%93%8D%E4%BD%9C.html</url>
    <content type="text"><![CDATA[图像的基本操作目标 访问像素值并修改它们 访问图像属性 设置兴趣区（ROI） 分割和合并图像本节中的几乎所有操作都主要与Numpy相关，而不是与OpenCV相关。要使用OpenCV编写更好的优化代码，需要Numpy的丰富知识。 （由于大多数示例都是单行代码，因此示例将在Python终端中显示）访问和修改像素值 让我们先加载彩色图像： 123&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import cv2 as cv&gt;&gt;&gt; img = cv.imread('messi5.jpg') 您可以通过像素值的行和列坐标来访问它。对于BGR图像，它将返回一个蓝色，绿色，红色值的数组。对于灰度图像，仅返回相应的强度。 1234567&gt;&gt;&gt; px = img[100,100]&gt;&gt;&gt; print( px )[157 166 200]# accessing only blue pixel&gt;&gt;&gt; blue = img[100,100,0]&gt;&gt;&gt; print( blue )157 您可以用相同的方式修改像素值。 123&gt;&gt;&gt; img[100,100] = [255,255,255]&gt;&gt;&gt; print( img[100,100] )[255 255 255] 警告:Numpy是用于快速数组计算的优化库。因此，简单地访问每个像素值并对其进行修改将非常缓慢，因此不建议使用。 注意:上面的方法通常用于选择数组的区域，例如前5行和后3列。对于单个像素访问，Numpy数组方法array.item()和array.itemset()更好，但是它们始终返回标量。如果要访问所有B，G，R值，则需要分别调用所有的array.item()。 更好的像素访问和编辑方法： 1234567# accessing RED value&gt;&gt;&gt; img.item(10,10,2)59# modifying RED value&gt;&gt;&gt; img.itemset((10,10,2),100)&gt;&gt;&gt; img.item(10,10,2)100 访问图像属性图像属性包括行数，列数和通道数，图像数据类型，像素数等。 图像的形状可通过img.shape访问。它返回行，列和通道数的元组（如果图像是彩色的）： 12&gt;&gt;&gt; print( img.shape )(342, 548, 3) 注意:如果图像是灰度的，则返回的元组仅包含行数和列数，因此这是检查加载的图像是灰度还是彩色的好方法。 像素总数可通过访问img.size： 12&gt;&gt;&gt; print( img.size )562248 图像数据类型通过img.dtype获得： 12&gt;&gt;&gt; print( img.dtype )uint8 注意:img.dtype在调试时非常重要，因为OpenCV-Python代码中的大量错误是由无效的数据类型引起的。 图像投资回报率有时，您将不得不使用图像的某些区域。为了在图像中进行眼睛检测，首先在整个图像上进行面部检测。当获得一张脸时，我们仅选择脸部区域并在其中搜索眼睛，而不是搜索整个图像。它提高了准确性（因为眼睛总是在脸上：D）和性能（因为我们在小范围内搜索）。 使用Numpy索引再次获得ROI。在这里，我要选择球并将其复制到图像中的另一个区域： 12&gt;&gt;&gt; ball = img[280:340, 330:390]&gt;&gt;&gt; img[273:333, 100:160] = ball 检查以下结果： 分割和合并图像通道有时您需要分别处理图像的B，G，R通道。在这种情况下，您需要将BGR图像拆分为单个通道。在其他情况下，您可能需要将这些单独的频道加入BGR图片。您可以通过以下方式简单地做到这一点： 12&gt;&gt;&gt; b,g,r = cv.split(img)&gt;&gt;&gt; img = cv.merge((b,g,r)) 要么 1&gt;&gt;&gt; b = img[:,:,0] 假设您要将所有红色像素都设置为零，则无需先拆分通道。numpy索引更快： 1&gt;&gt;&gt; img[:,:,2] = 0 警告:cv.split()是一项昂贵的操作（就时间而言）。因此，仅在需要时才这样做。否则请进行Numpy索引。 为图像设置边框（填充）如果要在图像周围创建边框（如相框），则可以使用cv.copyMakeBorder()。但是它在卷积运算，零填充等方面有更多应用。此函数采用以下参数： src-输入图像 top, bottom, left, right边界的宽度，以相应方向上的像素数为单位 borderType-定义要添加哪种边框的标志。它可以是以下类型： cv.BORDER_CONSTANT-添加恒定的彩色边框。该值应作为下一个参数给出。 cv.BORDER_REFLECT-边框将是边框元素的镜像，如下所示：fedcba|abcdefgh|hgfedcb cv.BORDER_REFLECT_101或cv.BORDER_DEFAULT-与上述相同，但略有变化，例如：gfedcb|abcdefgh|gfedcba cv.BORDER_REPLICATE-最后一个元素被复制，像这样：aaaaaa|abcdefgh|hhhhhhh cv.BORDER_WRAP-无法解释，它看起来像这样：cdefgh|abcdefgh|abcdefg value -边框颜色，如果边框类型为cv.BORDER_CONSTANT下面是一个示例代码，演示了所有这些边框类型，以便更好地理解： 1234567891011121314151617import cv2 as cvimport numpy as npfrom matplotlib import pyplot as pltBLUE = [255,0,0]img1 = cv.imread('opencv-logo.png')replicate = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REPLICATE)reflect = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT)reflect101 = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT_101)wrap = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_WRAP)constant= cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_CONSTANT,value=BLUE)plt.subplot(231),plt.imshow(img1,'gray'),plt.title('ORIGINAL')plt.subplot(232),plt.imshow(replicate,'gray'),plt.title('REPLICATE')plt.subplot(233),plt.imshow(reflect,'gray'),plt.title('REFLECT')plt.subplot(234),plt.imshow(reflect101,'gray'),plt.title('REFLECT_101')plt.subplot(235),plt.imshow(wrap,'gray'),plt.title('WRAP')plt.subplot(236),plt.imshow(constant,'gray'),plt.title('CONSTANT')plt.show() 请参阅下面的结果。（图像与matplotlib一起显示。因此红色和蓝色通道将互换）： 图像上的算术运算目标 学习图像的几种算术运算，例如加法，减法，按位运算等。 您将学习以下功能：cv.add()，cv.addWeighted()等。 图像加法您可以通过OpenCV函数cv.add()或仅通过numpy操作（res = img1 + img2 ）添加两个图像。两个图像应具有相同的深度和类型，或者第二个图像可以只是一个标量值。 注意:OpenCV加法和Numpy加法之间有区别。OpenCV加法是饱和运算，而Numpy加法是模运算。例如，考虑以下示例： 123456&gt;&gt;&gt; x = np.uint8([250])&gt;&gt;&gt; y = np.uint8([10])&gt;&gt;&gt; print( cv.add(x,y) ) # 250+10 = 260 =&gt; 255[[255]]&gt;&gt;&gt; print( x+y ) # 250+10 = 260 % 256 = 4[4] 当添加两个图像时，它将更加可见。OpenCV功能将提供更好的结果。因此，始终最好坚持使用OpenCV功能。 图像融合这也是图像加法，但是对图像赋予不同的权重，以使其具有融合或透明的感觉。 123456img1 = cv.imread('ml.png')img2 = cv.imread('opencv-logo.png')dst = cv.addWeighted(img1,0.7,img2,0.3,0)cv.imshow('dst',dst)cv.waitKey(0)cv.destroyAllWindows() 检查以下结果： 按位运算这包括按位与，或，非和异或运算。在提取图像的任何部分（如我们将在后续章节中看到），定义和使用非矩形ROI等方面，它们将非常有用。下面我们将看到有关如何更改图像特定区域的示例。 我想在图像上方放置OpenCV徽标。如果添加两个图像，它将改变颜色。如果混合它，我将获得透明效果。但我希望它不透明。如果是矩形区域，则可以像上一章一样使用ROI。但是OpenCV徽标不是矩形。因此，您可以按如下所示进行按位操作： 1234567891011121314151617181920# Load two imagesimg1 = cv.imread('messi5.jpg')img2 = cv.imread('opencv-logo-white.png')# I want to put logo on top-left corner, So I create a ROIrows,cols,channels = img2.shaperoi = img1[0:rows, 0:cols ]# Now create a mask of logo and create its inverse mask alsoimg2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)ret, mask = cv.threshold(img2gray, 10, 255, cv.THRESH_BINARY)mask_inv = cv.bitwise_not(mask)# Now black-out the area of logo in ROIimg1_bg = cv.bitwise_and(roi,roi,mask = mask_inv)# Take only region of logo from logo image.img2_fg = cv.bitwise_and(img2,img2,mask = mask)# Put logo in ROI and modify the main imagedst = cv.add(img1_bg,img2_fg)img1[0:rows, 0:cols ] = dstcv.imshow('res',img1)cv.waitKey(0)cv.destroyAllWindows() 请参阅下面的结果。左图显示了我们创建的遮罩。右图显示了最终结果。为了进一步理解，请显示以上代码中的所有中间图像，尤其是img1_bg和img2_fg。 绩效评估和改进技术目标在图像处理中，由于每秒要处理大量操作，因此必须使代码不仅提供正确的解决方案，而且还必须以最快的方式提供代码。因此，在本章中，您将学习 衡量代码的性能。 一些提高代码性能的技巧。 您将看到以下功能：cv.getTickCount，cv.getTickFrequency等。除了OpenCV，Python还提供了一个模块时间，这有助于衡量执行时间。另一个模块配置文件有助于获取有关代码的详细报告，例如代码中每个函数花费了多少时间，函数被调用了多少次等。但是，如果您使用的是IPython，则所有这些功能都集成在用户友好的界面中方式。我们将看到一些重要的信息，有关更多详细信息，请查看“ 其他资源”部分中的链接。 使用OpenCV衡量性能cv.getTickCount函数返回从参考事件（如打开机器的那一刻）到调用此函数一刻之间的时钟周期数。因此，如果在函数执行之前和之后调用它，则会获得用于执行函数的时钟周期数。 cv.getTickFrequency函数返回时钟周期的频率或每秒的时钟周期数。因此，要找到执行时间（以秒为单位），您可以执行以下操作： 1234e1 = cv.getTickCount()# your code executione2 = cv.getTickCount()time = (e2 - e1)/ cv.getTickFrequency() 我们将通过以下示例进行演示。下面的示例应用中位数过滤，其内核的奇数范围为5到49。（不用担心结果会是什么样，这不是我们的目标）： 12345678img1 = cv.imread('messi5.jpg')e1 = cv.getTickCount()for i in xrange(5,49,2): img1 = cv.medianBlur(img1,i)e2 = cv.getTickCount()t = (e2 - e1)/cv.getTickFrequency()print( t )# Result I got is 0.521107655 seconds 注意:您可以使用时间模块执行相同的操作。代替cv.getTickCount，使用time.time（）函数。然后取两次相差。 OpenCV中的默认优化许多OpenCV功能已使用SSE2，AVX等进行了优化。它还包含未优化的代码。因此，如果我们的系统支持这些功能，则应该加以利用（几乎所有现代处理器都支持它们）。默认在编译时启用。因此，OpenCV如果启用了优化代码，则将运行优化代码，否则它将运行未优化的代码。您可以使用cv.useOptimized()来检查是否启用/禁用它，并使用cv.setUseOptimized()来启用/禁用它。让我们看一个简单的例子。 1234567891011# check if optimization is enabledIn [5]: cv.useOptimized()Out[5]: TrueIn [6]: %timeit res = cv.medianBlur(img,49)10 loops, best of 3: 34.9 ms per loop# Disable itIn [7]: cv.setUseOptimized(False)In [8]: cv.useOptimized()Out[8]: FalseIn [9]: %timeit res = cv.medianBlur(img,49)10 loops, best of 3: 64.1 ms per loop 请参阅，优化的中值滤波比未优化的版本快2倍。如果检查其来源，则可以看到中值滤波已进行SIMD优化。因此，您可以使用它在代码顶部启用优化（请记住默认情况下已启用）。 在IPython中评估性能有时您可能需要比较两个类似操作的性能。IPython为您提供了一个神奇的命令计时器来执行此操作。它会多次运行代码以获得更准确的结果。同样，它们适用于测量单行代码。 例如，您知道以下哪个加法运算更好，x = 5; y = x ** 2，x = 5; y = x * x，x = np.uint8（[5]）; y = x * x或y = np.square（x）？我们将在IPython shell中使用timeit找到它。 12345678910In [10]: x = 5In [11]: %timeit y=x**210000000 loops, best of 3: 73 ns per loopIn [12]: %timeit y=x*x10000000 loops, best of 3: 58.3 ns per loopIn [15]: z = np.uint8([5])In [17]: %timeit y=z*z1000000 loops, best of 3: 1.25 us per loopIn [19]: %timeit y=np.square(z)1000000 loops, best of 3: 1.16 us per loop 您可以看到x = 5; y = x * x最快，比Numpy快20倍左右。如果您还考虑阵列的创建，它可能会快100倍。酷吧？（大量开发人员正在研究此问题） 注意:Python标量运算比Numpy标量运算快。因此，对于包含一两个元素的运算，Python标量比Numpy数组要好。当数组大小稍大时，Numpy会占优势。 我们将再尝试一个示例。这次，我们将比较同一图片的cv.countNonZero()和np.count_nonzero()性能。 1234In [35]: %timeit z = cv.countNonZero(img)100000 loops, best of 3: 15.8 us per loopIn [36]: %timeit z = np.count_nonzero(img)1000 loops, best of 3: 370 us per loop 可以看出，OpenCV功能比Numpy功能快25倍。 注意:通常，OpenCV函数比Numpy函数要快。因此，对于相同的操作，首选OpenCV功能。但是，可能会有例外，尤其是当Numpy处理视图而不是副本时。 更多IPython魔术命令还有其他一些魔术命令可以用来测量性能，性能分析，行性能分析，内存测量等。它们都有很好的文档记录。 性能优化技术有几种技术和编码方法可以充分利用Python和Numpy的性能。此处仅记录相关内容，并链接到重要资源。这里要注意的主要事情是，首先尝试以一种简单的方式实现该算法。工作正常后，对其进行概要分析，找到瓶颈并对其进行优化。 尽量避免在Python中使用循环，尤其是双/三重循环等。它们本来就很慢。 由于Numpy和OpenCV已针对向量运算进行了优化，因此将算法/代码向量化到最大程度。 利用缓存一致性。 除非需要，否则切勿制作数组的副本。尝试改用视图。阵列复制是一项昂贵的操作。即使执行了所有这些操作后，如果您的代码仍然很慢，或者不可避免地需要使用大循环，请使用Cython等其他库来使其更快。]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV中的Gui功能]]></title>
    <url>%2Fopencv%2FOpenCV%E4%B8%AD%E7%9A%84Gui%E5%8A%9F%E8%83%BD.html</url>
    <content type="text"><![CDATA[图像入门目标 在这里，您将学习如何读取图像，如何显示图像以及如何将其保存回 您将学习以下功能：cv.imread()，cv.imshow()，cv.imwrite() （可选）您将学习如何使用Matplotlib显示图像 读取图像使用函数cv.imread()读取图像第二个参数是一个标志，用于指定应读取图像的方式。 cv.IMREAD_COLOR：加载彩色图像。图像的任何透明度都将被忽略。这是默认标志。 cv.IMREAD_GRAYSCALE：以灰度模式加载图像 cv.IMREAD_UNCHANGED：加载图像，包括alpha通道 警告,即使图像路径错误，它也不会引发任何错误，但是print img会给您None 除了这三个标志，您可以分别简单地传递整数1、0或-1。 示例123import cv2 as cvimg = cv.imread('images/lena.jpg',0) 显示图像使用函数cv.imshow()在窗口中显示图像。窗口自动适合图像尺寸。 第一个参数是窗口名称，它是一个字符串。第二个参数是我们的图像。 示例123456import cv2 as cvimg = cv.imread('images/lena.jpg',0)cv.imshow('lena',img)cv.waitKey(0)cv.destroyAllWindows() cv.waitKey()是键盘绑定功能。它的参数是时间（以毫秒为单位）。该函数将为任何键盘事件等待指定的毫秒数。如果在此期间按任意键，程序将继续。如果传递 0，它将无限期地等待击键。还可以将其设置为检测特定的按键 除了绑定键盘事件之外，此功能还处理许多其他GUI事件，因此必须使用它来实际显示图像。 cv.destroyAllWindows()只会破坏我们创建的所有窗口。如果要销毁任何特定的窗口，请使用函数 cv.destroyWindow()在其中传递确切的窗口名称作为参数。 在一种特殊情况下，您已经可以创建一个窗口并稍后将图像加载到该窗口。在这种情况下，您可以指定窗口是否可调整大小。这是通过功能cv.namedWindow（）完成的。默认情况下，该标志为cv.WINDOW_AUTOSIZE。但是，如果将flag指定为cv.WINDOW_NORMAL，则可以调整窗口大小。当图像尺寸过大并将跟踪栏添加到窗口时，这将很有帮助。 1234cv.namedWindow（'image'，cv.WINDOW_NORMAL）cv.imshow（'image'，img）cv.waitKey（0）cv.destroyAllWindows（） 写图像使用函数cv.imwrite()保存图像。 第一个参数是文件名，第二个参数是您要保存的图像。 1cv.imwrite（&apos;messigray.png&apos;，img） 这会将图像以PNG格式保存在工作目录中。 总结一下下面的程序以灰度加载图像，显示它，如果按“ s”保存该图像然后退出，或者如果按ESC键只是退出而不保存即可 12345678910import cv2 as cvimg = cv.imread('images/lena.jpg',0)cv.imshow('image',img)k = cv.waitKey(0)if k == ord('s'): cv.imwrite('test1.jpg',img)elif k == 27: cv.destroyAllWindows() 如果使用的是64位计算机，则必须k = cv.waitKey(0)按如下所示修改行：k = cv.waitKey(0) &amp; 0xFF OpenCV加载的彩色图像处于BGR模式。但是Matplotlib以RGB模式显示。 视频入门目标 学习阅读视频，显示视频和保存视频。 学习从相机捕捉并显示它。 您将学习以下功能：cv.VideoCapture()，cv.VideoWriter() 从相机捕获视频要捕获视频，您需要创建一个VideoCapture对象。它的参数可以是设备索引或视频文件的名称。设备索引只是指定哪个摄像机的编号。通常，将连接一台摄像机（以我的情况为例）。所以我只是传递0（或-1）。您可以通过传递1来选择第二台摄像机，依此类推。之后，您可以逐帧捕获。但是最后，不要忘记释放捕获。 123456789101112131415161718import cv2 as cvcap = cv.VideoCapture(0)if not cap.isOpened(): print("Cannot open camera") exit()while True: ret , frame = cap.read() if not ret: print("Can't receive frame (stream end?). Exiting ...") break gray = cv.cvtColor(frame,cv.COLOR_BGR2GRAY) cv.imshow('frame',gray) if cv.waitKey(1) == ord('q'): breakcap.release()cv.destroyAllWindows() cap.read()返回布尔值（True/ False）。如果正确读取了帧，它将为True。因此，您可以通过检查此返回值来检查视频的结尾。 有时，cap可能尚未初始化捕获。在这种情况下，此代码显示错误。您可以通过cap.isOpened()方法检查它是否已初始化。如果是True，那么确定。否则，使用cap.open()打开它。 您还可以使用cap.get（propId）方法访问此视频的某些功能，其中propId是0到18之间的一个数字。每个数字表示视频的属性（如果适用于该视频），并且可以显示完整的详细信息在这里看到：cv :: VideoCapture :: get()。其中一些值可以使用cap.set（propId，value）进行修改。价值是您想要的新价值。 例如，我可以通过和检查框架的宽度和高度。默认情况下，它的分辨率为640x480。但我想将其修改为320x240。只需使用和即可。 1234cap.get(cv.CAP_PROP_FRAME_WIDTH)cap.get(cv.CAP_PROP_FRAME_HEIGHT)ret = cap.set(cv.CAP_PROP_FRAME_WIDTH,320)ret = cap.set(cv.CAP_PROP_FRAME_HEIGHT,240) 从文件播放视频它与从摄像机捕获相同，只是用视频文件名更改摄像机索引。另外，在显示框架时，请使用适当的时间cv.waitKey()。如果太少，则视频将非常快，而如果太高，则视频将变得很慢（嗯，这就是显示慢动作的方式）。正常情况下25毫秒就可以了。 123456789101112131415import numpy as npimport cv2 as cvcap = cv.VideoCapture('vtest.avi')while cap.isOpened(): ret, frame = cap.read() # if frame is read correctly ret is True if not ret: print("Can't receive frame (stream end?). Exiting ...") break gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) cv.imshow('frame', gray) if cv.waitKey(1) == ord('q'): breakcap.release()cv.destroyAllWindows() 注意:确保安装了正确版本的ffmpeg或gstreamer。有时，使用Video Capture令人头疼，主要是由于ffmpeg / gstreamer安装错误。 保存视频因此，我们捕获了一个视频，对其进行逐帧处理，然后我们希望保存该视频。对于图像，这非常简单，只需使用即可cv.imwrite()。这里需要做更多的工作。 这次我们创建一个VideoWriter对象。我们应该指定输出文件名（例如：output.avi）。然后，我们应指定FourCC代码（下一段中的详细信息）。然后应传递每秒的帧数（fps）和帧大小。最后一个是isColor标志。如果为True，则编码器需要彩色框，否则将与灰度框一起使用。 FourCC是一个4字节的代码，用于指定视频编解码器。可用代码列表可在fourcc.org中找到。它取决于平台。遵循编解码器对我来说效果很好。 在Fedora中：DIVX，XVID，MJPG，X264，WMV1，WMV2。（最好使用XVID。MJPG可以生成大尺寸的视频。X264提供非常小的尺寸的视频） 在Windows中：DIVX（尚待测试和添加） 在OSX中：MJPG（.mp4），DIVX（.avi），X264（.mkv）。FourCC代码作为MJPG的cv.VideoWriter_fourcc（&#39;M&#39;，&#39;J&#39;，&#39;P&#39;，&#39;G&#39;）or cv.VideoWriter_fourcc（*&#39;MJPG&#39;）传递。 在从摄像机捕获的代码下面，沿垂直方向翻转每一帧并保存。 1234567891011121314151617181920212223import cv2 as cvcap = cv.VideoCapture(0)fourcc = cv.VideoWriter_fourcc(*'XVID')out = cv.VideoWriter('output.avi',fourcc,20.0,(640,480))while cap.isOpened(): ret , frame = cap.read() if not ret: print("Can't receive frame (stream end?). Exiting ...") break frame = cv.flip(frame,0) out.write(frame) cv.imshow('frame',frame) if cv.waitKey(1) == ord('q'): breakcap.release()out.release()cv.destroyAllWindows() frame = cv.flip(frame,0)是垂直翻转，改成1是水平翻转，例如默认情况下，我的电脑摄像头成像出来是左右相反的，这时候弄成水平翻转，有助于我们欣赏自己的容颜，尤其工作中用到的很多摄像头都需要这样处理一下 OpenCV中的绘图功能目标 学习使用OpenCV绘制不同的几何形状 您将学习以下功能：cv.line()，cv.circle()，cv.rectangle()，cv.ellipse()，cv.putText()等。 在上述所有功能中，您将看到一些常见的参数，如下所示： img：您要绘制形状的图像 color：形状的颜色。对于BGR，将其作为元组传递，例如：（255,0,0）对于蓝色。对于灰度，只需传递标量值即可。 thickness：线或圆等的粗细。如果对闭合图形（如圆）传递*-1 *，它将填充形状。默认厚度= 1 lineType：线的类型，是否为8连接线，抗锯齿线等。默认情况下，为8连接线。 cv.LINE_AA给出了抗锯齿的线条，看起来非常适合曲线。 画线要绘制一条线，您需要传递线的开始和结束坐标。我们将创建一个黑色图像，并从左上角到右下角在其上绘制一条蓝线。 123456import cv2 as cvimport numpy as npimg = np.zeros((512,512,3),np.uint8)cv.line(img,(0,0),(511,511),(255,0,0),5) 绘制矩形要绘制矩形，您需要矩形的左上角和右下角。这次，我们将在图像的右上角绘制一个绿色矩形。 1cv.rectangle(img,(384,0),(510,128),(0,255,0),3) 绘制圆形需要其中心坐标和半径 1cv.circle(img,(447,63), 63, (0,0,255), -1) 绘制椭圆需要传递几个参数。一个参数是中心位置（x，y）。下一个参数是轴长度（长轴长度，短轴长度）。angle是椭圆沿逆时针方向旋转的角度。startAngle和endAngle表示从主轴沿顺时针方向测量的椭圆弧的开始和结束。即给出0和360给出完整的椭圆。 1cv.ellipse(img,(256,256),(100,50),0,0,180,255,-1) 绘图多边形首先需要顶点的坐标。将这些点组成形状为ROWSx1x2的数组，其中ROWS是顶点数，并且其类型应为int32。在这里，我们绘制了一个带有四个顶点的黄色小多边形。 123pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)pts = pts.reshape((-1,1,2))cv.polylines(img,[pts],True,(0,255,255)) 如果第三个参数为False，您将获得一条连接所有点的折线，而不是闭合形状。cv.polylines()可用于绘制多条线。只需创建要绘制的所有线条的列表，然后将其传递给函数即可。所有线条将单独绘制。与为每条线调用cv.line()相比，绘制一组线是一种更好，更快的方法。 向图像添加文本要将文本放入图像中，需要指定以下内容。 您要写入的文字数据 您要放置它的位置坐标（即数据开始的左下角）。 字体类型（检查cv.putText()文档以获取受支持的字体） 字体比例（指定字体大小） 常规的内容，例如颜色等。为了更好看，建议使用lineType = cv.LINE_AA。 12font = cv.FONT_HERSHEY_SIMPLEXcv.putText(img,&apos;OpenCV&apos;,(10,500), font, 4,(255,255,255),2,cv.LINE_AA) 总结结合之前的图像知识对绘图进行总结并保存结果图像 123456789101112131415161718192021222324import numpy as npimport cv2 as cvimg = np.zeros((512,512,3),np.uint8)cv.line(img,(0,0),(511,511),(255,0,0),5)cv.rectangle(img,(384,0),(510,128),(0,255,0),3)cv.circle(img,(447,63),63,(0,0,255),-1)cv.ellipse(img,(256,256),(100,50),0,0,180,255,-1)pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)pts = pts.reshape((-1,1,2))cv.polylines(img,[pts],True,(0,255,255))font = cv.FONT_HERSHEY_SIMPLEXcv.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,cv.LINE_AA)cv.imshow('img',img)cv.waitKey(0)cv.destroyAllWindows()cv.imwrite('drawing.jpg',img) 1True 把上面保存的drawing.jpg显示出来： 用鼠标画东西目标 了解如何在OpenCV中处理鼠标事件 您将学习以下功能：cv.setMouseCallback() 简单演示在这里，我们创建一个简单的应用程序，无论我们在哪里双击它，都可以在图像上绘制一个圆。 首先，我们创建一个鼠标回调函数，该函数在发生鼠标事件时执行。鼠标事件可以是与鼠标相关的任何东西，例如左键按下，左键按下，左键双击等。它为我们提供了每个鼠标事件的坐标（x，y）。通过此活动和地点，我们可以做任何我们喜欢的事情。要列出所有可用的可用事件，请在Python终端中运行以下代码： 123import cv2 as cvevents = [i for i in dir(cv) if 'EVENT' in i]print(events) 1[&apos;EVENT_FLAG_ALTKEY&apos;, &apos;EVENT_FLAG_CTRLKEY&apos;, &apos;EVENT_FLAG_LBUTTON&apos;, &apos;EVENT_FLAG_MBUTTON&apos;, &apos;EVENT_FLAG_RBUTTON&apos;, &apos;EVENT_FLAG_SHIFTKEY&apos;, &apos;EVENT_LBUTTONDBLCLK&apos;, &apos;EVENT_LBUTTONDOWN&apos;, &apos;EVENT_LBUTTONUP&apos;, &apos;EVENT_MBUTTONDBLCLK&apos;, &apos;EVENT_MBUTTONDOWN&apos;, &apos;EVENT_MBUTTONUP&apos;, &apos;EVENT_MOUSEHWHEEL&apos;, &apos;EVENT_MOUSEMOVE&apos;, &apos;EVENT_MOUSEWHEEL&apos;, &apos;EVENT_RBUTTONDBLCLK&apos;, &apos;EVENT_RBUTTONDOWN&apos;, &apos;EVENT_RBUTTONUP&apos;] 在我们双击的地方绘制一个圆圈 123456789101112131415import numpy as npimport cv2 as cv# mouse callback functiondef draw_circle(event,x,y,flags,param): if event == cv.EVENT_LBUTTONDBLCLK: cv.circle(img,(x,y),100,(255,0,0),-1)# Create a black image, a window and bind the function to windowimg = np.zeros((512,512,3), np.uint8)cv.namedWindow('image')cv.setMouseCallback('image',draw_circle)while(1): cv.imshow('image',img) if cv.waitKey(20) &amp; 0xFF == 27: breakcv.destroyAllWindows() 更高级的演示我们像在“画图”应用程序中一样，通过拖动鼠标来绘制矩形或圆形（取决于我们选择的模式）。因此，我们的鼠标回调函数有两个部分，一个用于绘制矩形，另一个用于绘制圆形。这个特定的示例对于创建和理解一些交互式应用程序（如对象跟踪，图像分割等）将非常有帮助。 1234567891011121314151617181920212223242526272829303132333435import numpy as npimport cv2 as cvdrawing = False # true if mouse is pressedmode = True # if True, draw rectangle. Press 'm' to toggle to curveix,iy = -1,-1# mouse callback functiondef draw_circle(event,x,y,flags,param): global ix,iy,drawing,mode if event == cv.EVENT_LBUTTONDOWN: drawing = True ix,iy = x,y elif event == cv.EVENT_MOUSEMOVE: if drawing == True: if mode == True: cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1) else: cv.circle(img,(x,y),5,(0,0,255),-1) elif event == cv.EVENT_LBUTTONUP: drawing = False if mode == True: cv.rectangle(img,(ix,iy),(x,y),(0,255,0),-1) else: cv.circle(img,(x,y),5,(0,0,255),-1)img = np.zeros((512,512,3), np.uint8)cv.namedWindow('image')cv.setMouseCallback('image',draw_circle)while(1): cv.imshow('image',img) k = cv.waitKey(1) &amp; 0xFF if k == ord('m'): mode = not mode elif k == 27: breakcv.destroyAllWindows() 轨迹栏作为调色板目标 了解将轨迹栏绑定到OpenCV窗口 您将学习以下功能：cv.getTrackbarPos()，cv.createTrackbar()等。 代码演示在这里，我们将创建一个简单的应用程序，以显示您指定的颜色。您有一个显示颜色的窗口，以及三个用于指定B，G，R颜色的跟踪栏。滑动轨迹栏，并相应地更改窗口颜色。默认情况下，初始颜色将设置为黑色。 对于cv.getTrackbarPos()函数，第一个参数是轨迹栏名称，第二个参数是它附加到的窗口名称，第三个参数是默认值，第四个参数是最大值，第五个是执行的回调函数每次跟踪栏值更改。回调函数始终具有默认参数，即轨迹栏位置。在我们的例子中，函数什么都不做，所以我们简单地通过。 轨迹栏的另一个重要应用是将其用作按钮或开关。默认情况下，OpenCV不具有按钮功能。因此，您可以使用跟踪栏来获得此类功能。在我们的应用程序中，我们创建了一个开关，只有在该开关为ON的情况下该应用程序才能运行，否则屏幕始终为黑色。 1234567891011121314151617181920212223242526272829import numpy as npimport cv2 as cvdef nothing(x): pass# Create a black image, a windowimg = np.zeros((300,512,3), np.uint8)cv.namedWindow('image')# create trackbars for color changecv.createTrackbar('R','image',0,255,nothing)cv.createTrackbar('G','image',0,255,nothing)cv.createTrackbar('B','image',0,255,nothing)# create switch for ON/OFF functionalityswitch = '0 : OFF \n1 : ON'cv.createTrackbar(switch, 'image',0,1,nothing)while(1): cv.imshow('image',img) k = cv.waitKey(1) &amp; 0xFF if k == 27: break # get current positions of four trackbars r = cv.getTrackbarPos('R','image') g = cv.getTrackbarPos('G','image') b = cv.getTrackbarPos('B','image') s = cv.getTrackbarPos(switch,'image') if s == 0: img[:] = 0 else: img[:] = [b,g,r]cv.destroyAllWindows() 效果如下：]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[OpenCV-Python教程简介]]></title>
    <url>%2Fopencv%2FOpenCV-Python%E7%AE%80%E4%BB%8B.html</url>
    <content type="text"><![CDATA[OpenCV-Python教程简介 参考自官方文档 为了对过往的使用作一个总结，也方便以后自己复习，对官方的示例一一进行了复现学习，第一遍先用python来大致了解一下API，整体熟悉后，准备再用C++过一遍。OpenCVOpenCV由加里·布拉德斯基（Gary Bradsky）于1999年在英特尔创立，第一版于2000年发布。瓦迪姆 ·皮萨列夫斯基（Vadim Pisarevsky）与加里·布拉德斯基（Gary Bradsky）一起管理英特尔的俄罗斯软件OpenCV团队。2005年，OpenCV用于Stanley，该车赢得了2005年DARPA大挑战赛的冠军。后来，在Willow Garage的支持下，它的积极发展得以继续，Gary Bradsky和Vadim Pisarevsky领导了该项目。OpenCV现在支持与计算机视觉和机器学习有关的多种算法，并且正在日益扩展。 OpenCV支持多种编程语言，例如C ++，Python，Java等，并且可在Windows，Linux，OS X，Android和iOS等不同平台上使用。基于CUDA和OpenCL的高速GPU操作接口也在积极开发中。 OpenCV-Python是用于OpenCV的Python API，结合了OpenCV C ++ API和Python语言的最佳质量。 OpenCV中的PythonOpenCV-Python是旨在解决计算机视觉问题的Python绑定库。 与C / C ++之类的语言相比，Python速度较慢。也就是说，可以使用C / C ++轻松扩展Python，这使我们可以用C / C ++编写计算密集型代码并创建可用作Python模块的Python包装器。这给我们带来了两个好处：首先，代码与原始C / C ++代码一样快（因为它是在后台运行的实际C ++代码），其次，在Python中比C / C ++编写代码更容易。OpenCV-Python是原始OpenCV C ++实现的Python包装器。 OpenCV-Python使用Numpy，这是一个高度优化的库，用于使用MATLAB风格的语法进行数值运算。所有OpenCV数组结构都与Numpy数组相互转换。这也使与使用Numpy的其他库（例如SciPy和Matplotlib）的集成变得更加容易。 OpenCV-Python教程本指南主要针对OpenCV 3.x版本，我这里使用的是4.1，如果实现示例的过程中遇到问题，会进行说明。 OpenCV-Python安装pip安装比较简单，一行命令，确保自己改了pip源，不会改的请访问我的ubuntu16.04装机教程 1pip install opencv-python 在开始之前，请确认一下自己的版本： 12In [7]:cv2.__version__Out[7]: '4.1.1' 教程目录 OpenCV简介 了解如何在计算机上设置OpenCV-Python！ OpenCV中的Gui功能 在这里，您将学习如何显示和保存图像和视频，控制鼠标事件以及创建轨迹栏。 核心操作 在本部分中，您将学习图像的基本操作，例如像素编辑，几何变换，代码优化，一些数学工具等。 OpenCV中的图像处理 在本节中，您将学习OpenCV内部的不同图像处理功能。 特征检测与描述 在本节中，您将学习有关特征检测器和描述符的信息 视频分析（视频模块） 在本部分中，您将学习与对象跟踪等视频配合使用的不同技术。 相机校准和3D重建 在本节中，我们将学习有关相机校准，立体成像等的信息。 机器学习 在本节中，您将学习OpenCV内部的不同图像处理功能。 计算摄影 在本节中，您将学习不同的计算摄影技术，例如图像去噪等。 对象检测（objdetect模块） 在本节中，您将学习对象检测技术，例如面部检测等。 OpenCV-Python绑定 在本节中，我们将了解如何生成OpenCV-Python绑定]]></content>
      <categories>
        <category>OpenCV-Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CUDA编程之快速入门]]></title>
    <url>%2FCUDA_Coding%2FCUDA%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[CUDA编程之快速入门CUDA（Compute Unified Device Architecture）的中文全称为计算统一设备架构。做图像视觉领域的同学多多少少都会接触到CUDA，毕竟要做性能速度优化，CUDA是个很重要的工具，CUDA是做视觉的同学难以绕过的一个坑，必须踩一踩才踏实。CUDA编程真的是入门容易精通难，具有计算机体系结构和C语言编程知识储备的同学上手CUDA编程应该难度不会很大。本文章将通过以下五个方面帮助大家比较全面地了解CUDA编程最重要的知识点，做到快速入门： GPU架构特点 CUDA线程模型 CUDA内存模型 CUDA编程模型 CUDA应用小例子 1. GPU架构特点首先我们先谈一谈串行计算和并行计算。我们知道，高性能计算的关键利用多核处理器进行并行计算。 当我们求解一个计算机程序任务时，我们很自然的想法就是将该任务分解成一系列小任务，把这些小任务一一完成。在串行计算时，我们的想法就是让我们的处理器每次处理一个计算任务，处理完一个计算任务后再计算下一个任务，直到所有小任务都完成了，那么这个大的程序任务也就完成了。如下图所示，就是我们怎么用串行编程思想求解问题的步骤。但是串行计算的缺点非常明显，如果我们拥有多核处理器，我们可以利用多核处理器同时处理多个任务时，而且这些小任务并没有关联关系（不需要相互依赖，比如我的计算任务不需要用到你的计算结果），那我们为什么还要使用串行编程呢？为了进一步加快大任务的计算速度，我们可以把一些独立的模块分配到不同的处理器上进行同时计算（这就是并行），最后再将这些结果进行整合，完成一次任务计算。下图就是将一个大的计算任务分解为小任务，然后将独立的小任务分配到不同处理器进行并行计算，最后再通过串行程序把结果汇总完成这次的总的计算任务。所以，一个程序可不可以进行并行计算，关键就在于我们要分析出该程序可以拆分出哪几个执行模块，这些执行模块哪些是独立的，哪些又是强依赖强耦合的，独立的模块我们可以试着设计并行计算，充分利用多核处理器的优势进一步加速我们的计算任务，强耦合模块我们就使用串行编程，利用串行+并行的编程思路完成一次高性能计算。 接下来我们谈谈CPU和GPU有什么区别，他们俩各自有什么特点，我们在谈并行、串行计算时多次谈到“多核”的概念，现在我们先从“核”的角度开始这个话题。首先CPU是专为顺序串行处理而优化的几个核心组成。而GPU则由数以千计的更小、更高效的核心组成，这些核心专门为同时处理多任务而设计，可高效地处理并行任务。也就是，CPU虽然每个核心自身能力极强，处理任务上非常强悍，无奈他核心少，在并行计算上表现不佳；反观GPU，虽然他的每个核心的计算能力不算强，但他胜在核心非常多，可以同时处理多个计算任务，在并行计算的支持上做得很好。 GPU和CPU的不同硬件特点决定了他们的应用场景，CPU是计算机的运算和控制的核心，GPU主要用作图形图像处理。图像在计算机呈现的形式就是矩阵，我们对图像的处理其实就是操作各种矩阵进行计算，而很多矩阵的运算其实可以做并行化，这使得图像处理可以做得很快，因此GPU在图形图像领域也有了大展拳脚的机会。下图表示的就是一个多GPU计算机硬件系统，可以看出，一个GPU内存就有很多个SP和各类内存，这些硬件都是GPU进行高效并行计算的基础。现在再从数据处理的角度来对比CPU和GPU的特点。CPU需要很强的通用性来处理各种不同的数据类型，比如整型、浮点数等，同时它又必须擅长处理逻辑判断所导致的大量分支跳转和中断处理，所以CPU其实就是一个能力很强的伙计，他能把很多事处理得妥妥当当，当然啦我们需要给他很多资源供他使用（各种硬件），这也导致了CPU不可能有太多核心（核心总数不超过16）。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境，GPU有非常多核心（费米架构就有512核），虽然其核心的能力远没有CPU的核心强，但是胜在多，在处理简单计算任务时呈现出“人多力量大”的优势，这就是并行计算的魅力。 整理一下两者特点就是： CPU：擅长流程控制和逻辑处理，不规则数据结构，不可预测存储结构，单线程程序，分支密集型算法 GPU：擅长数据并行计算，规则数据结构，可预测存储模式 现在的计算机体系架构中，要完成CUDA并行计算，单靠GPU一人之力是不能完成计算任务的，必须借助CPU来协同配合完成一次高性能的并行计算任务。 一般而言，并行部分在GPU上运行，串行部分在CPU运行，这就是异构计算。具体一点，异构计算的意思就是不同体系结构的处理器相互协作完成计算任务。CPU负责总体的程序流程，而GPU负责具体的计算任务，当GPU各个线程完成计算任务后，我们就将GPU那边计算得到的结果拷贝到CPU端，完成一次计算任务。 所以应用程序利用GPU实现加速的总体分工就是：密集计算代码（约占5%的代码量）由GPU负责完成，剩余串行代码由CPU负责执行。 2. CUDA线程模型下面我们介绍CUDA的线程组织结构。首先我们都知道，线程是程序执行的最基本单元，CUDA的并行计算就是通过成千上万个线程的并行执行来实现的。下面的机构图说明了GPU的不同层次的结构。CUDA的线程模型从小往大来总结就是： Thread：线程，并行的基本单位 Thread Block：线程块，互相合作的线程组，线程块有如下几个特点： 允许彼此同步 可以通过共享内存快速交换数据 以1维、2维或3维组织 Grid：一组线程块 以1维、2维组织 共享全局内存 Kernel：在GPU上执行的核心程序，这个kernel函数是运行在某个Grid上的。 One kernel &lt;-&gt; One Grid每一个block和每个thread都有自己的ID，我们通过相应的索引找到相应的线程和线程块。 threadIdx，blockIdx Block ID: 1D or 2D Thread ID: 1D, 2D or 3D理解kernel，必须要对kernel的线程层次结构有一个清晰的认识。首先GPU上很多并行化的轻量级线程。kernel在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，grid是线程结构的第一层次，而网格又可以分为很多线程块（block），一个线程块里面包含很多线程，这是第二个层次。线程两层组织结构如上图所示，这是一个gird和block均为2-dim的线程组织。grid和block都是定义为dim3类型的变量，dim3可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block可以灵活地定义为1-dim，2-dim以及3-dim结构，kernel调用时也必须通过执行配置&lt;&lt;&lt;grid, block&gt;&gt;&gt;来指定kernel所使用的网格维度和线程块维度。举个例子，我们以上图为例，分析怎么通过&lt;&lt;&lt;grid,block&gt;&gt;&gt;&gt;这种标记方式索引到我们想要的那个线程。CUDA的这种&lt;&lt;&lt;grid,block&gt;&gt;&gt;其实就是一个多级索引的方法，第一级索引是(grid.xIdx, grid.yIdy)，对应上图例子就是(1, 1)，通过它我们就能找到了这个线程块的位置，然后我们启动二级索引(block.xIdx, block.yIdx, block.zIdx)来定位到指定的线程。这就是我们CUDA的线程组织结构。 这里想谈谈SP和SM（流处理器），很多人会被这两个专业名词搞得晕头转向。 SP：最基本的处理单元，streaming processor，也称为CUDA core。最后具体的指令和任务都是在SP上处理的。GPU进行并行计算，也就是很多个SP同时做处理。 SM：多个SP加上其他的一些资源组成一个streaming multiprocessor。也叫GPU大核，其他资源如：warp scheduler，register，shared memory等。SM可以看做GPU的心脏（对比CPU核心），register和shared memory是SM的稀缺资源。CUDA将这些资源分配给所有驻留在SM中的threads。因此，这些有限的资源就使每个SM中active warps有非常严格的限制，也就限制了并行能力。需要指出，每个SM包含的SP数量依据GPU架构而不同，Fermi架构GF100是32个，GF10X是48个，Kepler架构都是192个，Maxwell都是128个。 简而言之，SP是线程执行的硬件单位，SM中包含多个SP，一个GPU可以有多个SM（比如16个），最终一个GPU可能包含有上千个SP。这么多核心“同时运行”，速度可想而知，这个引号只是想表明实际上，软件逻辑上是所有SP是并行的，但是物理上并不是所有SP都能同时执行计算（比如我们只有8个SM却有1024个线程块需要调度处理），因为有些会处于挂起，就绪等其他状态，这有关GPU的线程调度。 下面这个图将从硬件角度和软件角度解释CUDA的线程模型。 每个线程由每个线程处理器（SP）执行 线程块由多核处理器（SM）执行 一个kernel其实由一个grid来执行，一个kernel一次只能在一个GPU上执行block是软件概念，一个block只会由一个sm调度，程序员在开发时，通过设定block的属性，告诉GPU硬件，我有多少个线程，线程怎么组织。而具体怎么调度由sm的warps scheduler负责，block一旦被分配好SM，该block就会一直驻留在该SM中，直到执行结束。一个SM可以同时拥有多个blocks，但需要序列执行。下图显示了GPU内部的硬件架构： 3. CUDA内存模型CUDA中的内存模型分为以下几个层次： 每个线程都用自己的registers（寄存器） 每个线程都有自己的local memory（局部内存） 每个线程块内都有自己的shared memory（共享内存），所有线程块内的所有线程共享这段内存资源 每个grid都有自己的global memory（全局内存），不同线程块的线程都可使用 每个grid都有自己的constant memory（常量内存）和texture memory（纹理内存），），不同线程块的线程都可使用线程访问这几类存储器的速度是register &gt; local memory &gt;shared memory &gt; global memory 下面这幅图表示就是这些内存在计算机架构中的所在层次。 4. CUDA编程模型上面讲了这么多硬件相关的知识点，现在终于可以开始说说CUDA是怎么写程序的了。 我们先捋一捋常见的CUDA术语：第一个要掌握的编程要点：我们怎么写一个能在GPU跑的程序或函数呢？ 通过关键字就可以表示某个程序在CPU上跑还是在GPU上跑！如下表所示，比如我们用global定义一个kernel函数，就是CPU上调用，GPU上执行，注意global函数的返回值必须设置为void。第二个编程要点：CPU和GPU间的数据传输怎么写？ 首先介绍在GPU内存分配回收内存的函数接口： cudaMalloc(): 在设备端分配global memory cudaFree(): 释放存储空间CPU的数据和GPU端数据做数据传输的函数接口是一样的，他们通过传递的函数实参（枚举类型）来表示传输方向： cudaMemcpy(void dst, void src, size_t nbytes,enum cudaMemcpyKind direction) enum cudaMemcpyKind: cudaMemcpyHostToDevice（CPU到GPU） cudaMemcpyDeviceToHost（GPU到CPU） cudaMemcpyDeviceToDevice（GPU到GPU） 第三个编程要点：怎么用代码表示线程组织模型？我们可以用dim3类来表示网格和线程块的组织方式，网格grid可以表示为一维和二维格式，线程块block可以表示为一维、二维和三维的数据格式。 12dim3 DimGrid(100, 50); //5000个线程块，维度是100*50dim3 DimBlock(4, 8, 8); //每个线层块内包含256个线程，线程块内的维度是4*8*8 接下来介绍一个非常重要又很难懂的一个知识点，我们怎么计算线程号呢？ 1.使用N个线程块，每一个线程块只有一个线程，即 12dim3 dimGrid(N);dim3 dimBlock(1); 此时的线程号的计算方式就是 1threadId = blockIdx.x; 其中threadId的取值范围为0到N-1。对于这种情况，我们可以将其看作是一个列向量，列向量中的每一行对应一个线程块。列向量中每一行只有1个元素，对应一个线程。 2.使用M×N个线程块，每个线程块1个线程由于线程块是2维的，故可以看做是一个M*N的2维矩阵，其线程号有两个维度，即： 12dim3 dimGrid(M,N);dim3 dimBlock(1); 其中 12blockIdx.x 取值0到M-1blcokIdx.y 取值0到N-1 这种情况一般用于处理2维数据结构，比如2维图像。每一个像素用一个线程来处理，此时需要线程号来映射图像像素的对应位置，如 1pos = blockIdx.y * blcokDim.x + blockIdx.x; //其中gridDim.x等于M 3.使用一个线程块，该线程具有N个线程，即 12dim3 dimGrid(1);dim3 dimBlock(N); 此时线程号的计算方式为 1threadId = threadIdx.x; 其中threadId的范围是0到N-1，对于这种情况，可以看做是一个行向量，行向量中的每一个元素的每一个元素对应着一个线程。 4.使用M个线程块，每个线程块内含有N个线程，即 12dim3 dimGrid(M);dim3 dimBlock(N); 这种情况，可以把它想象成二维矩阵，矩阵的行与线程块对应，矩阵的列与线程编号对应，那线程号的计算方式为 1threadId = threadIdx.x + blcokIdx*blockDim.x; 上面其实就是把二维的索引空间转换为一维索引空间的过程。 5.使用M×N的二维线程块，每一个线程块具有P×Q个线程，即 12dim3 dimGrid(M, N);dim3 dimBlock(P, Q); 这种情况其实是我们遇到的最多情况，特别适用于处理具有二维数据结构的算法，比如图像处理领域。 其索引有两个维度 12threadId.x = blockIdx.x*blockDim.x+threadIdx.x;threadId.y = blockIdx.y*blockDim.y+threadIdx.y; 上述公式就是把线程和线程块的索引映射为图像像素坐标的计算方法。 CUDA应用例子我们已经掌握了CUDA编程的基本语法，现在我们开始以一些小例子来真正上手CUDA。 首先我们编写一个程序，查看我们GPU的一些硬件配置情况。 12345678910111213141516171819202122232425#include "device_launch_parameters.h"#include &lt;iostream&gt;int main()&#123; int deviceCount; cudaGetDeviceCount(&amp;deviceCount); for(int i=0;i&lt;deviceCount;i++) &#123; cudaDeviceProp devProp; cudaGetDeviceProperties(&amp;devProp, i); std::cout &lt;&lt; "使用GPU device " &lt;&lt; i &lt;&lt; ": " &lt;&lt; devProp.name &lt;&lt; std::endl; std::cout &lt;&lt; "设备全局内存总量： " &lt;&lt; devProp.totalGlobalMem / 1024 / 1024 &lt;&lt; "MB" &lt;&lt; std::endl; std::cout &lt;&lt; "SM的数量：" &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl; std::cout &lt;&lt; "每个线程块的共享内存大小：" &lt;&lt; devProp.sharedMemPerBlock / 1024.0 &lt;&lt; " KB" &lt;&lt; std::endl; std::cout &lt;&lt; "每个线程块的最大线程数：" &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; "设备上一个线程块（Block）种可用的32位寄存器数量： " &lt;&lt; devProp.regsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; "每个EM的最大线程数：" &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl; std::cout &lt;&lt; "每个EM的最大线程束数：" &lt;&lt; devProp.maxThreadsPerMultiProcessor / 32 &lt;&lt; std::endl; std::cout &lt;&lt; "设备上多处理器的数量： " &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl; std::cout &lt;&lt; "======================================================" &lt;&lt; std::endl; &#125; return 0;&#125; 我们利用nvcc来编译程序。 1nvcc test1.cu -o test1 输出结果：因为我的服务器是8个TITAN GPU，为了省略重复信息，下面只显示两个GPU结果 12345678910111213141516171819202122使用GPU device 0: TITAN X (Pascal)设备全局内存总量： 12189MBSM的数量：28每个线程块的共享内存大小：48 KB每个线程块的最大线程数：1024设备上一个线程块（Block）种可用的32位寄存器数量： 65536每个EM的最大线程数：2048每个EM的最大线程束数：64设备上多处理器的数量： 28======================================================使用GPU device 1: TITAN X (Pascal)设备全局内存总量： 12189MBSM的数量：28每个线程块的共享内存大小：48 KB每个线程块的最大线程数：1024设备上一个线程块（Block）种可用的32位寄存器数量： 65536每个EM的最大线程数：2048每个EM的最大线程束数：64设备上多处理器的数量： 28======================================================....... 第一个计算任务：将两个元素数目为1024×1024的float数组相加。首先我们思考一下如果只用CPU我们怎么串行完成这个任务。 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;sys/time.h&gt;#include &lt;math.h&gt;using namespace std;int main()&#123; struct timeval start, end; gettimeofday( &amp;start, NULL ); float*A, *B, *C; int n = 1024 * 1024; int size = n * sizeof(float); A = (float*)malloc(size); B = (float*)malloc(size); C = (float*)malloc(size); for(int i=0;i&lt;n;i++) &#123; A[i] = 90.0; B[i] = 10.0; &#125; for(int i=0;i&lt;n;i++) &#123; C[i] = A[i] + B[i]; &#125; float max_error = 0.0; for(int i=0;i&lt;n;i++) &#123; max_error += fabs(100.0-C[i]); &#125; cout &lt;&lt; "max_error is " &lt;&lt; max_error &lt;&lt; endl; gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; "total time is " &lt;&lt; timeuse/1000 &lt;&lt; "ms" &lt;&lt;endl; return 0;&#125; CPU方式输出结果 12max_error is 0total time is 22ms 如果我们使用GPU来做并行计算，速度将会如何呢？编程要点： 每个Block中的Thread数最大不超过512； 为了充分利用SM，Block数尽可能多，&gt;100。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include "cuda_runtime.h"#include &lt;stdlib.h&gt;#include &lt;iostream&gt;#include &lt;sys/time.h&gt;using namespace std;__global__ void Plus(float A[], float B[], float C[], int n)&#123; int i = blockDim.x * blockIdx.x + threadIdx.x; C[i] = A[i] + B[i];&#125;int main()&#123; struct timeval start, end; gettimeofday( &amp;start, NULL ); float*A, *Ad, *B, *Bd, *C, *Cd; int n = 1024 * 1024; int size = n * sizeof(float); // CPU端分配内存 A = (float*)malloc(size); B = (float*)malloc(size); C = (float*)malloc(size); // 初始化数组 for(int i=0;i&lt;n;i++) &#123; A[i] = 90.0; B[i] = 10.0; &#125; // GPU端分配内存 cudaMalloc((void**)&amp;Ad, size); cudaMalloc((void**)&amp;Bd, size); cudaMalloc((void**)&amp;Cd, size); // CPU的数据拷贝到GPU端 cudaMemcpy(Ad, A, size, cudaMemcpyHostToDevice); cudaMemcpy(Bd, B, size, cudaMemcpyHostToDevice); cudaMemcpy(Bd, B, size, cudaMemcpyHostToDevice); // 定义kernel执行配置，（1024*1024/512）个block，每个block里面有512个线程 dim3 dimBlock(512); dim3 dimGrid(n/512); // 执行kernel Plus&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(Ad, Bd, Cd, n); // 将在GPU端计算好的结果拷贝回CPU端 cudaMemcpy(C, Cd, size, cudaMemcpyDeviceToHost); // 校验误差 float max_error = 0.0; for(int i=0;i&lt;n;i++) &#123; max_error += fabs(100.0 - C[i]); &#125; cout &lt;&lt; "max error is " &lt;&lt; max_error &lt;&lt; endl; // 释放CPU端、GPU端的内存 free(A); free(B); free(C); cudaFree(Ad); cudaFree(Bd); cudaFree(Cd); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; "total time is " &lt;&lt; timeuse/1000 &lt;&lt; "ms" &lt;&lt;endl; return 0;&#125; GPU方式输出结果 12max error is 0total time is 1278ms 由上面的例子看出，使用CUDA编程时我们看不到for循环了，因为CPU编程的循环已经被分散到各个thread上做了，所以我们也就看到不到for一类的语句。从结果上看，CPU的循环计算的速度比GPU计算快多了，原因就在于CUDA中有大量的内存拷贝操作（数据传输花费了大量时间，而计算时间却非常少），如果计算量比较小的话，CPU计算会更合适一些。 下面计算一个稍微复杂的例子 矩阵加法即对两个矩阵对应坐标的元素相加后的结果存储在第三个的对应位置的元素上。 值得注意的是，这个计算任务我采用了二维数组的计算方式，注意一下二维数组在CUDA编程中的写法。 CPU版本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdlib.h&gt;#include &lt;iostream&gt;#include &lt;sys/time.h&gt;#include &lt;math.h&gt;#define ROWS 1024#define COLS 1024using namespace std;int main()&#123; struct timeval start, end; gettimeofday( &amp;start, NULL ); int *A, **A_ptr, *B, **B_ptr, *C, **C_ptr; int total_size = ROWS*COLS*sizeof(int); A = (int*)malloc(total_size); B = (int*)malloc(total_size); C = (int*)malloc(total_size); A_ptr = (int**)malloc(ROWS*sizeof(int*)); B_ptr = (int**)malloc(ROWS*sizeof(int*)); C_ptr = (int**)malloc(ROWS*sizeof(int*)); //CPU一维数组初始化 for(int i=0;i&lt;ROWS*COLS;i++) &#123; A[i] = 80; B[i] = 20; &#125; for(int i=0;i&lt;ROWS;i++) &#123; A_ptr[i] = A + COLS*i; B_ptr[i] = B + COLS*i; C_ptr[i] = C + COLS*i; &#125; for(int i=0;i&lt;ROWS;i++) for(int j=0;j&lt;COLS;j++) &#123; C_ptr[i][j] = A_ptr[i][j] + B_ptr[i][j]; &#125; //检查结果 int max_error = 0; for(int i=0;i&lt;ROWS*COLS;i++) &#123; //cout &lt;&lt; C[i] &lt;&lt; endl; max_error += abs(100-C[i]); &#125; cout &lt;&lt; "max_error is " &lt;&lt; max_error &lt;&lt;endl; gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; "total time is " &lt;&lt; timeuse/1000 &lt;&lt; "ms" &lt;&lt;endl; return 0;&#125; CPU方式输出 12max_error is 0total time is 29ms GPU版本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include "cuda_runtime.h"#include "device_launch_parameters.h"#include &lt;sys/time.h&gt; #include &lt;stdio.h&gt;#include &lt;math.h&gt;#define Row 1024#define Col 1024 __global__ void addKernel(int **C, int **A, int ** B)&#123; int idx = threadIdx.x + blockDim.x * blockIdx.x; int idy = threadIdx.y + blockDim.y * blockIdx.y; if (idx &lt; Col &amp;&amp; idy &lt; Row) &#123; C[idy][idx] = A[idy][idx] + B[idy][idx]; &#125;&#125; int main()&#123; struct timeval start, end; gettimeofday( &amp;start, NULL ); int **A = (int **)malloc(sizeof(int*) * Row); int **B = (int **)malloc(sizeof(int*) * Row); int **C = (int **)malloc(sizeof(int*) * Row); int *dataA = (int *)malloc(sizeof(int) * Row * Col); int *dataB = (int *)malloc(sizeof(int) * Row * Col); int *dataC = (int *)malloc(sizeof(int) * Row * Col); int **d_A; int **d_B; int **d_C; int *d_dataA; int *d_dataB; int *d_dataC; //malloc device memory cudaMalloc((void**)&amp;d_A, sizeof(int **) * Row); cudaMalloc((void**)&amp;d_B, sizeof(int **) * Row); cudaMalloc((void**)&amp;d_C, sizeof(int **) * Row); cudaMalloc((void**)&amp;d_dataA, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataB, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataC, sizeof(int) *Row*Col); //set value for (int i = 0; i &lt; Row*Col; i++) &#123; dataA[i] = 90; dataB[i] = 10; &#125; //将主机指针A指向设备数据位置，目的是让设备二级指针能够指向设备数据一级指针 //A 和 dataA 都传到了设备上，但是二者还没有建立对应关系 for (int i = 0; i &lt; Row; i++) &#123; A[i] = d_dataA + Col * i; B[i] = d_dataB + Col * i; C[i] = d_dataC + Col * i; &#125; cudaMemcpy(d_A, A, sizeof(int*) * Row, cudaMemcpyHostToDevice); cudaMemcpy(d_B, B, sizeof(int*) * Row, cudaMemcpyHostToDevice); cudaMemcpy(d_C, C, sizeof(int*) * Row, cudaMemcpyHostToDevice); cudaMemcpy(d_dataA, dataA, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); cudaMemcpy(d_dataB, dataB, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); dim3 threadPerBlock(16, 16); dim3 blockNumber( (Col + threadPerBlock.x - 1)/ threadPerBlock.x, (Row + threadPerBlock.y - 1) / threadPerBlock.y ); printf("Block(%d,%d) Grid(%d,%d).\n", threadPerBlock.x, threadPerBlock.y, blockNumber.x, blockNumber.y); addKernel &lt;&lt; &lt;blockNumber, threadPerBlock &gt;&gt; &gt; (d_C, d_A, d_B); //拷贝计算数据-一级数据指针 cudaMemcpy(dataC, d_dataC, sizeof(int) * Row * Col, cudaMemcpyDeviceToHost); int max_error = 0; for(int i=0;i&lt;Row*Col;i++) &#123; //printf("%d\n", dataC[i]); max_error += abs(100-dataC[i]); &#125; //释放内存 free(A); free(B); free(C); free(dataA); free(dataB); free(dataC); cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); cudaFree(d_dataA); cudaFree(d_dataB); cudaFree(d_dataC); printf("max_error is %d\n", max_error); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; printf("total time is %d ms\n", timeuse/1000); return 0;&#125; GPU输出 123Block(16,16) Grid(64,64).max_error is 0total time is 442 ms 从结果看出，CPU计算时间还是比GPU的计算时间短。这里需要指出的是，这种二维数组的程序写法的效率并不高（虽然比较符合我们的思维方式），因为我们做了两次访存操作。所以一般而言，做高性能计算一般不会采取这种编程方式。 最后一个例子我们将计算一个更加复杂的任务 矩阵乘法回顾一下矩阵乘法：两矩阵相乘，左矩阵第一行乘以右矩阵第一列（分别相乘，第一个数乘第一个数），乘完之后相加，即为结果的第一行第一列的数，依次往下算，直到计算完所有矩阵元素。 CPU版本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;sys/time.h&gt;#define ROWS 1024#define COLS 1024using namespace std;void matrix_mul_cpu(float* M, float* N, float* P, int width)&#123; for(int i=0;i&lt;width;i++) for(int j=0;j&lt;width;j++) &#123; float sum = 0.0; for(int k=0;k&lt;width;k++) &#123; float a = M[i*width+k]; float b = N[k*width+j]; sum += a*b; &#125; P[i*width+j] = sum; &#125;&#125;int main()&#123; struct timeval start, end; gettimeofday( &amp;start, NULL ); float *A, *B, *C; int total_size = ROWS*COLS*sizeof(float); A = (float*)malloc(total_size); B = (float*)malloc(total_size); C = (float*)malloc(total_size); //CPU一维数组初始化 for(int i=0;i&lt;ROWS*COLS;i++) &#123; A[i] = 80.0; B[i] = 20.0; &#125; matrix_mul_cpu(A, B, C, COLS); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; "total time is " &lt;&lt; timeuse/1000 &lt;&lt; "ms" &lt;&lt;endl; return 0;&#125; CPU输出 1total time is 7617ms 梳理一下CUDA求解矩阵乘法的思路：因为C=A×B，我们利用每个线程求解C矩阵每个(x, y)的元素，每个线程载入A的一行和B的一列，遍历各自行列元素，对A、B对应的元素做一次乘法和一次加法。 GPU版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include "cuda_runtime.h"#include "device_launch_parameters.h"#include &lt;sys/time.h&gt; #include &lt;stdio.h&gt;#include &lt;math.h&gt;#define Row 1024#define Col 1024 __global__ void matrix_mul_gpu(int *M, int* N, int* P, int width)&#123; int i = threadIdx.x + blockDim.x * blockIdx.x; int j = threadIdx.y + blockDim.y * blockIdx.y; int sum = 0; for(int k=0;k&lt;width;k++) &#123; int a = M[j*width+k]; int b = N[k*width+i]; sum += a*b; &#125; P[j*width+i] = sum;&#125; int main()&#123; struct timeval start, end; gettimeofday( &amp;start, NULL ); int *A = (int *)malloc(sizeof(int) * Row * Col); int *B = (int *)malloc(sizeof(int) * Row * Col); int *C = (int *)malloc(sizeof(int) * Row * Col); //malloc device memory int *d_dataA, *d_dataB, *d_dataC; cudaMalloc((void**)&amp;d_dataA, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataB, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataC, sizeof(int) *Row*Col); //set value for (int i = 0; i &lt; Row*Col; i++) &#123; A[i] = 90; B[i] = 10; &#125; cudaMemcpy(d_dataA, A, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); cudaMemcpy(d_dataB, B, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); dim3 threadPerBlock(16, 16); dim3 blockNumber((Col+threadPerBlock.x-1)/ threadPerBlock.x, (Row+threadPerBlock.y-1)/ threadPerBlock.y ); printf("Block(%d,%d) Grid(%d,%d).\n", threadPerBlock.x, threadPerBlock.y, blockNumber.x, blockNumber.y); matrix_mul_gpu &lt;&lt; &lt;blockNumber, threadPerBlock &gt;&gt; &gt; (d_dataA, d_dataB, d_dataC, Col); //拷贝计算数据-一级数据指针 cudaMemcpy(C, d_dataC, sizeof(int) * Row * Col, cudaMemcpyDeviceToHost); //释放内存 free(A); free(B); free(C); cudaFree(d_dataA); cudaFree(d_dataB); cudaFree(d_dataC); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; printf("total time is %d ms\n", timeuse/1000); return 0;&#125; GPU输出 12Block(16,16) Grid(64,64).total time is 506 ms 从这个矩阵乘法任务可以看出，我们通过GPU进行并行计算的方式仅花费了0.5秒，但是CPU串行计算方式却花费了7.6秒，计算速度提升了十多倍，可见并行计算的威力！]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Next+Github搭建个人博客]]></title>
    <url>%2Freading%2FHexo%2BNext%2BGithub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[Hexo+Next+Github搭建个人博客 其实官网教程已经很明白了，这里做个教程也是参考官网，防止遗忘也顺带学习一下 Hexo官网Next官网 参考博客：hexo史上最全搭建教程教程分三个部分， 第一部分：hexo的初级搭建还有部署到github page上，以及个人域名的绑定。第二部分：hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流第三部分：hexo添加各种功能，包括搜索的SEO，阅读量统计，访问量统计和评论系统等。 第一部分hexo的初级搭建还有部署到github page上，以及个人域名的绑定。 Hexo简介Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。大家可以进入hexo官网进行详细查看，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。 Hexo搭建步骤 安装Git 安装Node.js 安装Hexo GitHub创建个人仓库 生成SSH添加到GitHub 将hexo部署到GitHub 设置个人域名 发布文章 1.安装GitGit是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的hexo博客文章，上传到GitHub的工具。 windows：到git官网上下载,Download git,下载后会有一个Git Bash的命令行工具，以后就用这个工具来使用git。 linux：对linux来说实在是太简单了，因为最早的git就是在linux上编写的，只需要一行代码 1sudo apt-get install git 安装好后，用git --version 来查看一下版本 2. 安装nodejsHexo是基于nodeJS编写的，所以需要安装一下nodeJs和里面的npm工具。 windows：nodejs选择LTS版本就行了。 linux：12sudo apt-get install nodejssudo apt-get install npm 安装完后，打开命令行 12node -vnpm -v 3. 安装hexo前面git和nodejs安装好后，就可以安装hexo了，你可以先创建一个文件夹blog，然后cd到这个文件夹下（或者在这个文件夹下直接右键git bash打开）。 输入命令 1npm install -g hexo-cli 依旧用hexo -v查看一下版本 至此就全部安装完了。 接下来初始化一下hexo 1hexo init myblog 这个myblog可以自己取什么名字都行，然后 12cd myblog //进入这个myblog文件夹npm install 新建完成后，指定文件夹目录下有： node_modules: 依赖包 public：存放生成的页面 scaffolds：生成文章的一些模板 source：用来存放你的文章 themes：主题 _config.yml: 博客的配置文件 12hexo ghexo server 打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。 默认界面还是挺丑的，最后再进行优化，现在先不管。 4. GitHub创建个人仓库首先，你先要有一个GitHub账户，去注册一个吧。 注册完登录后，在GitHub.com中看到一个New repository，新建仓库,创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名 5. 生成SSH添加到GitHub回到你的git bash中 12git config --global user.name "yourname"git config --global user.email "youremail" 这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。 可以用以下两条，检查一下你有没有输对 12git config user.namegit config user.email 然后创建SSH,一路回车 1ssh-keygen -t rsa -C "youremail" id_rsa是你这台电脑的私人秘钥，不能给别人看的，id_rsa.pub是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。 而后在GitHub的setting中，找到SSH keys的设置选项，点击New SSH key把你的id_rsa.pub里面的信息复制进去。 在gitbash中，查看是否成功 1ssh -T git@github.com 6. 将hexo部署到GitHub这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件 _config.yml，翻到最后，修改为YourgithubName就是你的GitHub账户 1234deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master 这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。 1npm install hexo-deployer-git --save 然后 123hexo cleanhexo generatehexo deploy 其中 hexo clean清除了你之前生成的东西，也可以不加。hexo generate 顾名思义，生成静态文章，可以用 hexo g缩写hexo deploy 部署文章，可以用hexo d缩写 注意deploy时可能要你输入username和password。 过一会儿就可以在https://yourname.github.io 这个网站看到你的博客了 7. 设置个人域名现在你的个人网站的地址是yourname.github.io，如果觉得这个网址逼格不太够，这就需要你设置个人域名了。但是需要花钱。 注册一个阿里云账户,在阿里云上买一个域名，我买的是coderya.com，各个后缀的价格不太一样，比如最广泛的.com就比较贵，看个人喜好咯。 解析域名点解析进去，添加解析。 192.30.252.153 和192.30.252.154 是GitHub的服务器地址，把它作为记录值填进去 Github配置登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名coderya.com 然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。 第二部分hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流。 1. hexo基本配置在文件根目录下的_config.yml，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考官方的配置描述。 网站 参数 描述 title 网站标题 subtitle 网站副标题 description 网站描述 author 您的名字 language 网站使用的语言 timezone 网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。 其中，description主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。author参数用于主题显示文章的作者。 网址 参数 描述 url 网址 root 网站根目录 permalink 文章的 永久链接 格式 permalink_defaults 永久链接中各部分的默认值 在这里，你需要把url改成你的网站域名。 2. 更换主题下载NexT主题1git clone https://github.com/theme-next/hexo-theme-next themes/next 启用NexT主题修改站点配置文件，查找关键词theme，并修改为主题名字next： 1234# Extensions #(注意冒号间的空格)## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 设置Scheme搜索关键词找到scheme属性，选择自己喜欢的模式： 12345678# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes# scheme: Muse # 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白scheme: Mist # Muse 的紧凑版本，整洁有序的单栏外观# scheme: Pisces # 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白# scheme: Gemini # 类似 Pisces 设置语言编辑 站点配置文件 1language: zh-Hans 设置菜单菜单的||后面是菜单的图标,具体菜单图标可参考Font Awesome网站。 12345678# 菜单示例配置menu: home: / || home reading: /reading/ || book archives: /archives/ || archive categories: /categories/ || th #tags: /tags/ || tags about: /about/ || user 这些翻译文本放置在 NexT 主题目录下的 languages/{language}.yml（{language} 为你所使用的语言）。 注意默认是zh-CN和主题配置不匹配，需要改成zh-Hans.yml 设置侧栏设置侧栏的位置，修改 sidebar.position 的值，支持的选项有： left - 靠左放置 right - 靠右放置 头像设置编辑 主题配置文件， 修改字段 avatar， 值设置成头像的链接地址。其中，头像的链接地址可以是： 地址 值 完整的互联网 URI http://example.com/avatar.png 站点内的地址 将头像放置主题目录下的 source/uploads/ （新建 uploads 目录若不存在） 配置为：avatar: /uploads/avatar.png或者 放置在 source/images/ 目录下 配置为：avatar: /images/avatar.png 下面没有实际验证，记录在此 3. git分支进行多终端工作上传分支首先，先在github上新建一个hexo分支 然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便） 然后在本地的任意目录下，打开git bash 1git clone git@github.com:ZJUFangzh/ZJUFangzh.github.io.git 将其克隆到本地，因为默认分支已经设成了hexo，所以clone时只clone了hexo 接下来在克隆到本地的ZJUFangzh.github.io中，把除了.git 文件夹外的所有文件都删掉 把之前我们写的博客源文件全部复制过来，除了.deploy_git。这里应该说一句，复制过来的源文件应该有一个.gitignore，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git： 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的.git文件夹删掉，因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。而后 123git add .git commit –m "add branch"git push 这样就上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中node_modules、public、db.json已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装 。 4. coding page上部署实现国内外分流之前我们已经把hexo托管在github了，但是github是国外的，而且百度的爬虫是不能够爬取github的，所以如果你希望你做的博客能够在百度引擎上被收录，而且想要更快的访问，那么可以在国内的coding page做一个托管，这样在国内访问就是coding page，国外就走github page。 申请coding账户，新建项目先申请一个账户，然后创建新的项目，这一步项目名称应该是随意的。 添加ssh key这一步跟github一样。 添加后，检查一下是不是添加成功 1ssh -T git@git.coding.net 修改_config.yml12345deploy: type: git repo: coding: git@git.coding.net:ZJUFangzh/ZJUFangzh.git,master github: git@github.com:ZJUFangzh/ZJUFangzh.github.io.git,master 部署12hexo ghexo d 开启coding pages服务，绑定域名阿里云添加解析去除coding page的跳转广告1&lt;p&gt;Hosted by &lt;a href="https://pages.coding.me" style="font-weight: bold"&gt;Coding Pages&lt;/a&gt;&lt;/p&gt; 我的选择是把这一行代码放在主题文件夹/layout/common/footer.ejs里面，也就是本来在页面中看到的页脚部分。 第三部分hexo添加各种功能，包括搜索的SEO，阅读量统计，访问量统计和评论系统等。 1. SEO优化百度seogoogle的SEO2. 评论系统3. 添加百度统计4. 文章阅读量统计leanCloud5. 引入不蒜子访问量和访问人次统计]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[人生自有百种]]></title>
    <url>%2Flife%2F%E5%A4%9A%E6%89%8D%E5%A4%9A%E8%89%BA.html</url>
    <content type="text"><![CDATA[人生自有百种，我又有何不同？从小到大便羡慕有特长的同学，他们总是能够脱颖而出给人眼前一亮。近几年短视频火热起来，更加拉近了我对这个世间百态的认知。从直播到抖音快手，世情庸俗众多，而能人异士亦不少，只要有一技之长，总能够在这个网络的平台找到一席之地，自己也在思考，庸众平凡的自己是否能够找到自己的方向。有时候甚至怀疑自己是不是天赋有所缺陷，花费大量的时间，依然达不到入门的水平，我不寄望于能够成为顶尖，只希望自己的水平能够入门而已 身无一技之长还是挺烦躁的，生活也的确毫无乐趣可言，只能将所剩无几的闲暇时光消磨在几把输输赢赢的游戏，几个毫无内涵的段子。 已经很久没有在12点前睡过觉了，想起高中的时候，尽管学业重，睡眠少，依然无所忧虑，也能够按时睡觉。 现在即使眼睛都睁不开了，依然手持着手机不肯屈服，实在是愧对自己一天的碌碌无为，又不敢坦然面对未来的明天。 天生我材必有用，感觉自己没啥用，说到底还是不甘人生就这样匆匆而过，还是希望自己能够尽快找到附注终身的兴趣吧，不然老了广场舞都不会跳。 其实，思考是痛苦的过程，不反思，不作为，生活无非就是得过且过，想得多了，反而更加痛苦，也于事无补，收心努力吧，少年！]]></content>
      <categories>
        <category>生活杂谈</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Make命令教程]]></title>
    <url>%2Freading%2FMake%E5%91%BD%E4%BB%A4%E6%95%99%E7%A8%8B.html</url>
    <content type="text"><![CDATA[前言代码变成可执行文件，叫做 编译（compile）；先编译这个，还是先编译那个（即编译的安排），叫做 构建（build）。Make是最常用的构建工具，诞生于1977年，主要用于C语言的项目。但是实际上 ，任何只要某个文件有变化，就要重新构建的项目，都可以用Make构建。一、Make的概念Make这个词，英语的意思是”制作”。Make命令直接用了这个意思，就是要做出某个文件。比如，要做出文件a.txt，就可以执行下面的命令。 1make a.txt 但是，如果你真的输入这条命令，它并不会起作用。因为Make命令本身并不知道，如何做出a.txt，需要有人告诉它，如何调用其他命令完成这个目标。 比如，假设文件 a.txt 依赖于 b.txt 和 c.txt ，是后面两个文件连接（cat命令）的产物。那么，make 需要知道下面的规则。 12a.txt: b.txt c.txt cat b.txt c.txt &gt; a.txt 也就是说，make a.txt 这条命令的背后，实际上分成两步：第一步，确认 b.txt 和 c.txt 必须已经存在，第二步使用 cat 命令 将这个两个文件合并，输出为新文件。 像这样的规则，都写在一个叫做Makefile的文件中，Make命令依赖这个文件进行构建。Makefile文件也可以写为makefile， 或者用命令行参数指定为其他文件名。 123$ make -f rules.txt# 或者$ make --file=rules.txt 上面代码指定make命令依据rules.txt文件中的规则，进行构建。 总之，make只是一个根据指定的Shell命令进行构建的工具。它的规则很简单，你规定要构建哪个文件、它依赖哪些源文件，当那些文件有变动时，如何重新构建它。 二、Makefile文件的格式2.1 概述Makefile文件由一系列规则（rules）构成。每条规则的形式如下。 12&lt;target&gt; : &lt;prerequisites&gt; [tab] &lt;commands&gt; 上面第一行冒号前面的部分，叫做”目标”（target），冒号后面的部分叫做”前置条件”（prerequisites）；第二行必须由一个tab键起首，后面跟着”命令”（commands）。 “目标”是必需的，不可省略；”前置条件”和”命令”都是可选的，但是两者之中必须至少存在一个。 每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。下面就详细讲解，每条规则的这三个组成部分。 2.2 目标（target）一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上文的 a.txt 。目标可以是一个文件名，也可以是多个文件名，之间用空格分隔。 除了文件名，目标还可以是某个操作的名字，这称为”伪目标”（phony target）。 12clean: rm *.o 上面代码的目标是clean，它不是文件名，而是一个操作的名字，属于”伪目标 “，作用是删除对象文件。 1$ make clean 但是，如果当前目录中，正好有一个文件叫做clean，那么这个命令不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令。 为了避免这种情况，可以明确声明clean是”伪目标”，写法如下。 123.PHONY: cleanclean: rm *.o temp 声明clean是”伪目标”之后，make就不会去检查是否存在一个叫做clean的文件，而是每次运行都执行对应的命令。像.PHONY这样的内置目标名还有不少，可以查看手册。 如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。 1$ make 上面代码执行Makefile文件的第一个目标。 2.3 前置条件（prerequisites）前置条件通常是一组文件名，之间用空格分隔。它指定了”目标”是否重新构建的判断标准：只要有一个前置文件不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），”目标”就需要重新构建。 12result.txt: source.txt cp source.txt result.txt 上面代码中，构建 result.txt 的前置条件是source.txt 。如果当前目录中，source.txt 已经存在，那么make result.txt可以正常运行，否则必须再写一条规则，来生成 source.txt 。 12source.txt: echo "this is the source" &gt; source.txt 上面代码中source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成。 12$ make result.txt$ make result.txt 上面命令连续执行两次make result.txt。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，Make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt也不会重新生成。 如果需要生成多个文件，往往采用下面的写法。 1source: file1 file2 file3 上面代码中，source 是一个伪目标，只有三个前置文件，没有任何对应的命令。 1$ make source 执行make source命令后，就会一次性生成file1，file2，file3 三个文件。这比下面的写法要方便很多。 123$ make file1$ make file2$ make file3 2.4 命令（commands）命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建”目标”的具体指令，它的运行结果通常就是生成目标文件。 每行命令之前必须有一个tab键。如果想用其他键，可以用内置变量.RECIPEPREFIX声明。 123.RECIPEPREFIX = &gt;all:&gt; echo Hello, world 上面代码用.RECIPEPREFIX指定，大于号（&gt;）替代tab键。所以，每一行命令的起首变成了大于号，而不是tab键。 需要注意的是，每行命令在一个单独的shell中执行。这些Shell之间没有继承关系。 123var-lost: export foo=bar echo "foo=[$$foo]" 上面代码执行后（make var-lost），取不到foo的值。因为两行命令在两个不同的进程执行。一个解决办法是将两行命令写在一行，中间用分号分隔。 12var-kept: export foo=bar; echo "foo=[$$foo]" 另一个解决办法是在换行符前加反斜杠转义。 123var-kept: export foo=bar; \ echo "foo=[$$foo]" 最后一个方法是加上.ONESHELL:命令。 1234.ONESHELL:var-kept: export foo=bar; echo "foo=[$$foo]" 三、Makefile文件的语法3.1 注释井号（#）在Makefile中表示注释。 1234# 这是注释result.txt: source.txt # 这是注释 cp source.txt result.txt # 这也是注释 3.2 回声（echoing）正常情况下，make会打印每条命令，然后再执行，这就叫做回声（echoing）。 12test: # 这是测试 执行上面的规则，会得到下面的结果。 12$ make test# 这是测试 在命令的前面加上@，就可以关闭回声。 12test: @# 这是测试 现在再执行make test，就不会有任何输出。 由于在构建过程中，需要了解当前在执行哪条命令，所以通常只在注释和纯显示的echo命令前面加上@。 123test: @# 这是测试 @echo TODO 3.3 通配符通配符（wildcard）用来指定一组符合条件的文件名。Makefile 的通配符与 Bash 一致，主要有星号（）、问号（？）和 […] 。比如， .o 表示所有后缀名为o的文件。 12clean: rm -f *.o 3.4 模式匹配Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。 1%.o: %.c 等同于下面的写法。 12f1.o: f1.cf2.o: f2.c 使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。 3.5 变量和赋值符Makefile 允许使用等号自定义变量。 123txt = Hello Worldtest: @echo $(txt) 上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。 调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。 12test: @echo $$HOME 有时，变量的值可能指向另一个变量。 1v1 = $(v2) 上面代码中，变量 v1 的值是另一个变量 v2。这时会产生一个问题，v1 的值到底在定义时扩展（静态扩展），还是在运行时扩展（动态扩展）？如果 v2 的值是动态的，这两种扩展方式的结果可能会差异很大。 为了解决类似问题，Makefile一共提供了四个赋值运算符 （=、:=、？=、+=），它们的区别请看StackOverflow。 1234567891011VARIABLE = value# 在执行时扩展，允许递归扩展。VARIABLE := value# 在定义时扩展。VARIABLE ?= value# 只有在该变量为空时才设置值。VARIABLE += value# 将值追加到变量的尾端。 3.6 内置变量（Implicit Variables）Make命令提供一系列内置变量，比如，$(CC) 指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。 12output: $(CC) -o output input.c 3.7 自动变量（Automatic Variables）Make命令还提供一些自动变量，它们的值与当前规则有关。主要有以下几个。 （1）$@ $@指代当前目标，就是Make命令当前构建的那个目标。比如，make foo的 $@ 就指代foo。 12a.txt b.txt: touch $@ 等同于下面的写法。 1234a.txt: touch a.txtb.txt: touch b.txt （2）$&lt; $&lt; 指代第一个前置条件。比如，规则为 t: p1 p2，那么$&lt; 就指代p1。 12a.txt: b.txt c.txt cp $&lt; $@ 等同于下面的写法。 12a.txt: b.txt c.txt cp b.txt a.txt （3）$? $? 指代比目标更新的所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，其中 p2 的时间戳比 t 新，$?就指代p2。 （4）$^ $^ 指代所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，那么 $^ 就指代 p1 p2 。 （5）$* $ 指代匹配符 % 匹配的部分， 比如% 匹配 f1.txt 中的f1 ，$ 就表示 f1。 （6）$(@D) 和 $(@F) $(@D) 和 $(@F) 分别指向 $@ 的目录名和文件名。比如，$@是 src/input.c，那么$(@D) 的值为 src ，$(@F) 的值为input.c。 （7）$(&lt;D) 和 $(&lt;F) $(&lt;D) 和 $(&lt;F) 分别指向 $&lt; 的目录名和文件名。 所有的自动变量清单，请看手册。下面是自动变量的一个例子。 123dest/%.txt: src/%.txt @[ -d dest ] || mkdir dest cp $&lt; $@ 上面代码将 src 目录下的 txt 文件，拷贝到 dest 目录下。首先判断 dest 目录是否存在，如果不存在就新建，然后，$&lt; 指代前置文件（src/%.txt）， $@ 指代目标文件（dest/%.txt）。 3.8 判断和循环Makefile使用 Bash 语法，完成判断和循环。 12345ifeq ($(CC),gcc) libs=$(libs_for_gcc)else libs=$(normal_libs)endif 上面代码判断当前编译器是否gcc ，然后指定不同的库文件。 123456789101112LIST = one two threeall: for i in $(LIST); do \ echo $$i; \ done# 等同于all: for i in one two three; do \ echo $i; \ done 上面代码的运行结果。 123onetwothree 3.9 函数Makefile 还可以使用函数，格式如下。 123$(function arguments)# 或者$&#123;function arguments&#125; Makefile提供了许多内置函数，可供调用。下面是几个常用的内置函数。 （1）shell 函数 shell 函数用来执行 shell 命令 1srcfiles := $(shell echo src/&#123;00..99&#125;.txt) （2）wildcard 函数 wildcard 函数用来在 Makefile 中，替换 Bash 的通配符。 1srcfiles := $(wildcard src/*.txt) （3）subst 函数 subst 函数用来文本替换，格式如下。 1$(subst from,to,text) 下面的例子将字符串”feet on the street”替换成”fEEt on the strEEt”。 1$(subst ee,EE,feet on the street) 下面是一个稍微复杂的例子。 1234567comma:= ,empty:=# space变量用两个空变量作为标识符，当中是一个空格space:= $(empty) $(empty)foo:= a b cbar:= $(subst $(space),$(comma),$(foo))# bar is now `a,b,c'. （4）patsubst函数 patsubst 函数用于模式匹配的替换，格式如下。 1$(patsubst pattern,replacement,text) 下面的例子将文件名”x.c.c bar.c”，替换成”x.c.o bar.o”。 1$(patsubst %.c,%.o,x.c.c bar.c) （5）替换后缀名 替换后缀名函数的写法是：变量名 + 冒号 + 后缀名替换规则。它实际上patsubst的一种简写形式。 1min: $(OUTPUT:.js=.min.js) 上面代码的意思是，将变量OUTPUT中的后缀名.js全部替换成.min.js。 四、Makefile 的实例（1）执行多个目标1234567.PHONY: cleanall cleanobj cleandiffcleanall : cleanobj cleandiff rm programcleanobj : rm *.ocleandiff : rm *.diff 上面代码可以调用不同目标，删除不同后缀名的文件，也可以调用一个目标（cleanall），删除所有指定类型的文件。 （2）编译C语言项目12345678910111213edit : main.o kbd.o command.o display.o cc -o edit main.o kbd.o command.o display.omain.o : main.c defs.h cc -c main.ckbd.o : kbd.c defs.h command.h cc -c kbd.ccommand.o : command.c defs.h command.h cc -c command.cdisplay.o : display.c defs.h cc -c display.cclean : rm edit main.o kbd.o command.o display.o.PHONY: edit clean]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[page]]></title>
    <url>%2Ftensorflow%2F22.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%85%A8%E6%96%87%E4%BB%A3%E7%A0%81%E9%9B%86.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[使用Flask调用百度接口实现自己的图像分类应用]]></title>
    <url>%2Freading%2F%E4%BD%BF%E7%94%A8Flask%E8%B0%83%E7%94%A8%E7%99%BE%E5%BA%A6%E6%8E%A5%E5%8F%A3%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%BA%94%E7%94%A8.html</url>
    <content type="text"><![CDATA[使用Flask调用百度接口实现自己的图像分类应用使用flask这个轻量级的web框架可以实现一些简单的应用，一开始我们会做一些比较简单的功能，例如总结自己的所学写一个博客网站，或者收集爬取一些免费的文档资料，音乐视频，做一个小说，教程，音频分享的网站应用。老夫作为一个算法攻城狮，按照项目需求，常常也需要将自己的算法进行封装上线测试，那么老夫如何在网页中看到自己算法的实现效果呢，于是于是乎就诞生了这篇博客，为了让大多数非专业人士也能够轻松实现，这里采用了百度的算法接口。话不多说，进入正文。 前期准备编程基础 略懂Python 略懂Flask(会装flask,能实现官方的hello world示例即可) 工具准备 你自己的分类数据 百度帐号一枚 使用百度的EasyDL训练数据百度EasyDL操作文档：https://ai.baidu.com/docs#/EasyDL_VIS_Classification_Intro/dc6d85fe看着文档进行操作就好了，让它训练着吧，我们先补充后面的代码部分 编写调用API文件名称：api.py 123456789101112131415161718192021222324import jsonimport requestsimport base64def api(img_path): host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type='\ 'client_credentials&amp;client_id=你的API Key'\ '&amp;client_secret=你的Secret Key' response = requests.get(host) content = response.json() access_token = content["access_token"] image = open(img_path, 'rb').read() data = &#123;'image': base64.b64encode(image).decode()&#125; request_url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/classification/teaeyes" + "?access_token=" + access_token response = requests.post(request_url, data=json.dumps(data)) content = response.json() return contentif __name__ == "__main__": img_path ='static/test.jpg' content = api(img_path) print(content) print(type(content)) 等你训练结束，申请发布之后会得到你的密钥，填写到host 就可以了，可以拿test文件测试一下 12&#123;'log_id': 6512631446211722547, 'results': [&#123;'name': '2', 'score': 0.9987852573394775&#125;, &#123;'name': '4', 'score': 0.00020876689814031124&#125;, &#123;'name': '7', 'score': 0.0001958562934305519&#125;, &#123;'name': '1', 'score': 0.0001313904213020578&#125;, &#123;'name': '3', 'score': 0.0001222034334205091&#125;, &#123;'name': '6', 'score': 0.00011624570470303297&#125;]&#125;&lt;class 'dict'&gt; 编写Flask应用这里已经可以返回我们想要的东西，接下来开始编写flask应用吧先测一个官方示例确保我们的flask是没有问题的文件名称：app_blog.py 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 网页中应该给你正确返回：Hello World!，详细的教程参见官方文档 我们模仿这个简单的应用开始编写自己的应用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from flask import Flask, request, render_templatefrom werkzeug.utils import secure_filename # 使用这个是为了确保filename是安全的from os import pathimport timefrom PIL import Imagefrom api import apiapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!' @app.route('/api',methods=['GET','POST']) #设置路由地址def index_tea_easy(): if request.method == 'POST': f = request.files['file'] base_path = path.abspath(path.dirname(__file__)) upload_path = path.join(base_path, 'static/images') #把用户上传的图片先保存起来，为了防止重名覆盖，用了时间来避免 file_name = path.join(upload_path,str(time.time()).replace('.','blog') + '.jpg') f.save(file_name) # ------------------把图片裁剪成正方形，可以不需要------------- im = Image.open(file_name) w, h = im.size cro = abs(w - h) // 2 if w &gt; h: im = im.crop((cro, 0, w - cro, h)) im.save(file_name, 'JPEG') if w &lt; h: im = im.crop((0, cro, w, h - cro)) im = im.save(file_name) if w == h: pass # ------------------------------------------------------- # 调用api进行预测，这里给它匹配了一些类别名称 pred = api(file_name) dic = &#123;'0': '大红袍', '1': '太平猴魁', '2': '松针滇红', '3': '白毫银针', '4': '碧螺春', '5': '祁门红茶', '6': '铁观音', '7': '雅安毛峰', '8': '雅安甘露', '9': '雅安竹叶青', '10': '雅安龙井'&#125; names = [] scores = [] for line in pred['results']: names.append(dic[line['name']]) scores.append(round(line['score']*100,3)) # 处理完成之后将数据传给show.html页面进行显示 return render_template('show.html',ziped=zip(names,scores),img=file_name.split('/')[-1]) return render_template('select_file.html') # 如果用户不是post请求，就返回这个选择文件的页面if __name__ == '__main__': app.run() 编写网页模板数据已经处理完成送发给html页面了，我们上面涉及的页面应该如何编写呢？这里主要是涉及到html,css,bootstrap,如果没有基础可以到w3cschool进行学习文件名称：select_file.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;link href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;title&gt;选择你的文件&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="container"&gt; &lt;div class="row clearfix"&gt; &lt;div class="col-md-12 column"&gt; &lt;div id="padd" align="center"&gt; &lt;form role="form" class="form-inline" action="" method="post" enctype="multipart/form-data"&gt; &lt;div class="form-group span12"&gt; &lt;button type="button" class="btn btn-primary" name="file" &gt; &lt;input type="file" name="file" placeholder="请选择你的文件" &gt; &lt;/button&gt; &lt;button class="btn btn-success" type="submit" &gt; &lt;span class="glyphicon glyphicon-cloud-upload"&gt;&lt;/span&gt; &lt;/button&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;style&gt; button&#123; font-size: 60px; height: 50px; &#125; #padd&#123; &#123;#background-color: #3e88f1;#&#125; height: 50%; width: 50%; padding-top: 10%; margin-top: 10%; margin-left: 20%; &#125;&lt;/style&gt;&lt;/html&gt; 文件名称：show.html 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;!DOCTYPE html&gt;&lt;html lang="en" xmlns="http://www.w3.org/1999/html"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;预测结果&lt;/title&gt; &lt;link href="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet"&gt; &lt;script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"&gt;&lt;/script&gt; &lt;script src="https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="container-fluid"&gt; &lt;div class="row-fluid"&gt; &lt;div class="span12"&gt; &lt;h3 class="text-center"&gt; 智能分析 &lt;/h3&gt; &lt;hr&gt; &#123;% if ziped %&#125; &lt;div align="center"&gt; &lt;img src=&#123;&#123; url_for('static',filename='images/'+img)&#125;&#125; class="img-rounded"&gt; &lt;/div&gt; &lt;div id="pred"&gt; &lt;table class="table table-hover"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt; 预测种类 &lt;/th&gt; &lt;th&gt; 估计得分 &lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &#123;% for n in ziped %&#125; &lt;tr class="success"&gt; &lt;td&gt;&#123;&#123; n[0] &#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123; n[1] &#125;&#125;%&lt;/td&gt; &lt;/tr&gt; &#123;% endfor %&#125; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &#123;% else %&#125; &lt;h3 style="text-align: center"&gt;&#123;&#123; name &#125;&#125;&lt;/h3&gt; &lt;div align="center"&gt; &lt;img src=&#123;&#123; url_for('static',filename='images/'+img)&#125;&#125; class="img-rounded"&gt; &lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;style&gt; h3&#123; color: seagreen; &#125; img&#123; height: 60%; width: 80%; &#125;&lt;/style&gt;&lt;/html&gt; 这个展示的网页重点是什么？它可以随着输入数据的不同显示不同的网页，所以它叫做模板，模板使用 Jinja2 语法并默认开启自动转义关于Jinja2的东西，我后面做Django教程的时候再单独写一篇博客吧。其实，理解了模板，就理解了动态网页。无非就是数据库存放数据，用这些框架写处理函数，函数访问数据库，再将处理后的数据送给模板进行显示。说起数据库，我先立个flag，空了再写个数据库教程，暂时定一个关系型mysql和非关系型mongodb吧 总结做一个调用算法的服务需要几步？1.算法包装 2.调用算法 3.进行显示 好像也没啥说的，整个应用还是比较简单]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python与C++相互调用_附录]]></title>
    <url>%2Freading%2FPython%E4%B8%8EC%2B%2B%E7%9B%B8%E4%BA%92%E8%B0%83%E7%94%A8_%E9%99%84%E5%BD%95.html</url>
    <content type="text"><![CDATA[加载DLL访问dll，首先需引入ctypes库from ctypes import *假设你已经有了一个的DLL（名字是add.dll)，且该DLL有一个符合cdecl（这里强调调用约定是因为，stdcall调用约定和cdecl调用约定声明的导出函数，在使用python加载时使用的加载函数是不同的，后面会有说明）调用约定的导出函数Add。stdcall调用约定： 两种加载方式 12Objdll = ctypes.windll.LoadLibrary("dllpath") Objdll = ctypes.WinDLL("dllpath") cdecl调用约定： 也有两种加载方式 12Objdll = ctypes.cdll.LoadLibrary("dllpath") Objdll = ctypes.CDLL("dllpath") 其实windll和cdll分别是WinDLL类和CDll类的对象 调用DLL方法 加载dll后会返回一个DLL对象，使用其中的函数方法则相当于操作该对象的对应属性。 注意，经过stdcall声明的方法，如果不是用def文件声明的导出函数或者extern “C” 声明的话，编译器会对函数名进行修改 函数参数申明，通过设置函数的argtypes属性 函数返回类型，函数默认返回c_int类型，如果需要返回其他类型，需要设置函数的restype属性 python3 获得shell的输出python3 获得shell的输出内容subprocess.getstatusoutput 123import subprocessPIDS=subprocess.getstatusoutput('ps -ef |grep appium ') 注意： 返回的数据结果是一个元组，第一位为shell运行结果的状态（0通过），第二位是输出的内容（string类型）所以如果想用值，使用PIDS[1]举个例子:比如我想杀appium的进程 12345678910111213141516import osimport subprocessPIDS=subprocess.getstatusoutput('ps -ef |grep appium ')pidlist=[]for i in PIDS[1].split("\n"): try: pidlist.append(i.split()[1]) except Exception as e: passprint(pidlist)for i in pidlist: os.system("kill -9 "+i) 调用系统命令 os.system()和os.popen()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&gt;&gt;&gt; import os&gt;&gt;&gt; dir(os)['CLD_CONTINUED', 'CLD_DUMPED', 'CLD_EXITED', 'CLD_TRAPPED', 'DirEntry', 'EX_CANTCREAT', 'EX_CONFIG', 'EX_DATAERR', 'EX_IOERR','EX_NOHOST', 'EX_NOINPUT', 'EX_NOPERM', 'EX_NOUSER', 'EX_OK', 'EX_OSERR', 'EX_OSFILE', 'EX_PROTOCOL', 'EX_SOFTWARE','EX_TEMPFAIL', 'EX_UNAVAILABLE', 'EX_USAGE', 'F_LOCK', 'F_OK','F_TEST', 'F_TLOCK', 'F_ULOCK', 'GRND_NONBLOCK', 'GRND_RANDOM','MutableMapping', 'NGROUPS_MAX', 'O_ACCMODE', 'O_APPEND', 'O_ASYNC', 'O_CLOEXEC', 'O_CREAT', 'O_DIRECT', 'O_DIRECTORY', 'O_DSYNC', 'O_EXCL', 'O_LARGEFILE', 'O_NDELAY', 'O_NOATIME', 'O_NOCTTY', 'O_NOFOLLOW', 'O_NONBLOCK', 'O_PATH', 'O_RDONLY', 'O_RDWR', 'O_RSYNC', 'O_SYNC', 'O_TMPFILE', 'O_TRUNC', 'O_WRONLY', 'POSIX_FADV_DONTNEED', 'POSIX_FADV_NOREUSE', 'POSIX_FADV_NORMAL', 'POSIX_FADV_RANDOM', 'POSIX_FADV_SEQUENTIAL', 'POSIX_FADV_WILLNEED', 'PRIO_PGRP', 'PRIO_PROCESS', 'PRIO_USER', 'P_ALL', 'P_NOWAIT','P_NOWAITO', 'P_PGID', 'P_PID', 'P_WAIT', 'PathLike', 'RTLD_DEEPBIND', 'RTLD_GLOBAL', 'RTLD_LAZY', 'RTLD_LOCAL', 'RTLD_NODELETE', 'RTLD_NOLOAD','RTLD_NOW', 'R_OK', 'SCHED_BATCH', 'SCHED_FIFO', 'SCHED_IDLE','SCHED_OTHER', 'SCHED_RESET_ON_FORK', 'SCHED_RR', 'SEEK_CUR', 'SEEK_DATA', 'SEEK_END', 'SEEK_HOLE', 'SEEK_SET', 'ST_APPEND', 'ST_MANDLOCK', 'ST_NOATIME', 'ST_NODEV', 'ST_NODIRATIME', 'ST_NOEXEC','ST_NOSUID', 'ST_RDONLY', 'ST_RELATIME', 'ST_SYNCHRONOUS', 'ST_WRITE','TMP_MAX', 'WCONTINUED', 'WCOREDUMP', 'WEXITED', 'WEXITSTATUS','WIFCONTINUED', 'WIFEXITED', 'WIFSIGNALED', 'WIFSTOPPED', 'WNOHANG','WNOWAIT', 'WSTOPPED', 'WSTOPSIG', 'WTERMSIG', 'WUNTRACED', 'W_OK', 'XATTR_CREATE', 'XATTR_REPLACE', 'XATTR_SIZE_MAX', 'X_OK','_Environ', '__all__', '__builtins__', '__cached__', '__doc__','__file__', '__loader__', '__name__', '__package__', '__spec__', '_execvpe', '_exists', '_exit', '_fspath', '_fwalk', '_get_exports_list', '_putenv', '_spawnvef', '_unsetenv', '_wrap_close', 'abc', 'abort', 'access', 'altsep', 'chdir', 'chmod', 'chown', 'chroot', 'close', 'closerange', 'confstr', 'confstr_names', 'cpu_count', 'ctermid', 'curdir', 'defpath', 'device_encoding', 'devnull', 'dup', 'dup2', 'environ', 'environb', 'errno', 'error', 'execl', 'execle', 'execlp', 'execlpe', 'execv', 'execve', 'execvp', 'execvpe', 'extsep', 'fchdir', 'fchmod', 'fchown', 'fdatasync', 'fdopen', 'fork', 'forkpty', 'fpathconf', 'fsdecode', 'fsencode', 'fspath', 'fstat', 'fstatvfs', 'fsync', 'ftruncate', 'fwalk', 'get_blocking', 'get_exec_path', 'get_inheritable', 'get_terminal_size', 'getcwd', 'getcwdb', 'getegid', 'getenv', 'getenvb', 'geteuid', 'getgid', 'getgrouplist', 'getgroups', 'getloadavg', 'getlogin', 'getpgid', 'getpgrp', 'getpid', 'getppid', 'getpriority', 'getrandom', 'getresgid', 'getresuid', 'getsid', 'getuid', 'getxattr', 'initgroups', 'isatty', 'kill', 'killpg', 'lchown', 'linesep', 'link', 'listdir', 'listxattr', 'lockf', 'lseek', 'lstat', 'major', 'makedev', 'makedirs', 'minor', 'mkdir', 'mkfifo', 'mknod', 'name', 'nice', 'open', 'openpty', 'pardir', 'path', 'pathconf', 'pathconf_names', 'pathsep', 'pipe', 'pipe2', 'popen', 'posix_fadvise', 'posix_fallocate', 'pread', 'putenv', 'pwrite', 'read','readlink', 'readv', 'remove', 'removedirs', 'removexattr', 'rename','renames', 'replace', 'rmdir', 'scandir', 'sched_get_priority_max', 'sched_get_priority_min', 'sched_getaffinity', 'sched_getparam', 'sched_getscheduler', 'sched_param', 'sched_rr_get_interval', 'sched_setaffinity', 'sched_setparam', 'sched_setscheduler', 'sched_yield', 'sendfile', 'sep', 'set_blocking', 'set_inheritable', 'setegid','seteuid', 'setgid', 'setgroups', 'setpgid', 'setpgrp', 'setpriority','setregid', 'setresgid', 'setresuid', 'setreuid', 'setsid', 'setuid', 'setxattr', 'spawnl', 'spawnle', 'spawnlp', 'spawnlpe', 'spawnv', 'spawnve', 'spawnvp', 'spawnvpe', 'st', 'stat', 'stat_float_times','stat_result', 'statvfs', 'statvfs_result', 'strerror', 'supports_bytes_environ', 'supports_dir_fd', 'supports_effective_ids', 'supports_fd', 'supports_follow_symlinks', 'symlink', 'sync', 'sys', 'sysconf', 'sysconf_names', 'system', 'tcgetpgrp', 'tcsetpgrp','terminal_size', 'times', 'times_result', 'truncate', 'ttyname', 'umask', 'uname', 'uname_result', 'unlink', 'unsetenv', 'urandom', 'utime','wait', 'wait3', 'wait4', 'waitid', 'waitid_result', 'waitpid', 'walk','write', 'writev'] os.system()12345&gt;&gt;&gt; help(os.system)Help on built-in function system in module nt:system(command) Execute the command in a subshell. 从字面意思上看，os.system()是在当前进程中打开一个子shell（子进程）来执行系统命令。 官方说法： On Unix, the return value is the exit status of the process encoded in the format specified for wait(). The subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function. 这个方法只返回状态码，执行结果会输出到stdout，也就是输出到终端。不过官方建议使用subprocess模块来生成新进程并获取结果是更好的选择。 os.popen()12345&gt;&gt;&gt; help(os.popen)Help on function popen in module os:popen(cmd, mode='r', buffering=-1) # Supply os.popen() cmd：要执行的命令。mode：打开文件的模式，默认为’r’，用法与open()相同。buffering：0意味着无缓冲；1意味着行缓冲；其它正值表示使用参数大小的缓冲。负的bufsize意味着使用系统的默认值，一般来说，对于tty设备，它是行缓冲；对于其它文件，它是全缓冲。 官方说法： Open a pipe to or from command cmd. The return value is an open file object connected to the pipe, which can be read or written depending on whether mode is ‘r’ (default) or ‘w’. The close method returns None if the subprocess exited successfully, or the subprocess’s return code if there was an error. This is implemented using subprocess.Popen; 这个方法会打开一个管道，返回结果是一个连接管道的文件对象，该文件对象的操作方法同open()，可以从该文件对象中读取返回结果。如果执行成功，不会返回状态码，如果执行失败，则会将错误信息输出到stdout，并返回一个空字符串。这里官方也表示subprocess模块已经实现了更为强大的subprocess.Popen()方法。 123456789&gt;&gt;&gt; os.popen('ls')&lt;os._wrap_close object at 0x7f93c5a2d780&gt;&gt;&gt;&gt; os.popen('la')&lt;os._wrap_close object at 0x7f93c5a37588&gt;&gt;&gt;&gt; /bin/sh: la: command not found&gt;&gt;&gt; f = os.popen('ls')&gt;&gt;&gt; type(f)&lt;class 'os._wrap_close'&gt; 读取执行结果： 1234&gt;&gt;&gt; f.readlines()['access.log\n', 'douban.py\n', 'import_test.py\n', 'mail.py\n', 'myapp.py\n', 'polipo\n', 'proxychains\n', '__pycache__\n', 'spider.py\n', 'test.py\n', 'users.txt\n']]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python与C++相互调用]]></title>
    <url>%2Freading%2FPython%E4%B8%8EC%2B%2B%E7%9B%B8%E4%BA%92%E8%B0%83%E7%94%A8.html</url>
    <content type="text"><![CDATA[Python与C++相互调用 参考博客：Python实例浅谈之三Python与C/C++相互调用 问题Python模块和C/C++的动态库间相互调用在实际的应用中会有所涉及，在此作一总结。Python调用C/C++Python调用C动态链接库Python调用C库比较简单，不经过任何封装打包成so，再使用python的ctypes调用即可 C语言文件：pycall.c 12345678/***gcc -o libpycall.so -shared -fPIC pycall.c*/ #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; int foo(int a, int b) &#123; printf("you input %d and %d\n", a, b); return a+b; &#125; 2.gcc编译生成动态库libpycall.so 1gcc -o libpycall.so -shared -fPIC pycall.c 使用g++编译生成C动态库的代码中的函数或者方法时，需要使用extern “C”来进行编译。 3.Python调用动态库的文件：pycall.py 1234567import ctypes ll = ctypes.cdll.LoadLibrary lib = ll("./libpycall.so") result = lib.foo(1, 3) print('***start***')print(result)print('***finish***') 4.运行结果： 12345c@j:~/Desktop/BLOG$ python3 pycall.py you input 1 and 3***start***4***finish*** Python调用C++(类)动态链接库需要extern “C”来辅助，也就是说还是只能调用C函数，不能直接调用方法，但是能解析C++方法。不是用extern “C”，构建后的动态链接库没有这些函数的符号表。 1.C++类文件：pycallclass.cpp 12345678910111213141516171819202122232425#include &lt;iostream&gt; using namespace std; class TestLib &#123; public: void display(); void display(int a); &#125;; void TestLib::display() &#123; cout&lt;&lt;"First display"&lt;&lt;endl; &#125; void TestLib::display(int a) &#123; cout&lt;&lt;"Second display:"&lt;&lt;a&lt;&lt;endl; &#125; extern "C" &#123; TestLib obj; void display() &#123; obj.display(); &#125; void display_int() &#123; obj.display(2); &#125; &#125; 2.g++编译生成动态库libpycall.so 1g++ -o libpycallclass.so -shared -fPIC pycallclass.cpp 3.Python调用动态库的文件：pycallclass.py 1234567import ctypes so = ctypes.cdll.LoadLibrary lib = so("./libpycallclass.so") print('display()')lib.display() print('display(100)') lib.display_int(100) 4.运行结果： 12345c@j:~/Desktop/BLOG$ python3 pycallclass.py display()First displaydisplay(100)Second display:2 Python调用C/C++可执行程序1.C/C++程序：main.cpp 1234567891011121314#include &lt;iostream&gt; using namespace std; int test() &#123; int a = 10, b = 5; return a+b; &#125; int main() &#123; cout&lt;&lt;"---begin---"&lt;&lt;endl; int num = test(); cout&lt;&lt;"num="&lt;&lt;num&lt;&lt;endl; cout&lt;&lt;"---end---"&lt;&lt;endl; &#125; 2.编译成二进制可执行文件 1g++ -o testmain main.cpp 3.Python调用程序：main.py 12345678910111213141516import subprocess //commands是python2版本的，这里用python3所以用subprocess进行替换import os main = "./testmain" if os.path.exists(main): rc, out = subprocess.getstatusoutput(main) print('rc = %d, \nout =\n%s' % (rc, out)) print('*'*10) f = os.popen(main) data = f.readlines() f.close() print(data) print('*'*10) os.system(main) //这句话是调用 4.运行结果： 123456789101112c@j:~/Desktop/BLOG$ python3 main.py rc = 0, out =---begin---num=15---end---**********['---begin---\n', 'num=15\n', '---end---\n']**********---begin---num=15---end--- 附：这里用到了一些函数，细说起来篇幅很多，不如单独写一篇解释一下，命名暂时叫：Python与C++相互调用_附录 扩展Python（C++为Python编写扩展模块）为 Python 创建扩展需要三个主要的步骤： 创建应用程序代码 利用样板来包装代码 编译与测试 1.创建应用程序代码 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; int fac(int n) &#123; if (n &lt; 2) return(1); /* 0! == 1! == 1 */ return (n)*fac(n-1); /* n! == n*(n-1)! */ &#125; char *reverse(char *s) &#123; register char t, /* tmp */ *p = s, /* fwd */ *q = (s + (strlen(s) - 1)); /* bwd */ while (p &lt; q) /* if p &lt; q */ &#123; t = *p; /* swap &amp; move ptrs */ *p++ = *q; *q-- = t; &#125; return(s); &#125; int main() &#123; char s[BUFSIZ]; printf("4! == %d\n", fac(4)); printf("8! == %d\n", fac(8)); printf("12! == %d\n", fac(12)); strcpy(s, "abcdef"); printf("reversing 'abcdef', we get '%s'\n", \ reverse(s)); strcpy(s, "madam"); printf("reversing 'madam', we get '%s'\n", \ reverse(s)); return 0; &#125; 上述代码中有两个函数，一个是递归求阶乘的函数fac()；另一个reverse()函数实现了一个简单的字符串反转算法，其主要目的是修改传入的字符串，使其内容完全反转，但不需要申请内存后反着复制的方法。 2.用样板来包装代码 接口的代码被称为“样板”代码，它是应用程序代码与Python解释器之间进行交互所必不可少的一部分。样板主要分为4步： 包含Python的头文件 为每个模块的每一个函数增加一个型如PyObject* Module_func()的包装函数 为每个模块增加一个型如PyMethodDef ModuleMethods[]的数组 增加模块初始化函数void initModule() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; int fac(int n) &#123; if (n &lt; 2) return(1); return (n)*fac(n-1); &#125; char *reverse(char *s) &#123; register char t, *p = s, *q = (s + (strlen(s) - 1)); while (s &amp;&amp; (p &lt; q)) &#123; t = *p; *p++ = *q; *q-- = t; &#125; return(s); &#125; int test() &#123; char s[BUFSIZ]; printf("4! == %d\n", fac(4)); printf("8! == %d\n", fac(8)); printf("12! == %d\n", fac(12)); strcpy(s, "abcdef"); printf("reversing 'abcdef', we get '%s'\n", \ reverse(s)); strcpy(s, "madam"); printf("reversing 'madam', we get '%s'\n", \ reverse(s)); return 0; &#125; #include "Python.h" static PyObject * Extest_fac(PyObject *self, PyObject *args) &#123; int num; if (!PyArg_ParseTuple(args, "i", &amp;num)) return NULL; return (PyObject*)Py_BuildValue("i", fac(num)); &#125; static PyObject * Extest_doppel(PyObject *self, PyObject *args) &#123; char *orig_str; char *dupe_str; PyObject* retval; if (!PyArg_ParseTuple(args, "s", &amp;orig_str)) return NULL; retval = (PyObject*)Py_BuildValue("ss", orig_str, dupe_str=reverse(strdup(orig_str))); free(dupe_str); #防止内存泄漏 return retval; &#125; static PyObject * Extest_test(PyObject *self, PyObject *args) &#123; test(); return (PyObject*)Py_BuildValue(""); &#125; static PyMethodDef ExtestMethods[] = &#123; &#123; "fac", Extest_fac, METH_VARARGS &#125;, &#123; "doppel", Extest_doppel, METH_VARARGS &#125;, &#123; "test", Extest_test, METH_VARARGS &#125;, &#123; NULL, NULL &#125;, &#125;; void initExtest() &#123; Py_InitModule("Extest", ExtestMethods); &#125; Python.h头文件在大多数类Unix系统中会在/usr/local/include/python2.x或/usr/include/python2.x目录中，系统一般都会知道文件安装的路径 增加包装函数，所在模块名为Extest，那么创建一个包装函数叫Extest_fac()，在Python脚本中使用是先import Extest，然后调用Extest.fac()，当Extest.fac()被调用时，包装函数Extest_fac()会被调用，包装函数接受一个 Python的整数参数，把它转为C的整数，然后调用C的fac()函数，得到一个整型的返回值，最后把这个返回值转为Python的整型数做为整个函数调用的结果返回回去。其他两个包装函数Extest_doppel()和Extest_test()类似。从Python到C的转换用PyArg_Parse*系列函数，int PyArg_ParseTuple()：把Python传过来的参数转为C；int PyArg_ParseTupleAndKeywords()与PyArg_ParseTuple()作用相同，但是同时解析关键字参数；它们的用法跟C的sscanf函数很像，都接受一个字符串流，并根据一个指定的格式字符串进行解析，把结果放入到相应的指针所指的变量中去，它们的返回值为1表示解析成功，返回值为0表示失败。从C到Python的转换函数是PyObject* Py_BuildValue()：把C的数据转为Python的一个对象或一组对象，然后返回之；Py_BuildValue的用法跟sprintf很像，把所有的参数按格式字符串所指定的格式转换成一个Python的对象。 为每个模块增加一个型如PyMethodDef ModuleMethods[]的数组，以便于Python解释器能够导入并调用它们，每一个数组都包含了函数在Python中的名字，相应的包装函数的名字以及一个METH_VARARGS常量，METH_VARARGS表示参数以tuple形式传入。 若需要使用PyArg_ParseTupleAndKeywords()函数来分析命名参数的话，还需要让这个标志常量与METH_KEYWORDS常量进行逻辑与运算常量 。数组最后用两个NULL来表示函数信息列表的结束。所有工作的最后一部分就是模块的初始化函数，调用Py_InitModule()函数，并把模块名和ModuleMethods[]数组的名字传递进去，以便于解释器能正确的调用模块中的函数。 3.编译 为了让新Python的扩展能被创建，需要把它们与Python库放在一起编译，distutils包被用来编译、安装和分发这些模块、扩展和包。创建一个setup.py文件，编译最主要的工作由setup()函数来完成： 123456#!/usr/bin/env python from distutils.core import setup, Extension MOD = 'Extest' setup(name=MOD, ext_modules=[Extension(MOD, sources=['Extest2.c'])]) Extension()第一个参数是(完整的)扩展的名字，如果模块是包的一部分的话，还要加上用’.’分隔的完整的包的名字。上述的扩展是独立的，所以名字只要写”Extest”就行；sources参数是所有源代码的文件列表，只有一个文件Extest2.c。setup需要两个参数：一个名字参数表示要编译哪个内容；另一个列表参数列出要编译的对象，上述要编译的是一个扩展，故把ext_modules参数的值设为扩展模块的列表。 4.运行setup.py build命令就可以开始编译我们的扩展了，提示部分信息： 12creating build/lib.linux-x86_64-2.6 gcc -pthread -shared build/temp.linux-x86_64-2.6/Extest2.o -L/usr/lib64 -lpython2.6 -o build/lib.linux-x86_64-2.6/Extest.so 5.导入和测试 6.引用计数和线程安全 Python对象引用计数的宏：Py_INCREF(obj)增加对象obj的引用计数，Py_DECREF(obj)减少对象obj的引用计数。Py_INCREF()和Py_DECREF()两个函数也有一个先检查对象是否为空的版本，分别为Py_XINCREF()和Py_XDECREF()。编译扩展的程序员必须要注意，代码有可能会被运行在一个多线程的Python环境中。这些线程使用了两个C宏Py_BEGIN_ALLOW_THREADS和Py_END_ALLOW_THREADS，通过将代码和线程隔离，保证了运行和非运行时的安全性，由这些宏包裹的代码将会允许其他线程的运行。 C/C++调用PythonC++可以调用Python脚本，那么就可以写一些Python的脚本接口供C++调用了，至少可以把Python当成文本形式的动态链接库，需要的时候还可以改一改，只要不改变接口。缺点是C++的程序一旦编译好了，再改就没那么方便了。 1.Python脚本：pytest.py 123456789101112131415161718192021222324252627#test function def add(a,b): print "in python function add" print "a = " + str(a) print "b = " + str(b) print "ret = " + str(a+b) return def foo(a): print "in python function foo" print "a = " + str(a) print "ret = " + str(a * a) return class guestlist: def __init__(self): print "aaaa" def p(): print "bbbbb" def __getitem__(self, id): return "ccccc" def update(): guest = guestlist() print guest['aa'] #update() 2.C++代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/**g++ -o callpy callpy.cpp -I/usr/include/python2.6 -L/usr/lib64/python2.6/config -lpython2.6**/ #include &lt;Python.h&gt; int main(int argc, char** argv) &#123; // 初始化Python //在使用Python系统前，必须使用Py_Initialize对其 //进行初始化。它会载入Python的内建模块并添加系统路 //径到模块搜索路径中。这个函数没有返回值，检查系统 //是否初始化成功需要使用Py_IsInitialized。 Py_Initialize(); // 检查初始化是否成功 if ( !Py_IsInitialized() ) &#123; return -1; &#125; // 添加当前路径 //把输入的字符串作为Python代码直接运行，返回0 //表示成功，-1表示有错。大多时候错误都是因为字符串 //中有语法错误。 PyRun_SimpleString("import sys"); PyRun_SimpleString("print '---import sys---'"); PyRun_SimpleString("sys.path.append('./')"); PyObject *pName,*pModule,*pDict,*pFunc,*pArgs; // 载入名为pytest的脚本 pName = PyString_FromString("pytest"); pModule = PyImport_Import(pName); if ( !pModule ) &#123; printf("can't find pytest.py"); getchar(); return -1; &#125; pDict = PyModule_GetDict(pModule); if ( !pDict ) &#123; return -1; &#125; // 找出函数名为add的函数 printf("----------------------\n"); pFunc = PyDict_GetItemString(pDict, "add"); if ( !pFunc || !PyCallable_Check(pFunc) ) &#123; printf("can't find function [add]"); getchar(); return -1; &#125; // 参数进栈 *pArgs; pArgs = PyTuple_New(2); // PyObject* Py_BuildValue(char *format, ...) // 把C++的变量转换成一个Python对象。当需要从 // C++传递变量到Python时，就会使用这个函数。此函数 // 有点类似C的printf，但格式不同。常用的格式有 // s 表示字符串， // i 表示整型变量， // f 表示浮点数， // O 表示一个Python对象。 PyTuple_SetItem(pArgs, 0, Py_BuildValue("l",3)); PyTuple_SetItem(pArgs, 1, Py_BuildValue("l",4)); // 调用Python函数 PyObject_CallObject(pFunc, pArgs); //下面这段是查找函数foo 并执行foo printf("----------------------\n"); pFunc = PyDict_GetItemString(pDict, "foo"); if ( !pFunc || !PyCallable_Check(pFunc) ) &#123; printf("can't find function [foo]"); getchar(); return -1; &#125; pArgs = PyTuple_New(1); PyTuple_SetItem(pArgs, 0, Py_BuildValue("l",2)); PyObject_CallObject(pFunc, pArgs); printf("----------------------\n"); pFunc = PyDict_GetItemString(pDict, "update"); if ( !pFunc || !PyCallable_Check(pFunc) ) &#123; printf("can't find function [update]"); getchar(); return -1; &#125; pArgs = PyTuple_New(0); PyTuple_SetItem(pArgs, 0, Py_BuildValue("")); PyObject_CallObject(pFunc, pArgs); Py_DECREF(pName); Py_DECREF(pArgs); Py_DECREF(pModule); // 关闭Python Py_Finalize(); return 0; &#125; 3.C++编译成二进制可执行文件 1g++ -o callpy callpy.cpp -I/usr/include/python3.6 -L/usr/lib64/python3.6/config -lpython3.6 编译选项需要手动指定Python的include路径和链接接路径（Python版本号根据具体情况而定） 4.运行结果：]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[page]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-7.%E8%B0%B7%E6%AD%8C%E8%BE%93%E5%85%A5%E6%B3%95.html</url>
    <content type="text"><![CDATA[Ubuntu上如何安装谷歌输入法要安装谷歌拼音输入法，可以直接安装fcitx-googlepinyin就可以了1.更新资源1sudo apt-get update2.安装fcitx-googlepinyin1sudo apt-get install fcitx-googlepinyin3.安装完成后，我们注销或者重启一次 4.顶部面板找到fcitx输入法标志，小键盘或者小企鹅，选择“配置Fcitx” 5.“输入法配置”窗口左下角，找到“+”加号 6.找到“Google拼音”，点选“确认”进行添加 7.可以切换到google拼音进行输入了]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo报错]]></title>
    <url>%2Freading%2Fhexo%E6%8A%A5%E9%94%99.html</url>
    <content type="text"><![CDATA[报错1234567H:\LeeSincc&gt;hexo gINFO Start processingFATAL Something's wrong. Maybe you can find the solution here: https://hexo.io/docs/troubleshooting.htmlTypeError: Cannot set property 'lastIndex' of undefined at highlight (H:\LeeSincc\node_modules\highlight.js\lib\highlight.js:579:35) at H:\LeeSincc\node_modules\highlight.js\lib\highlight.js:629:21 at Array.forEach (&lt;anonymous&gt;) 解决方法修改配置文件_config.yml，注意不是主题里面的配置文件！把auto_detect设置为false，即可解决。]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[配置深度学习GPU环境]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-6.%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0GPU%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[配置深度学习GPU环境若想配置GPU环境，首先，点击这里，选择 CUDA-Enabled GeForce Products ，查看你的GPU的计算能力，最好要大于3.0再配置GPU环境。如果你的电脑和服务器支持GPU环境，那么你就可以使用GPU来训练和测试模型。一般来说使用Nvidia显卡用于深度学习，在N卡上我们需要安装Nvidia显卡驱动、cuda、（cuDNN）三个东西，其中cuDNN可选安装。显卡驱动这里就不过多说明。官网CUDA(Compute Unified Device Architecture)是Nvidia推出的用于图像处理单元（GPU）通用计算的并行计算平台和编程模型，借助CUDA，开发人员可以通过利用GPU的强大功能大幅提高计算应用的速度。cuDNN(The NVIDIA CUDA Deep Neural Network library)是用于深度神经网络的GPU计算加速库。它提供了高度优化的标准程序，例如前向卷积反向卷积(forward and backward convolution)、池化(pooling)、标准化(normalization)、激活层(activation layers)等等。cuDNN是Nvidia深度学习软件开发工具包（SDK）的一部分。 下载说明为了在多框架下的兼容性考虑（比如 Tensorflow 1.4目前不支持cuda 9 和 cuDNN 7），Nvidia显卡驱动版本要为官网上提供的与显卡对应的最新版本，cuda版本(当前最新版本是9.0）最好选择8.0(下载地址），cuDNN版本(当前最新版本是7）选择cuDNN 6版本与cuda 8.0对应的版本(下载地址）。注意：cuda安装包下载时最好选择runfile下载:cuDNN需要先注册账号后再填一个调查文件方可下载选择cuDNN v6.0 Library for Linux下载：若下载时显示不出页面，或显示正在维护，则换个浏览器或是翻墙访问附：CUDA其他版本下载地址 Nvidia显卡驱动安装在安装CUDA的过程中可以选择安装CUDA中自带的显卡驱动，这样的好处是与cuda兼容，简便，但是有可能它自带的不是最新版本的，这样可能会发生掉驱动的问题，我推荐还是先装N卡驱动后再装CUDA。在正式安装N卡驱动前，必须将nouveau（Ubuntu系统集成的显卡驱动程序）禁用，它是第三方为NVIDIA开发的开源驱动 第一步：我们需要先将其屏蔽才能安装NVIDIA官方驱动，两者不兼容,执行 1lsmod | grep nouveau 若无结果输出，则直接进入第二步；若有结果输出，则说明当前的驱动是nouveau，执行如下程序命令禁用： 1sudo gedit /etc/modprobe.d/blacklist.conf 在文本中添加 12blacklist nouveauoptions nouveau modeset=0 保存后执行 1sudo update-initramfs -u 重启后再执行第一步的第一条命令，若无结果输出，则禁用nouveau成功。 第二步： 关闭X-Window服务，进入命令行模式 1sudo service lightdm stop 输入后图形界面会消失，按Ctrl+Alt+F1进入tty1第三步： 在tty1登录后在驱动对应目录下执行 12sudo chmod a+x NVIDIA-linux-x86_64-&lt;version&gt;.runsudo ./NVIDIA-linux-x86_64-&lt;version&gt;.run –no-opengl-files –no-x-check –no-nouveau-check -no-opengl-files：表示只安装驱动文件，不安装OpenGL文件。这个参数不可省略，否则会导致登陆界面死循环(”login loop”或者”stuck in login”) –no-x-check：表示安装驱动时不检查X服务，非必须。 –no-nouveau-check：表示安装驱动时不检查nouveau,非必须，但必须保证nouveau已经禁用。 在安装过程中，需先同意协议，若出现警告，则忽视即可。若安装失败，则可以关闭X-Window服务进入tty1，输入 1sudo ./NVIDIA-linux-x86_64-&lt;version&gt;.run –-uninstall 即可卸载，查找错误原因后重新安装。第四步： 安装完成后，打开X-Window服务，恢复GUI模式 1sudo service lightdm start 按Ctrl+Alt+F7进入GUI界面。至此N卡驱动安装完成。 Nvidia显卡驱动测试在安装完驱动后，在命令行输入 12nvidia-sminvidia-settings 若列出GPU的信息列表和设置对话框，表示驱动安装成功 此外，也可以输入如下命令查看当前驱动版本信息 1cat /proc/driver/nvidia/version CUDA安装CUDA的详细安装方法可以查看官网：链接第一步：我们这里只需要在当前目录下运行 1sudo sh cuda_&lt;version&gt;_linux.run 在安装过程中，每个选项依次为：接受协议不安装自带驱动，安装cuda8.0，同意默认安装位置，同意创建链接，同意安装样例，同意默认安装位置，即可完成。第二步: 配置环境变量 1sudo vim ~/.bashrc 在文件末尾添加 123export CUDA_HOME=/usr/local/cuda-8.0export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATHexport PATH=/usr/local/cuda-8.0/bin:$PATH 保存退出后刷新 1source ~/.bashrc 第三步： 动态链接库设置创建文件 1sudo vim /etc/ld.so.conf.d/cuda.conf 写入 1/usr/local/cuda/lib64 保存后，执行命令，立即生效 1sudo ldconfig -v CUDA测试执行命令查看cuda版本 1nvcc --version cuDNN安装cnDNN的详细安装方法可以查看官网：链接在下载完cuDNN文件后解压(其他版本方法一致）,进入到解压的cuda目录下,执行 123456#这里以v5.1版本举例，其他版本步骤具体版本号需要有微幅变动tar -zxvf cudnn-8.0-linux-x64-v5.1-ga.tgzcd cudasudo cp include/cudnn.h /usr/local/cuda/include/sudo cp lib64/lib* /usr/local/cuda/lib64/sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn* 接下来更新cuDNN库文件的软链接，注意顺序 12345cd /usr/local/cuda/lib64/sudo rm -rf libcudnn.so libcudnn.so.5sudo ln -s libcudnn.so.5.1.10 libcudnn.so.5sudo ln -s libcudnn.so.5 libcudnn.sosudo ldconfig cuDNN测试在下载cuDNN的网址，下载对应cuDNN版本的测试程序cuDNN v5 Code Samples，解压后切换到mnistCUDNN做测试，编译后执行。 1234tar -xvf cudnn-sample-v5.tgzcd mnistCUDNNmake clean &amp;&amp; make./mnistCUDNN 如果结果为：Test passed! ，则cuDNN安装成功。 参考博客：Yolo v2——Darknet框架搭建教程（含GPU安装）]]></content>
      <categories>
        <category>Ubuntu16.04装机系列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[编译安装OpenCV]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-5.%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85OpenCV.html</url>
    <content type="text"><![CDATA[编译安装OpenCVOpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效——由一系列 C 函数和少量 C++ 类构成，同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。OpenCV用C++语言编写，它的主要接口也是C++语言，但是依然保留了大量的C语言接口。该库也有大量的Python、Java and MATLAB/OCTAVE（版本2.5）的接口。这些语言的API接口函数可以通过在线文档获得。如今也提供对于C#、Ch、Ruby,GO的支持。所有新的开发和算法都是用C++接口。一个使用CUDA的GPU接口也于2010年9月开始实现。 如果你想详细了解OpenCV，请访问官网,上面不但有详细的文档，也有很多的示例供大家学习，后面我会模仿官网一一实现示例，提高对OpenCV的理解和学习 第一步：安装编译工具、依赖包和可选包 1234sudo apt-get updatesudo apt-get install build-essential #编译工具sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev #依赖包sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev #可选包 第二步：在官网下载对应版本的源代码，解压后新建release文件夹，编译。 123456cd opencvmkdir releasecd releasecmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..make -j8 #可根据自己电脑的性能 开启多线程sudo make install 第三步：刷新动态链接库 1sudo ldconfig 测试OPENCV测试1：在命令行输入python，输入 1import cv2 若成功加载，则安装成功。测试2：编写c++程序test.cpp 123456789101112131415161718192021222324252627#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;int main( int argc, char** argv )&#123; if( argc != 2) &#123; cout &lt;&lt;" Usage: display_image ImageToLoadAndDisplay" &lt;&lt; endl; return -1; &#125; Mat image; image = imread(argv[1], CV_LOAD_IMAGE_COLOR); // 读取图片 if(! image.data ) &#123; cout &lt;&lt; "Could not open or find the image" &lt;&lt; std::endl ; return -1; &#125; Mat canny_img; Canny(image,canny_img,50,200);//边缘检测 imwrite("canny.jpg", canny_img); //保存图片 //imshow( "canny", canny_img ); // 显示图片 GUI界面下可取消注释 //waitKey(0); // 等待关闭窗口 需要和显示图片同时取消注释 return 0;&#125; 编译： 1g++ test.cpp -o test `pkg-config --cflags --libs opencv` 若无错误提示，则正常编译后执行 1./test dog.jpg]]></content>
      <categories>
        <category>Ubuntu16.04装机系列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[七年级上册]]></title>
    <url>%2FEnglish%2F%E4%B8%83%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.html</url>
    <content type="text"><![CDATA[本来想自己写一个托福教程的，又不知道怎么搭建知识体系，那就不妨从零开始吧。前面大家都学过，考虑普适性还是放出来，有基础的直接跳过，基础差的扫扫雷，Let’s go!Module 1 My teacher and my friendsword hello class my is Miss good morning good morning afternoon good afternoon goodbye I am Mr what your please sorry can you spell it yes thank how are fine thanks Mrs too this she teacher friend her his nice to meet time go now bye see tomorrow Grammar my name is 询问对方姓名，表示礼貌先介绍自己姓名 Miss用于未婚女性姓之前，必须大写 问候：Good morning / afternoon / evening 道别：Goodbye , 较为正式，bye 多用于口语 询问对方姓名 12What&apos;s your name,please?I&apos;m Chen Sorry ? 请你再说一遍 询问对方是否能做某事 123Can you spell it,please?can 为情态动词，后接动词原形 熟人见面问候 12345How are you?Fine,thank you是熟人见面的问候语thank you 比 thanks 正式 向他人介绍某人 12345This is Miss Zhou.距离较近用 this 较远用 that通常不用 She is / He is This is 不能缩写， That is 可以缩写成 That&apos;s 初次见面打招呼 1nice to meet you 用于初次见面和久未见面回答：nice to meet you too 该是做某事的时间了 123It&apos;s time to go now //现在到走的时候了It&apos;s time to + 动词原形 //该做某事了 Module 2 My English lessonword sit down sit down open book listen draw up hand stand stand up close new student here in one two three four five six seven eight nine ten telphone number old how old twelve eleven thirteen fourteen fifteen sixteen seventeen eighteen nineteen twenty boy girl desk chair bag Grammar 祈使句 12345Please sit down 祈使句肯定结构都以动词原形开头可在句首句位加 please 委婉表达否定句构成为 Don&apos;t + 动词原形 询问对方是否是某身份 123Are you a new student here?一般疑问句，只能用 Yes 和 No 回答 回答在哪个班级 1I&apos;m in Class Three // 注意class 和 three 都要大写 询问对方在哪个班级 123What class are you in ?Mr Chen&apos;s class 是名词所有格，表示有生命的事物的所属关系，单数名词后加 ‘s 询问电话号码 1what&apos;s your telphone number? 询问年龄 123How old + be + 主语？回答：主语 + be + 数值 + years old // years old 可以省略，be动词的单复数和主语的人称和数保持一致 询问数量 123How many boys are there?how many 多少，后跟可数名词的复数形式，长于 are there 连用构成特殊疑问句，主要对数量提问，回答用 there is/are... 数值求和 12What is twelve and four ?It&apos;s sixteen Module 3 My English bookword in English in English a write on the blackboard Ok pen no pencil do bird cat dog flower help me of course of course classroom know say that again welcome Your’re welcome black white blue green yellow red brown orange colour and Grammar 询问某物用英文表达 1234What’s this in English?It&apos;s a book 是一个特殊疑问句，What&apos;s this 用来询问距离较近的事物，what&apos;s that 用来询问距离较远的事物，回答用it来替代 this 和 that the 的用法 123Write it on the blackboard,please.定冠词，用于特指已经谈到或正在谈到的事物 询问某词如何拼写 1How do you spell &apos;pencil&apos;? 请求句型 123Can you help me,please?Can you 表示请求。回答用 Yes,I can /of course /Sorry,I can&apos;t 表达要对方再重复一遍 123Can you say that again,please?口语中，通常使用 Sorry?/Pardon?来表达 表达不用谢 12You&apos;re welcome 用于回答 thank you类似的表达：That&apos;s OK/That&apos;s all right/Not at all---一点儿也不 询问物品颜色 123What colour is it?被询问的是复数，用are and 的用法 123It&apos;s red,green and orange.//它是红绿橙相间的and 作连词意思为’和，与‘，可以连接两个并列的单词短语，连接两个以上，and放在最后一个之前 orange 的用法 1234形容词：橘黄色可数名词：橘子，单数用 an 不可数名词：橘汁I want a cup of orange juice. Module 4 My everyday lifeword day today Monday Tuesday Wednesday Thursday Friday Saturday Sunday birthday favourite spring summer autumn winter warm hot cool cold weather like London basketball football swimming table tennis sport let us let’s = let us play after school idea Grammar 询问星期几 12What&apos;s day is it today?It&apos;s Monday 询问最喜欢哪天 1What day is your favourite day? 询问天气状况 1234What&apos;s the weather like in spring?同义表达：How is the weather...?回答：It&apos;s + 形容词 以Let开头的祈使句 1Let&apos;s play football after school. 介词on的用法 123在...的时候，主要用在星期几，具体的某一天on Sundayon a cold morning Module 5 My classmatesword Chinese from where year about what about Ms America not England hi American our grade he China everyone capital but very big city small first first name last last name all Grammar be动词的用法 123456be 动词有三种形式，am is areI amhe/she isyou are单数is 复数are be动词的否定表达 1直接在后面加 not be动词的一般疑问句 1陈述句改为一般疑问句直接将be动词提到主语之前构成 询问对方来自哪里 12Where are you from?I&apos;m from BeiJing. 确定对方国籍 1Are you English too? 人称代词 123451.I 任何情况都大写2.you3.he/she4.it5.they 形容词性物主代词 123451.my our2.her his its3.their形容词性物主代词在句子中作定语，充当形容词成分，后面必须跟名词 but用法 1是转折词，连接两个并列句 homework12341.Wang Hui and Liu Mei ( ) good friends. // are 缺少一个连系动词，注意主语是两个人，是复数2.Are you Wei Ming? // No,I&apos;m not注意不能用 Yes,I&apos;m ,肯定答语中不能缩写I am Module 6 My familyword aunt brother cousin daughter family father grandfather grandmother grandparent mother parent sister son uncle photo photograph these they mum left on the left dad right on the right who woman women next next to husband front in front of those bus station hospital hotel police theatre actor driver manager nurse policeman policemen we an job at same doctor farm worker man men shop its their Grammar 指示代词 12345this that these thosethis,these 指距离较近的人或者事物that,those 指距离较远的人或者事物注意：用于疑问句，注意答语，this和that用it；these和those用they 询问某人、物是否属于某人 12Is this your pen?Yes,it is 感叹句 123What a big family!what + a/an +adj + 单数名词 构成感叹句，如果是不可数名词，就不要a/an 询问某人是谁 123Who&apos;s this/that?很少使用who&apos;s she/he 因为这种询问不礼貌 名词所有格 1234567891.单数名词直接在后面加&apos;s2.以s，es结尾的复数名词在后面加&apos;3.of + 名词形式也可以表示所属关系，多用于无生命物体4.也可以用of + 名词所有格 或 of + 名词性物主代词构成双重所有格1.Lily&apos;s book2.my friends&apos; names3.the door of the room4.a friend of her father&apos;s 某些动词加上er/r/or构成相应的职业名词 12345act-actorteach-teacherwork-workerdrive-drivermanage-manager job和work的区别 12345job指某种具体的工作或职业，为可数名词It&apos;s a nice job.work通常指抽象意义的工作，为不可数名词I&apos;m out of work. 某些短语中有无冠词的区别 123in/at hospital 住院 in/at the hospital 在医院at school 在上学 at a school 在某一所学校go to school 去上学 go to the school 去学校 询问某人职业 12345What&apos;s your father job?同义表达：What&apos;s sb. ?What&apos;s do/does sb do? 名词所有格的特殊用法 123456789101112131415&apos;s 多用于有生命的名词1.在不以-s/-es结尾的复数名词，直接加&apos;sWomen&apos;s Day2.表示时间，距离的名词所有格与有生命的名词所有格构成方法相同today&apos;s class3.所有格表示地点at my aunt&apos;s 在我姑姑家at the doctor&apos;s 在医生的诊所4.当两个并列名词同时拥有某物，只需要在最后的名词加&apos;s，若两个分别拥有，则两个都加&apos;sTom and Jack&apos;s room.Tom&apos;s and Jack&apos;s room. homework1234567891.-Is that your sister? -Yes,___is. // it ,注意不能用she，一般用it回答that/this2.behind 在...后面 in front of 在...前面 next to 紧挨着 3.The Green family ___ from America. // is侧重整体，用单数 Module 7 My schoolword computer furniture 家具（总称） map picture television TV wall thirty forty fifty sixty seventy eighty ninety really 真正地；非常 how many there lot a lot of oh any world tree building 建筑物 hall 大厅 dining hall 饭厅 gate 大门 library 图书馆 office 办公室 playground 操场 science 科学 lab 实验室 laboratory 实验室 behind between middle near with 和；具有（品质，特征） for 为了 room Grammar There be 句型 123456789101112131415161718192021222324252627282930There are thirty students in my class.there is/are + 某物/人 + 地点 表示某地存在某物或某物，是一种客观上的存在，具体用法：1.肯定形式 1.There is + 不可数名词/可数名词单数 + 介词短语. // 不可数名词前加some，可数名词前加a/an eg1:There is a bed in my room. eg2:There is some milk in the bottle. 2.There are + 可数名词复数形式 + 介词短语 eg1:There are some students in the classroom.2.否定形式 There is/are not + 某物/人 + 地点 eg1:There isn&apos;t a pen in the desk. eg2:There isn&apos;t any meat in the bag. eg3:There aren&apos;t any boys in the classroom.3.一般疑问句形式 Be + there + (any) + 某物 + 地点？ Yes,there be./No,there be + not. eg1:-Is there any milk in the bottle? -Yes,there is eg2:-Are there any gilr in the classroom? -No,there aren&apos;t4.特殊疑问句 疑问句 + 一般疑问句？ eg1:-How many books are there on the desk? -There is only one book on the desk.5.当there be 句型接多个主语，它要用就近原则 即：如果靠近be动词的名词是单数，则用is，如果是复数，用are eg1:There is a pen and two pencil on the desk. eg2:There are two pencil and one pen on the desk. some 和 any 的用法 12345671.some用在肯定句中，some + 不可数名词或者可数名词的复数 eg1:There are some rice in the bottle. eg2.There are some bananas on the tree.2.any用在否定句/疑问句中，any + 不可数名词或可数名词的复数 eg1:There aren&apos;t any coats under the bed. eg2:Is there any milk in the cup? eg3:Are there any cakes on the table? 常用介词短语 behind 在…之后 next to 在…旁边 in front of 在…前面 on the right of 在…右边 on the left of 在…左边 between…and… 在…和…之间 in the middle of 在…中间 询问某地位置 1234Where is the playground?It&apos;s behind the classroom budilding.where is + 某地？ 用于询问某地的位置 There be 句型与 Have 的比较 12there be 句型表示‘某地有某物’，即存在关系动词have或has也表示有的意思，单数表示所属关系，经常用人作主语 in front of 和 in the front of 12345in front of 在...（范围外）前面There is a house in front of the river.in the front of 在...（范围内）前面There is a blackboard in the front of the classroom. after 和 behind区别 123456789after 表示时间，表示在一点或一段时间之后表示地点时，表示次序，意为‘在...后面&apos;He goes to school after 6 o&apos;clock. //他6点钟以后去学校We walk into the meeting room one after another. // 我们依次进入会议室behind 表示时间时，指迟于既定时间表示地点时，强调位置的后面The project is already a month behind with things. // 这个项目已经比预定进度晚了一个月The chair is behind the desk. // 椅子在桌子后面 homework123451.Cola isn&apos;t ___ healthy drink // a drink是可数名词2.Don&apos;t eat candy ___ ice cream. // or 否定句中用or3.There ___ some juice in the glass // is juice作果汁讲为不可数名词4.Jim is a ___ boy. He does exercise every day. // healthy 形容词5.I&apos;m thirty.Could I have ___ hot water? -OK.Here you are. // some用于肯定句和希望得到肯定的一般疑问句中 Module 8 Healthy foodword food drink candy 糖果 fruit meat vegetable apple bean 豆 beef 牛肉 carro 胡萝卜 chicken chocolate 巧克力 coffee cola 可乐 juice 果汁 milk potato 土豆 tea tomato 西红柿 water shop go shopping have have got 有；拥有 some much too much kind 种类 lots of so how about …好吗？ has bad healthy delicious bread 面包 fish hamburger 汉堡包 ice cream 冰淇淋 noodle 面条 rice suger 糖 eat child children be good for sweet 甜的 be bad for right egg eye cheese 奶酪 tooth teeth bit 一点儿 a bit tired soup 汤 important remember well stay 保持 fat get or breakfast every lunch home dinner banana buy Grammar have/has got 的用法 123456Have we got lots of chocolates?我们有很多巧克力吗？1.have got 表示某人有某物，如果主语是第三人称单数，用has got2.have/has got 否定在have/has后面加not3.have/has got 疑问句将have/has提前 some 和 any 区别 12345都有一些的意思，都作形容词，修饰可数名词，也可修饰不可数名词，用法有明显区别：1.some常用于肯定句2.any常用于否定句和疑问句3.some用于疑问句表示希望得到肯定的回答4.any用于肯定句表示任何一个 too much与too many 12too much 意思为太多的，其后接不可数名词too many 也表太多，其后接可数名词复数形式 可数名词与不可数名词 1234567891011121314151617181920212223241.可数名词复数形式有规则变化和不规则变化1.规则变化 1.一般直接加-s. egg-eggs,student-students 2.以o,s,x,ch,sh结尾的，在词尾加es，如果是无生命的物体就还是加s bus-buses,watch-watches,tomato-tomatoes,potato-potatoes,photo-photos,piano-pianos 3.以辅音+y结尾的，将y变i加es family-families,city-cities 4.以f(fe)结尾的词，变f(fe)为v，再加es life-lives,wife-wives,leaf-leaves 2.不规则变化 1.单复数同形 people-people，fish-fish 2.变单词中的元音 foot-feet，man-men 3.其他形式 child-children，mouse-mice 2.不可数名词无单复数变化，前面不能加数词或a/an 表数量用数词+量词+of+不可数名词 two cups of tea three pieces of bread 祈使句 12345Eat the right food and be healthy肯定祈使句多以动词原形开头否定祈使句应该在动词前加don&apos;tDon&apos;t drink too much cola! 连词or,and,but,so的用法 123456789101.or意思为或者，常用于否定句和选择疑问句中He can&apos;t read and write.Is it a boy or a girl?2.and 意思为和，连接两个并列的词，作为主语时，即使是两个不可数名词用and连接也是表达复数含义，谓语动词用复数Meat and fish are healthy food.Candy and cola are sweet food.3.but 意思为但是，表转折My room is small ,but it&apos;s clean.4.so意思所以，表结果I&apos;m tired so I go to bed. A bit ​ 123A bit tired? 感觉有点累吗？a bit 表示有点儿，稍微，用来修饰形容词、副词、动词如：The shoes are a bit old. homework What’s in the fridge? ___some milk in it. //There is 1表示某处有某物时用there be 句型，milk为不可数名词 It’s good for us ___ a glass of milk every day. // to drink 1It&apos;s + adj + for sb. + to do sth. //对某人来说做某事是...的 Module 9 My school dayword half past 晚于 o’clock to 距离点时间差… art geography history IT maths PE physical education lesson 课 then like difficult love subject 科目 because interesting talk begin when go to school weekday get up 起床 have breakfast house start work break 课间休息 have go home evening watch have dinner do homework bed go to bed sleep go to sleep park busy wash 洗涤 face minute Grammar 时刻的表达 12345678910111213141.顺读法5:20 five twenty12:45 twelve forty five2.倒读法 1.半个小时内用介词past表示，表示...点过...分。 3:10 ten past three 6:30 half past six 2.超过半小时用to表示，表示差...分到...点 6:50 ten to seven 8:58 two to nine 注意：a quarter = 十五分钟 表示时间的常用介词 12345671.at 指某个具体时间点at noon 在正午at 5 o&apos;clock 在正五点2.on 表示星期几 on Sunday 表示具体某天的早晨下午等，on Monday evening 周一晚上3.in 泛指在早上/中午/晚上 in the morning，表示月，季，年份 in May 在五月 in spring 在春天 in 1989在1989年 行为动词的一般现在时 1234567肯定句：主语 + 行为动词 + 其他I study in NO.1 Middle school.否定句：主语 + don&apos;t + 行为动词 + 其他You don&apos;t draw well.疑问句：Do/Does + 主语 + 行为动词原形 + 其他 ？ 询问时间的表达 123456789101112What&apos;s the time?It&apos;s twelve o&apos;clock.What&apos;s the time ? 用于询问时间，同义表达：What time is it?当询问何时上某课时，表达如下：When/What time is + 课程名注意：when 和 what time 有区别when 使用的范围较广，既可以对时刻提问，还可以问日期，月份，年份what time 只能对时刻提问如：When is your geography lesson?I have geography lesson at ten o&apos;clock. because 用法 123It&apos;s my favourite subject because it&apos;s very interesting.注意：because 与 so 不能出现在同一个句子中 talk 一词的用法 123456I can talk with my Chinese friends.talk 意为谈论，说话，是不及物动词，后面不能直接接宾语talk about sb./sth. 谈论某人/某事talk with sb. about sth. 与某人谈论某事 行为动词的一般现在时 123456789101.表示经常性或习惯性的动作I go to bed at 10:00.2.表示客观真理，客观存在，科学事实The earth move around the sun.3.表示格言警句Pride does before a fall. 骄者必败4.表示现在时刻的状态，能力，性格，个性I like English very much.5.表示按规定，计划，和时间表将要发生的事情He starts next week. 他下个星期出发。 start一词的用法 123456789We start work at nine o&apos;clock.start 意为开始作不及物动词Our first class starts at nine作及物动词start sth./doing sth./to do sth. 开始做某事I start to have lunch at 12:00.= I start having lunch at 12:00. with的用法 123456In the evening,I watch TV and have dinner with my family.1.作介词，意为和，同，与...一起I go to school with my brother.2.作介词，意为具有（品质/特征），带有China is a large country with a long history. homework We have _ English class _ Thursday afternoon. // an on 1具体到某一天下午用on We do ___ homework every day. //much 1homework 为不可数名词，所以用much Module 10 A trip to the zooword bear elephant giraffe lion monkey panda tiger zebra zoo guide animal such as such as come come from different country other dangerous ugh also plant look look at tall leaf leaves sure bamboo cute shall them which over there over there funny call Africa Asia Europe little a little only about kilo kilogram people all over the world African as well as grass large usually alone be good at strong catch many kinds of even Grammar 行为动词的一般现在时 1234561.当主语是第三人称单数时，谓语动词要用第三人称单数形式。变化规则： 1.一般直接加s，like-likes 2.以s,x,ch,sh结尾的动词，加es，kiss-kisses,watch-watches 3.以辅音字母+o结尾的动词，一般加es,go-gose 4.以辅音字母加y结尾的，变y为i再加es，study-studies 5.特殊变化。have-has 否定句：主语是第三人称单数时，行为动词的否定形式要用助动词doesn’t，再将行为动词变成原形 结构：主语 + doesn’t + 动词原形 + 其他 He likes tigers. He doesn’t like tigers. 一般疑问句：does放句首，再将行为动词变为动词原形 结构：Does + 主语 + 动词原形 + 其他？ Does your father like sports? Yes,he does./No,he doesn’t. look at ,see ,read与watch 12345678910111.look at 意为看，强调看的动作，但不一定能看得见或看得清楚Look at the elephant.2.see 表示看见，看到，强调看的结果Look at the picture.What can you see?3.read 意为读，阅读I like reading books.4.watch 意为观看，常指聚精会神地去看Let&apos;s watch TV. 征求他人意见 123Shall we go and see them? //去看看它们行吗？Shall I/We ... ? 意为...好吗？用于征求他人的意见 常见的倒装表达 123Look! There she is! //看，她在那儿！副词there，here放句首，常用倒装，多有强调之意，或者引起他人的注意，正常语序：Look!she is there! 表达物品类别 1234567891011121314The elephant lives Africa and in Asia.表达物品类别的有三种方式1.the + 名词单数2.a/an + 名词单数3.名词复数The panda loves bamboo.=A panda loves bamboo.=Panda love bamboo.注意：冠词an用在以元音音素（a,e,i,o,u)开头的名词前an art teacheran elephanta zebra a little ,little 和 fewer,a fewer 12345678910111. a little 少量的，修饰不可数名词，表示肯定意义Can you give me a little sugar?2. little 几乎没有，修饰不可数名词，表示否定意义There&apos;s little milk at home.Let&apos;s buy some.3.a fewer 非常少，修饰可数名词，表示肯定意义There are a fewer students in classroom.4. fewer 较少的，修饰可数名词，表示否定意义Few towns have such splendid trees. //有这么好看的树木的城市不多 even 一词的用法 123Monkeys eat meat,leaves,fruit and even eggs! 猴子吃肉，树叶，水果甚至是蛋类！even 意为甚至，一般放在所修饰成分的前面，强调甚至；连...也/都... but not 用法 123The zebra eats plants,leaves and grass but not bamboo.but not 意为但是不... ，在此相当于，but it does not eat ... 省略了与but前句子重复的部分 homework Giraffes like to eat _ and _ . //grass ；leaves 1grass 是不可数名词，leaf的复数形式是leaves The monkeys ___ are very clever. //from Asia 1the monkeys from Asia 其用法为介宾短语（from Asia）作定语修饰monkeys Is there ___ European elephant in the zoo? // a 1European 的读音是以辅音开头的，所以用a Module 11 Computersword keyboard mouse mice screen connect turn turn on learn document click use box finally print paper share Australia company often customer Internet check train travel plan ticket music movie night search search for information email send game sometimes cinema clothes visit holiday Grammar connect常用短语搭配 123456First,connect the screen to the computer.connect ... to ... 使...连接，通常指水电气connect ... with ... 把...联系起来The bridge connects the village with the town. turn on 与 open 比较 1234Finally，turn on the computer.turn on 与 open 均为打开，但turn on 指接通电流，煤气，水等Open 指打开门窗，盖子，容器，书籍等 对身体状况，方式等提问的句型 1How do I save the document? 表示先后顺序的句子衔接逻辑词 1234firstnextthenfinally what about 12345What about some paper?what about 用于建议，邀请，询问信息等后面跟名词，代词，动名词等What about going swimming? want 用法 1234561.want sth.I want a new bag.2.want sb. to do sth.I want to have a bike.I want him to go with me. 行为动词的一般现在时 1234行为动词的一般现在时的特殊疑问句结构：特殊疑问词 + do/does + 主语 + 谓语 + 其他？When does he do homework? 特殊疑问句 12345678910111213141.what ... ? 询问姓名，号码，名称，年龄，喜好，动作等 1.what time 2.what day 3.what colour2.how ... ? 询问状况，方式等 1.how old 2.how many 3.how much3.when ...?4.who ...?5.where ...?6.why ...?注意：特殊疑问句回答不能用yes或no send的用法 12345678I send email to my fridens and play computer games.send 是及物动词，意为发送，寄send sb sth = send sth to sbMy sister often sends me her photos.=My sister often sends her photos to me. check 123check 查看，检查常用表达：check in 入住、登记 plan 用法 1234561.plan 作可数名词，意为计划，方案make a plan 制定计划2.plan作动词，意为设计，计划plan to do sth. 计划做某事He plan to buy a computer. homework123turn on/off 打开/关闭look at 看pick up 捡起 Do the students often _ online _ school? -No,sometimes. // go ; at 12go online 和 at school 是固定搭配visit the website 访问网站，固定搭配 This is a present for you. That’s so ___ of you. // kind 1That&apos;s so kind of you. //你太好了 Module 12 Choosing presentsword card party present would always great cake never special cut give sing happy secret 秘密 concert 音乐会 magazine scarf silk dress T-shirt choose exercise wear expensive shoe spend money film song match weekday at weekday dear hear hear from afraid I’m afraid 恐怕（礼貌拒绝） can’t can not Grammar 向对方提出有礼貌的请求，邀请 1234Would you like to come to my birthday party?Yes,I&apos;d love towould you like ...? 常用来向对方提出有礼貌的请求，邀请 never用法 12345She never wears jeans or trainers. 她从不穿牛仔裤或软运动鞋never是个否定词，用于行为动词之前否定句中并列一般用or不用andThere is no water or food 接双宾语的常用动词 1234make sb sth = make sth for sb //给某人制作某物give sb sth = give sth to sb //给某人某物buy sb sth = buy sth for sb //给某人买某物give sb sth = give sth to sb //给某人某物 spend用法 12345Daming&apos;s mother spended a lot of money on clothes.spend + 名词 // 花费spend + 时间/金钱 + on sth. // 在...上花费spend + 时间/金钱 + (in) doing sth. // 花费...做 a lot of ,lots of ,many,much 1234a lot of = lots of = much 许多，大量，接不可数名词many 接可数名词复数形式a lot of 与 lots of 多用于肯定句many 或 much 多用于否定句或疑问句中 like to do 和 like doing 123like to do 表示做一件很具体而且是当下准备要做的事情like doing 指长期的爱好，喜欢经常做的事情 wear 与 put on 12wear 穿表示状态，后接衣物，眼镜，首饰等戴在身上的物品put on 表示穿上的动作，后接衣物鞋帽 频度副词 1234567891011常见的频度副词always 总是often 经常usually 通常sometimes 有时seldom 很少never 从不在句中位置：行为动词之前，be动词，情态动词或者助动词之后I often get up at five past six.You must never tell him. homework Sandra usually listens to CDs ___ her favourite singers. // by 1by 在此为介词，意为‘由...’ I have a dog. Wangwang is ___ name. //its 1名词前使用形容词性物主代词its He always ___ music and goes to concerts. // listens to 1由and连接的两个动词形式前后要保持一致，listen是不及物动词，其后不能直接接宾语 Module 13 People and placesword postcard call lie sun line take take photos wait wait for walk trip few a few sale on sale enjoy anyway 尽管如此 back go back drive off get off hotdog leave restaurant moment place thing most still star run study Grammar 重点短语表达 the Great Wall 长城 talk to friends 与朋友们交谈 take photos 拍照 a lot of / lots of 许多 lying in the sun 躺在阳关下 send me postcards 给我发贺卡 wait for the bus 等公共汽车 on sale 正在出售 go back to school 回到学校 postcards on sale 正在出售的贺卡 on a school trip 在郊游 by email 通过电子邮件 现在进行时 123456789101112131.现在进行时：表示现在，说话瞬间正在进行或发生的动作常与表示现在进行状态的时间状语连用look ,listen , now , at the moment , at this moment2.现在进行时的构成肯定句：主语 + am/is/are + 现在分词...They are waiting for buses. 否定句：主语 + am/is/are + not + 现在分词...They aren&apos;t waiting for buses.疑问句：Am/Is/Are + 主语 + 现在分词...?Are they waiting for buses? 现在分词的构成 123456781.动词 + ingbuy-buying ; stand-standing ; talk-talking2.以不发音的e结尾，去e再加ingwrite-writing ; lie-lying ; dance-dancing3.以重读闭音节结尾，呈现辅音+元音+辅音的结构，双写最后字母再加ingshop-shopping ; run-running ; swim-swimming enjoy用法 12345enjoy doing sth. 喜欢做某事She enjoys playing tennis.enjoy oneself 玩得开心，过得愉快I enjoyed myself in the party. wait for buses or trains 等公共汽车或火车 leave work 下班 drive home 开车回家 get off 下车 have afternoon tea 喝下午茶 go to the theater 去剧院 have a drink 喝点饮料 watch a film 看电影 go home 回家 drink coffee 喝咖啡 see friends 看望朋友 call home 给家人打电话 at this moment 此刻 询问某人正在做什么句型 12345What are you doing now?We are visiting the Forbidden City. 我们在参观紫禁城what + be + 主语 + going？ 用于询问某人正在做什么回答：主语 + be + V-ing homework Thank you for ___ us. // helping 1for 是介词，后接动词时应该用动名词形式（V-ing) I have got lots of pen friends.___ are from England. //Some /Some of them 1some 作代词，可作主语，of是介词，其后的人称代词应用宾格 Module 14 Spring Festivalword lantern dragon dance clean sweep 打扫 floor cook meal 一餐 speak happen ready get ready for festival 节日 quite 十分，相当 at the moment 此刻 beautiful at work away 在安全的地方 put away hard join hurry 赶快 hurry up Christmas February January before sweep away 扫去 luck table celebrate traditional dumpling programme sweater 毛线衣 coat 外套 Grammar clean the house 打扫房间 sweep the floor 扫地 cook the meal 做饭 make lanterns 做灯笼 get ready for …作准备 learn a dragon dance 学舞龙 Spring Festival 春节 at work 在工作 Hurry up 赶快 询问发生什么事句型 12345What&apos;s happening at Lingling&apos;s home?They&apos;re getting readying for Spring Festival.what&apos;s happening + 其他? 用于询问发生什么事回答要用现在进行时，因为询问现在的状态 询问某人是否正在做某事句型 1234Is Tom making lanterns?Yes,he is.Be + 主语 + doing sth? 用于询问某人是否正在做某事 go shopping 购物 have a big dinner 吃年夜饭 watch CCTV program 看CCTV节目 get luck money(Hongbao) 收压岁钱 set off firecrackers and fireworks 放爆竹烟花 put up red lanterns 挂红灯笼 put up Chinese couplets 贴春联 wear new clothes 穿新衣 visit friends and relatives 走亲访友 一般现在时与现在进行时比较 123456789101112131.现在进行时由：be + V-ing 1.现在进行时与always连用，往往含有赞扬厌恶责备等感情色彩 He always thinking of himself more than others. 2.常与现在进行时连用的时间状语有： now , at the moment ,除此之外，以look,listen等开头的祈使句常常是现在进行时 Listen!Who is singing over there? 听！是谁在唱歌？ 2.一般现在时：be + V原形+s/es 1.表示某种状态的动词，用一般现在时。一般不说I am knowing 而说 I know 2.在there和here引起的句子中，常用一般现在时代替现在进行时 Here comes the bus. 3.常与一般现在时连用的时间状语 always ， often ， usually ， sometimes ， never ， every day/week/month/year,on Sunday,in the moring... homework I know a man ___ Bill. //named 1named 意为名叫 It’s very cold.___ your coat,please. // Put on 12put on 意为穿上，表示动作wear 意为穿着，表示状态]]></content>
      <categories>
        <category>一战到托福</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pip安装虚拟环境]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-4.pip%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83.html</url>
    <content type="text"><![CDATA[pip安装虚拟环境软件行业最蛋疼的莫过于版本问题，很遭心，各个依赖包的版本问题，解决起来相当麻烦，能理清楚版本问题已经算是大牛了。团队协作开发，版本更为重要，同样的代码不同环境不能通用是很常见的事情。所以大家统一安装版本，开辟一个独立空间进行项目管理，这就是虚拟环境。1.pip安装virtualenv 1pip install virtualenv 2.查找python位置因为我还想灵活选择python版本，所以我先查了下 1whereis python 显示如下： 12t@t:~$ whereis pythonpython: /usr/bin/python3.5 /usr/bin/python3.5m-config /usr/bin/python2.7 /usr/bin/python3.5m /usr/bin/python /usr/bin/python2.7-config /usr/bin/python3.5-config /usr/lib/python3.5 /usr/lib/python2.7 /etc/python3.5 /etc/python2.7 /etc/python /usr/local/lib/python3.5 /usr/local/lib/python2.7 /usr/include/python3.5 /usr/include/python2.7 /usr/include/python3.5m /usr/share/python /usr/share/man/man1/python.1.gz 3.创建python3.5的虚拟环境1virtualenv --python=/usr/bin/python3.5 test 查看虚拟环境文件夹下的文件： 123t@t:~$ cd test/t@t:~/test$ lsbin include lib 4.激活虚拟环境1source bin/activate 显示如下： 12t@t:~/test$ source bin/activate(test) t@t:~/test$ 前面有个test说明激活成功 5.退出虚拟环境1deactivate 显示如下： 12(test) t@t:~/test$ deactivate t@t:~/test$ 一些问答 Q : 我想看看这个虚拟环境下文件的结构组成，ls太麻烦 A : 试试 tree 命令吧 1.如果电脑没有 tree ，尝试如下安装： 1sudo apt install tree 2.运行命令 1tree -L 3 显示如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980.├── bin│ ├── activate│ ├── activate.csh│ ├── activate.fish│ ├── activate.ps1│ ├── activate_this.py│ ├── activate.xsh│ ├── easy_install│ ├── easy_install-3.5│ ├── pip│ ├── pip3│ ├── pip3.5│ ├── python -&gt; python3.5│ ├── python3 -&gt; python3.5│ ├── python3.5│ ├── python-config│ └── wheel├── include│ └── python3.5m -&gt; /usr/include/python3.5m└── lib └── python3.5 ├── abc.py -&gt; /usr/lib/python3.5/abc.py ├── base64.py -&gt; /usr/lib/python3.5/base64.py ├── bisect.py -&gt; /usr/lib/python3.5/bisect.py ├── _bootlocale.py -&gt; /usr/lib/python3.5/_bootlocale.py ├── codecs.py -&gt; /usr/lib/python3.5/codecs.py ├── collections -&gt; /usr/lib/python3.5/collections ├── _collections_abc.py -&gt; /usr/lib/python3.5/_collections_abc.py ├── config-3.5m-x86_64-linux-gnu -&gt; /usr/lib/python3.5/config-3.5m-x86_64-linux-gnu ├── copy.py -&gt; /usr/lib/python3.5/copy.py ├── copyreg.py -&gt; /usr/lib/python3.5/copyreg.py ├── distutils ├── _dummy_thread.py -&gt; /usr/lib/python3.5/_dummy_thread.py ├── encodings -&gt; /usr/lib/python3.5/encodings ├── fnmatch.py -&gt; /usr/lib/python3.5/fnmatch.py ├── functools.py -&gt; /usr/lib/python3.5/functools.py ├── __future__.py -&gt; /usr/lib/python3.5/__future__.py ├── genericpath.py -&gt; /usr/lib/python3.5/genericpath.py ├── hashlib.py -&gt; /usr/lib/python3.5/hashlib.py ├── heapq.py -&gt; /usr/lib/python3.5/heapq.py ├── hmac.py -&gt; /usr/lib/python3.5/hmac.py ├── importlib -&gt; /usr/lib/python3.5/importlib ├── imp.py -&gt; /usr/lib/python3.5/imp.py ├── io.py -&gt; /usr/lib/python3.5/io.py ├── keyword.py -&gt; /usr/lib/python3.5/keyword.py ├── lib-dynload -&gt; /usr/lib/python3.5/lib-dynload ├── LICENSE.txt -&gt; /usr/lib/python3.5/LICENSE.txt ├── linecache.py -&gt; /usr/lib/python3.5/linecache.py ├── locale.py -&gt; /usr/lib/python3.5/locale.py ├── no-global-site-packages.txt ├── ntpath.py -&gt; /usr/lib/python3.5/ntpath.py ├── operator.py -&gt; /usr/lib/python3.5/operator.py ├── orig-prefix.txt ├── os.py -&gt; /usr/lib/python3.5/os.py ├── plat-x86_64-linux-gnu -&gt; /usr/lib/python3.5/plat-x86_64-linux-gnu ├── posixpath.py -&gt; /usr/lib/python3.5/posixpath.py ├── __pycache__ ├── random.py -&gt; /usr/lib/python3.5/random.py ├── reprlib.py -&gt; /usr/lib/python3.5/reprlib.py ├── re.py -&gt; /usr/lib/python3.5/re.py ├── rlcompleter.py -&gt; /usr/lib/python3.5/rlcompleter.py ├── shutil.py -&gt; /usr/lib/python3.5/shutil.py ├── site-packages ├── site.py ├── sre_compile.py -&gt; /usr/lib/python3.5/sre_compile.py ├── sre_constants.py -&gt; /usr/lib/python3.5/sre_constants.py ├── sre_parse.py -&gt; /usr/lib/python3.5/sre_parse.py ├── stat.py -&gt; /usr/lib/python3.5/stat.py ├── struct.py -&gt; /usr/lib/python3.5/struct.py ├── tarfile.py -&gt; /usr/lib/python3.5/tarfile.py ├── tempfile.py -&gt; /usr/lib/python3.5/tempfile.py ├── tokenize.py -&gt; /usr/lib/python3.5/tokenize.py ├── token.py -&gt; /usr/lib/python3.5/token.py ├── types.py -&gt; /usr/lib/python3.5/types.py ├── warnings.py -&gt; /usr/lib/python3.5/warnings.py ├── weakref.py -&gt; /usr/lib/python3.5/weakref.py └── _weakrefset.py -&gt; /usr/lib/python3.5/_weakrefset.py14 directories, 63 files 这个可以帮助你更好的理解虚拟环境整体架构我们装一个包看看它是不是在虚拟环境中：source bin/activate，pip install requests在site-packages，可以找到： 1234567891011121314151617181920├── site-packages │ ├── certifi │ ├── certifi-2019.6.16.dist-info │ ├── chardet │ ├── chardet-3.0.4.dist-info │ ├── easy_install.py │ ├── idna │ ├── idna-2.8.dist-info │ ├── pip │ ├── pip-19.2.3.dist-info │ ├── pkg_resources │ ├── __pycache__ │ ├── requests │ ├── requests-2.22.0.dist-info │ ├── setuptools │ ├── setuptools-41.2.0.dist-info │ ├── urllib3 │ ├── urllib3-1.25.3.dist-info │ ├── wheel │ └── wheel-0.33.6.dist-info Q : 这样tree找安装包是不是太麻烦了？有其他办法只看已安装包吗？ A : 使用 pip list 1pip list 显示如下： 12345678910Package Version ---------- ---------certifi 2019.6.16chardet 3.0.4 idna 2.8 pip 19.2.3 requests 2.22.0 setuptools 41.2.0 urllib3 1.25.3 wheel 0.33.6 Q : 想了解更多 pip 的骚操作 A : 查一下文档就可以了 1pip --help 显示如下： 1234567891011121314151617181920212223242526272829303132333435363738Usage: pip &lt;command&gt; [options]Commands: install Install packages. download Download packages. uninstall Uninstall packages. freeze Output installed packages in requirements format. list List installed packages. show Show information about installed packages. check Verify installed packages have compatible dependencies. config Manage local and global configuration. search Search PyPI for packages. wheel Build wheels from your requirements. hash Compute hashes of package archives. completion A helper command used for command completion. debug Show information useful for debugging. help Show help for commands.General Options: -h, --help Show help. --isolated Run pip in an isolated mode, ignoring environment variables and user configuration. -v, --verbose Give more output. Option is additive, and can be used up to 3 times. -V, --version Show version and exit. -q, --quiet Give less output. Option is additive, and can be used up to 3 times (corresponding to WARNING, ERROR, and CRITICAL logging levels). --log &lt;path&gt; Path to a verbose appending log. --proxy &lt;proxy&gt; Specify a proxy in the form [user:passwd@]proxy.server:port. --retries &lt;retries&gt; Maximum number of retries each connection should attempt (default 5 times). --timeout &lt;sec&gt; Set the socket timeout (default 15 seconds). --exists-action &lt;action&gt; Default action when a path already exists: (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort. --trusted-host &lt;hostname&gt; Mark this host as trusted, even though it does not have valid or any HTTPS. --cert &lt;path&gt; Path to alternate CA bundle. --client-cert &lt;path&gt; Path to SSL client certificate, a single file containing the private key and the certificate in PEM format. --cache-dir &lt;dir&gt; Store the cache data in &lt;dir&gt;. --no-cache-dir Disable the cache. --disable-pip-version-check Don't periodically check PyPI to determine whether a new version of pip is available for download. Implied with --no-index. --no-color Suppress colored output Q : 如何把自己包的版本信息导出？ A : 使用 freeze 1pip freeze &gt;requirements.txt 显示如下： 123456789(test) t@t:~/test$ pip freeze &gt;requirements.txt(test) t@t:~/test$ lsbin include lib requirements.txt(test) t@t:~/test$ cat requirements.txt certifi==2019.6.16chardet==3.0.4idna==2.8requests==2.22.0urllib3==1.25.3 Q ： 拿到这个requirements.txt如何一次安装呢？ A : -r 递归安装即可 1pip install -r requirements.txt]]></content>
      <categories>
        <category>Ubuntu16.04装机系列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu和Windows改pip源]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-3.%E6%9B%B4%E6%94%B9pip%E6%BA%90.html</url>
    <content type="text"><![CDATA[Ubuntu和Windows改pip源如果使用Python开发较多，那么pip安装方式一定不会陌生，和Ubuntu改源的原因一样，也是为了提高装包速度，下面介绍pip的改源方法Ubuntu 改pip源1.临时使用1pip install [package] -i https://pypi.tuna.tsinghua.edu.cn/simple 2.永久修改（推荐） 修改 ~/.pip/pip.conf (没有就创建一个) 1234cd ~mkdir .pipcd .pipvim pip.conf 填入内容 12[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple Windows 改pip源 直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini 填入内容12[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple 其他pip 还是有很多坑点的，不过我写博客的时候我电脑已经搞好了，等那天又遇到问题，再来补充。相信2020年python2逐渐淘汰，情况会好很多 注意关注你的pip是2还是3版本，两个玩意装包的位置是不一样，提示你更新按报错信息升级就可以了，3用的多，干脆就换pip3 ,安装方式： 1sudo apt-get install python3-pip]]></content>
      <categories>
        <category>Ubuntu16.04装机系列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04改源]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-2.%E6%9B%B4%E6%94%B9Ubuntu%E6%BA%90%E4%B8%BA%E5%9B%BD%E5%86%85%E6%BA%90.html</url>
    <content type="text"><![CDATA[Ubuntu16.04改源为了后续安装的快捷，装好系统的第一件事就是改源，通常我们选择国内的源1.编辑源文件1sudo gedit /etc/apt/sources.list2.删除内容，粘贴以下源内容，可任意选择 清华源 12345678910deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 阿里云源 12345678910deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse 网易源 12345678910deb http://mirrors.163.com/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ xenial-backports main restricted universe multiverse 浙大源 12345678910deb http://mirrors.zju.edu.cn/ubuntu xenial main universe restricted multiversedeb http://mirrors.zju.edu.cn/ubuntu xenial-security main universe restricted multiversedeb http://mirrors.zju.edu.cn/ubuntu xenial-updates main universe restricted multiversedeb http://mirrors.zju.edu.cn/ubuntu xenial-proposed main universe restricted multiversedeb http://mirrors.zju.edu.cn/ubuntu xenial-backports main universe restricted multiversedeb-src http://mirrors.zju.edu.cn/ubuntu xenial main universe restricted multiversedeb-src http://mirrors.zju.edu.cn/ubuntu xenial-security main universe restricted multiversedeb-src http://mirrors.zju.edu.cn/ubuntu xenial-updates main universe restricted multiversedeb-src http://mirrors.zju.edu.cn/ubuntu xenial-proposed main universe restricted multiversedeb-src http://mirrors.zju.edu.cn/ubuntu xenial-backports main universe restricted multiverse 中科大源 12345678910deb http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiversedeb http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-proposed main restricted universe multiversedeb-src http://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse 东北大学 1234567891011121314151617deb-src http://mirror.neu.edu.cn/ubuntu/ xenial main restricted #Added by software-propertiesdeb http://mirror.neu.edu.cn/ubuntu/ xenial main restricteddeb-src http://mirror.neu.edu.cn/ubuntu/ xenial restricted multiverse universe #Added by software-propertiesdeb http://mirror.neu.edu.cn/ubuntu/ xenial-updates main restricteddeb-src http://mirror.neu.edu.cn/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirror.neu.edu.cn/ubuntu/ xenial universedeb http://mirror.neu.edu.cn/ubuntu/ xenial-updates universedeb http://mirror.neu.edu.cn/ubuntu/ xenial multiversedeb http://mirror.neu.edu.cn/ubuntu/ xenial-updates multiversedeb http://mirror.neu.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirror.neu.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirror.neu.edu.cn/ubuntu/ xenial-security main restricteddeb-src http://mirror.neu.edu.cn/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirror.neu.edu.cn/ubuntu/ xenial-security universedeb http://mirror.neu.edu.cn/ubuntu/ xenial-security multiverse 西电源（只有校内网网线使用，但是不限制流量） 12345678910deb http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial main restricted universe multiverse#deb-src http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial main restricted universe multiversedeb http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-security main restricted universe multiverse#deb-src http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-security main restricted universe multiversedeb http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-updates main restricted universe multiverse#deb-src http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-updates main restricted universe multiverse#deb http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-backports main restricted universe multiverse#deb-src http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-backports main restricted universe multiverse#deb http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-proposed main restricted universe multiverse#deb-src http://linux.xidian.edu.cn/mirrors/ubuntu/ xenial-proposed main restricted universe multiverse 3.更新源1sudo apt-get update 4.改源心得改源的主要用途是提高下载软件包的速度，还有一个点会被忽略，那就是不同的源，含有的包会有些差异，有时候安装某些包一直安装不上，可以换个源试试。这个经验是我很多次安装不了包，报一堆错，跑到网上查教程，结果全是教我怎么用sudo apt-get ...，偶然的机会发现了这个技巧，虽然很白痴 5.其他方法系统自带了可视化改源设置，适合快速改源 1.点开右上角设置,选择系统设置![](/images/Screenshot from 2019-09-11 02-12-16.png) 2.选择软件和更新![](/images/Screenshot from 2019-09-11 02-13-46.png) 3.点击 Download from 选择 other 即可选择你喜欢的源![](/images/Screenshot from 2019-09-11 02-14-56.png) 4.关闭设置，弹出界面选择加载，让它更新![](/images/Screenshot from 2019-09-11 02-16-19.png) 两个方法结合使用，更加深刻理解其中步骤]]></content>
      <categories>
        <category>Ubuntu16.04装机系列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 系统安装]]></title>
    <url>%2FUbuntu%2FUbuntu16.04%E8%A3%85%E6%9C%BA%E7%B3%BB%E5%88%97-1.%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[Ubuntu 16.04 系统安装前期准备 一个Ubuntu16.04的系统 一台破电脑 一个破启动盘 一根破网线 启动盘可以用软碟通进行制作系统可以上Ubuntu官网，或者清华源镜像进行下载 装机过程1.插入U盘2.启动电脑，出现品牌图样时，狂按bios键，选择U盘启动，进入装机界面3.进行安装，不要联网，点完重启，拔掉U盘 总结因为整体安装简单，所以没有详细介绍，网上教程较多，这篇博客进行占位，哪天我需要重装的时候，有图片说明再来补充]]></content>
      <categories>
        <category>Ubuntu16.04装机系列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[21.21个项目玩转深度学习之策略梯度算法]]></title>
    <url>%2Ftensorflow%2F21.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[策略梯度（ Policy Gradient ）方法和以上做法不同，不再去学习价值函数 Q ，而是直接通过模型（如神经网络）输出需要采取的动作 策略梯度算法的原理策略网络策略梯度算法主要是训练、一个策略网络（ Policy Network ） 这个网络的输入还是当前的状态 state ，而输出是当前应当采取的动作 训练策略网络将每一个动作都扔进背包，最终根据输赢决定背包状态对结果的影响，将赢的动作统计起来，将输的动作统计起来。 在 TensorFlow 中实现策略梯度程序大体分为三个部分：初始化 、定义策略网络 、 训练策略网络三部分 cartpole_pg.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152# coding:utf-8from __future__ import divisionfrom __future__ import print_functionimport numpy as npimport tensorflow as tfimport gym# gym环境env = gym.make('CartPole-v0')# 超参数D = 4 # 输入层神经元个数H = 10 # 隐层神经元个数batch_size = 5 # 一个batch中有5个episode，即5次游戏learning_rate = 1e-2 # 学习率gamma = 0.99 # 奖励折扣率gamma# 定义policy网络# 输入观察值，输出右移的概率observations = tf.placeholder(tf.float32, [None, D], name="input_x")W1 = tf.get_variable("W1", shape=[D, H], initializer=tf.contrib.layers.xavier_initializer())layer1 = tf.nn.relu(tf.matmul(observations, W1))W2 = tf.get_variable("W2", shape=[H, 1], initializer=tf.contrib.layers.xavier_initializer())score = tf.matmul(layer1, W2)probability = tf.nn.sigmoid(score)# 定义和训练、loss有关的变量tvars = tf.trainable_variables()input_y = tf.placeholder(tf.float32, [None, 1], name="input_y")advantages = tf.placeholder(tf.float32, name="reward_signal")# 定义loss函数loglik = tf.log(input_y * (input_y - probability) + (1 - input_y) * (input_y + probability))loss = -tf.reduce_mean(loglik * advantages)newGrads = tf.gradients(loss, tvars)# 优化器、梯度。adam = tf.train.AdamOptimizer(learning_rate=learning_rate)W1Grad = tf.placeholder(tf.float32, name="batch_grad1")W2Grad = tf.placeholder(tf.float32, name="batch_grad2")batchGrad = [W1Grad, W2Grad]updateGrads = adam.apply_gradients(zip(batchGrad, tvars))def discount_rewards(r): """ 输入： 1维的float类型数组，表示每个时刻的奖励 输出： 计算折扣率gamma后的期望奖励 """ discounted_r = np.zeros_like(r) running_add = 0 for t in reversed(range(0, r.size)): running_add = running_add * gamma + r[t] discounted_r[t] = running_add return discounted_rxs, hs, dlogps, drs, ys, tfps = [], [], [], [], [], []running_reward = Nonereward_sum = 0episode_number = 1total_episodes = 10000init = tf.global_variables_initializer()# 开始训练with tf.Session() as sess: rendering = False sess.run(init) # observation是环境的初始观察量（输入神经网络的值） observation = env.reset() # gradBuffer会存储梯度，此处做一初始化 gradBuffer = sess.run(tvars) for ix, grad in enumerate(gradBuffer): gradBuffer[ix] = grad * 0 while episode_number &lt;= total_episodes: # 当一个batch内的平均奖励达到180以上时，显示游戏窗口 if reward_sum / batch_size &gt; 180 or rendering is True: env.render() rendering = True # 输入神经网络的值 x = np.reshape(observation, [1, D]) # action=1表示向右移 # action=0表示向左移 # tfprob为网络输出的向右走的概率 tfprob = sess.run(probability, feed_dict=&#123;observations: x&#125;) # np.random.uniform()为0~1之间的随机数 # 当它小于tfprob时，就采取右移策略，反之左移 action = 1 if np.random.uniform() &lt; tfprob else 0 # xs记录每一步的观察量，ys记录每一步采取的策略 xs.append(x) y = 1 if action == 0 else 0 ys.append(y) # 执行action observation, reward, done, info = env.step(action) reward_sum += reward # drs记录每一步的reward drs.append(reward) # 一局游戏结束 if done: episode_number += 1 # 将xs、ys、drs从list变成numpy数组形式 epx = np.vstack(xs) epy = np.vstack(ys) epr = np.vstack(drs) tfp = tfps xs, hs, dlogps, drs, ys, tfps = [], [], [], [], [], [] # reset array memory # 对epr计算期望奖励 discounted_epr = discount_rewards(epr) # 对期望奖励做归一化 discounted_epr -= np.mean(discounted_epr) discounted_epr //= np.std(discounted_epr) # 将梯度存到gradBuffer中 tGrad = sess.run(newGrads, feed_dict=&#123;observations: epx, input_y: epy, advantages: discounted_epr&#125;) for ix, grad in enumerate(tGrad): gradBuffer[ix] += grad # 每batch_size局游戏，就将gradBuffer中的梯度真正更新到policy网络中 if episode_number % batch_size == 0: sess.run(updateGrads, feed_dict=&#123;W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]&#125;) for ix, grad in enumerate(gradBuffer): gradBuffer[ix] = grad * 0 # 打印一些信息 print('Episode: %d ~ %d Average reward: %f. ' % (episode_number - batch_size + 1, episode_number, reward_sum // batch_size)) # 当我们在batch_size游戏中平均能拿到200的奖励，就停止训练 if reward_sum // batch_size &gt;= 200: print("Task solved in", episode_number, 'episodes!') break reward_sum = 0 observation = env.reset()print(episode_number, 'Episodes completed.') 直接运行1python cartpole_pg.py]]></content>
  </entry>
  <entry>
    <title><![CDATA[20.21个项目玩转深度学习之强化学习入门之Deep_Q_Learning]]></title>
    <url>%2Ftensorflow%2F20.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B9%8BDeep_Q_Learning.html</url>
    <content type="text"><![CDATA[DQN 算法采用深度神经网络来表示 Q 函数，通常也被称为 Deep Q Learning DQN 算法的原理Deep Q NetworkDQN 算法用一个深度卷积神经网络来表示 Q 函数 训练方法经验回放机制（ experience replay mechanism ），用来产生神经网络的训练样本 。 在 TensorFlow 中运行 DQN 算法安装依赖库1pip install gym[all] scipy tqdm 训练使用GPU训练： 1python main.py --network_header_type=nips --env_name=Breakout-v0 --use_gpu=True 使用CPU训练： 1python main.py --network_header_type=nips --env_name=Breakout-v0 --use_gpu=False 打开TensorBoard：1tensorboard --logdir logs/ 测试测试在GPU上训练的模型： 1python main.py --network_header_type=nips --env_name=Breakout-v0 --use_gpu=True --is_train=False 测试在CPU上训练的模型： 1python main.py --network_header_type=nips --env_name=Breakout-v0 --use_gpu=True --is_train=True 在上述命令中加入–display=True选项，可以实时显示游戏进程。]]></content>
  </entry>
  <entry>
    <title><![CDATA[19.21个项目玩转深度学习之强化学习入门之SARSA算法]]></title>
    <url>%2Ftensorflow%2F19.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B9%8BSARSA%E7%AE%97%E6%B3%95.html</url>
    <content type="text"><![CDATA[SARSA ( State-Action-Reward-State-Action ）算法同样是一种基本的强化学习的算法。 SARSA 算法的原理智能体从一个状态（ state ，用 S 表示）出发， 执行一个动作（ action ，用 A 表示），得到奖励（ reward ，用 R 表示）和新的状态（ S），在新的状态下又会选择一个新的动作（ A ），通过新的状态和新的动作来更新 Q 函数 。 S-A- R-S -A ，这是 SARSA 算法名字的由来 off-policy 与 on-policyQ Learning 算法是一个经典的 oflιpolicy 方法，而本章介绍的 SARSA 算法则是 on-policy方法 运行：1python sarsa.py reprint版本（便于观察结果）： 1python sarsa_reprint.py env.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485from __future__ import print_functionimport copyMAP = \ '''.......... .. o .. ..........'''# MAP = \# '''# .........# . x .# . x o .# . .# .........# '''MAP = MAP.strip().split('\n')MAP = [[c for c in line] for line in MAP]DX = [-1, 1, 0, 0]DY = [0, 0, -1, 1]class Env(object): def __init__(self): self.map = copy.deepcopy(MAP) self.x = 1 self.y = 1 self.step = 0 self.total_reward = 0 self.is_end = False def interact(self, action): assert self.is_end is False new_x = self.x + DX[action] new_y = self.y + DY[action] new_pos_char = self.map[new_x][new_y] self.step += 1 if new_pos_char == '.': reward = 0 # do not change position elif new_pos_char == ' ': self.x = new_x self.y = new_y reward = 0 elif new_pos_char == 'o': self.x = new_x self.y = new_y self.map[new_x][new_y] = ' ' # update map self.is_end = True # end reward = 100 elif new_pos_char == 'x': self.x = new_x self.y = new_y self.map[new_x][new_y] = ' ' # update map reward = -5 self.total_reward += reward return reward @property def state_num(self): rows = len(self.map) cols = len(self.map[0]) return rows * cols @property def present_state(self): cols = len(self.map[0]) return self.x * cols + self.y def print_map(self): printed_map = copy.deepcopy(self.map) printed_map[self.x][self.y] = 'A' print('\n'.join([''.join([c for c in line]) for line in printed_map])) def print_map_with_reprint(self, output_list): printed_map = copy.deepcopy(self.map) printed_map[self.x][self.y] = 'A' printed_list = [''.join([c for c in line]) for line in printed_map] for i, line in enumerate(printed_list): output_list[i] = line sarsa.py123456789101112131415161718192021222324252627282930313233343536373839from __future__ import print_functionimport numpy as npimport timefrom env import EnvEPSILON = 0.1ALPHA = 0.1GAMMA = 0.9MAX_STEP = 50np.random.seed(1)def epsilon_greedy(Q, state): if (np.random.uniform() &gt; 1 - EPSILON) or ((Q[state, :] == 0).all()): action = np.random.randint(0, 4) # 0~3 else: action = Q[state, :].argmax() return actione = Env()Q = np.zeros((e.state_num, 4))for i in range(200): e = Env() action = epsilon_greedy(Q, e.present_state) while (e.is_end is False) and (e.step &lt; MAX_STEP): state = e.present_state reward = e.interact(action) new_state = e.present_state new_action = epsilon_greedy(Q, e.present_state) Q[state, action] = (1 - ALPHA) * Q[state, action] + \ ALPHA * (reward + GAMMA * Q[new_state, new_action]) action = new_action e.print_map() time.sleep(0.1) print('Episode:', i, 'Total Step:', e.step, 'Total Reward:', e.total_reward) time.sleep(2) sarsa_reprint.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546from __future__ import print_functionimport numpy as npimport timefrom env import Envfrom reprint import outputEPSILON = 0.1ALPHA = 0.1GAMMA = 0.9MAX_STEP = 50np.random.seed(1)def epsilon_greedy(Q, state): if (np.random.uniform() &gt; 1 - EPSILON) or ((Q[state, :] == 0).all()): action = np.random.randint(0, 4) # 0~3 else: action = Q[state, :].argmax() return actione = Env()Q = np.zeros((e.state_num, 4))with output(output_type="list", initial_len=len(e.map), interval=0) as output_list: for i in range(100): e = Env() action = epsilon_greedy(Q, e.present_state) while (e.is_end is False) and (e.step &lt; MAX_STEP): state = e.present_state reward = e.interact(action) new_state = e.present_state new_action = epsilon_greedy(Q, e.present_state) Q[state, action] = (1 - ALPHA) * Q[state, action] + \ ALPHA * (reward + GAMMA * Q[new_state, new_action]) action = new_action e.print_map_with_reprint(output_list) time.sleep(0.1) for line_num in range(len(e.map)): if line_num == 0: output_list[0] = 'Episode:&#123;&#125; Total Step:&#123;&#125;, Total Reward:&#123;&#125;'.format(i, e.step, e.total_reward) else: output_list[line_num] = '' time.sleep(2)]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[18.21个项目玩转深度学习之强化学习入门之Q_Learning]]></title>
    <url>%2Ftensorflow%2F18.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B9%8BQ_Learning.html</url>
    <content type="text"><![CDATA[强化学习的几个核心概念 智能体（ Agent ） 环境（ Environment ） 动作（ Action ） 奖励 （ Reward ） Q Learning 的原理与实验Q Learning 是强化学习的一种基础算法 运行：1python q_learning.py由于q_learning.py 会不断刷新输出，不太适合观察，因此又提供了一个q_learning_reprint.py便于观察结果（需要pip install reprint）： 1python q_learning_reprint.py env.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586from __future__ import print_functionimport copyMAP = \ '''.......... .. o .. ..........'''# MAP = \# '''# .........# . x .# . x o .# . .# .........# '''MAP = MAP.strip().split('\n')MAP = [[c for c in line] for line in MAP]DX = [-1, 1, 0, 0]DY = [0, 0, -1, 1]class Env(object): def __init__(self): self.map = copy.deepcopy(MAP) self.x = 1 self.y = 1 self.step = 0 self.total_reward = 0 self.is_end = False def interact(self, action): assert self.is_end is False new_x = self.x + DX[action] new_y = self.y + DY[action] new_pos_char = self.map[new_x][new_y] self.step += 1 if new_pos_char == '.': reward = 0 # do not change position elif new_pos_char == ' ': self.x = new_x self.y = new_y reward = 0 elif new_pos_char == 'o': self.x = new_x self.y = new_y self.map[new_x][new_y] = ' ' # update map self.is_end = True # end reward = 100 elif new_pos_char == 'x': self.x = new_x self.y = new_y self.map[new_x][new_y] = ' ' # update map reward = -5 self.total_reward += reward return reward @property def state_num(self): rows = len(self.map) cols = len(self.map[0]) return rows * cols @property def present_state(self): cols = len(self.map[0]) return self.x * cols + self.y def print_map(self): printed_map = copy.deepcopy(self.map) printed_map[self.x][self.y] = 'A' print('\n'.join([''.join([c for c in line]) for line in printed_map])) def print_map_with_reprint(self, output_list): printed_map = copy.deepcopy(self.map) printed_map[self.x][self.y] = 'A' printed_list = [''.join([c for c in line]) for line in printed_map] for i, line in enumerate(printed_list): output_list[i] = line q_learning.py12345678910111213141516171819202122232425262728293031323334353637from __future__ import print_functionimport numpy as npimport timefrom env import EnvEPSILON = 0.1ALPHA = 0.1GAMMA = 0.9MAX_STEP = 30np.random.seed(0)def epsilon_greedy(Q, state): if (np.random.uniform() &gt; 1 - EPSILON) or ((Q[state, :] == 0).all()): action = np.random.randint(0, 4) # 0~3 else: action = Q[state, :].argmax() return actione = Env()Q = np.zeros((e.state_num, 4))for i in range(200): e = Env() while (e.is_end is False) and (e.step &lt; MAX_STEP): action = epsilon_greedy(Q, e.present_state) state = e.present_state reward = e.interact(action) new_state = e.present_state Q[state, action] = (1 - ALPHA) * Q[state, action] + \ ALPHA * (reward + GAMMA * Q[new_state, :].max()) e.print_map() time.sleep(0.1) print('Episode:', i, 'Total Step:', e.step, 'Total Reward:', e.total_reward) time.sleep(2) q_learning_reprint.py1234567891011121314151617181920212223242526272829303132333435363738394041424344from __future__ import print_functionimport numpy as npimport timefrom env import Envfrom reprint import outputEPSILON = 0.1ALPHA = 0.1GAMMA = 0.9MAX_STEP = 30np.random.seed(0)def epsilon_greedy(Q, state): if (np.random.uniform() &gt; 1 - EPSILON) or ((Q[state, :] == 0).all()): action = np.random.randint(0, 4) # 0~3 else: action = Q[state, :].argmax() return actione = Env()Q = np.zeros((e.state_num, 4))with output(output_type="list", initial_len=len(e.map), interval=0) as output_list: for i in range(100): e = Env() while (e.is_end is False) and (e.step &lt; MAX_STEP): action = epsilon_greedy(Q, e.present_state) state = e.present_state reward = e.interact(action) new_state = e.present_state Q[state, action] = (1 - ALPHA) * Q[state, action] + \ ALPHA * (reward + GAMMA * Q[new_state, :].max()) e.print_map_with_reprint(output_list) time.sleep(0.1) for line_num in range(len(e.map)): if line_num == 0: output_list[0] = 'Episode:&#123;&#125; Total Step:&#123;&#125;, Total Reward:&#123;&#125;'.format(i, e.step, e.total_reward) else: output_list[line_num] = '' time.sleep(2)]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[17.21个项目玩转深度学习之看图说话]]></title>
    <url>%2Ftensorflow%2F17.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%9C%8B%E5%9B%BE%E8%AF%B4%E8%AF%9D.html</url>
    <content type="text"><![CDATA[Image Caption 技术综述从 Encoder-Decoder结构谈起原始的RNN结构中，输入序列和输出序列是等长的，但是机器翻译等问题，源语言句子的长度和目标语言的长度往往不同，所以需要将原始序列隐射为一个不同长度的序列，Encoder-Decoder模型就是解决这样的问题将 Encoder-Decoder 应用到 Image Caption任务中 只需要将原来的 Encoder RNN 换成 CNN ，为图像提取一个“视觉特征” 然后还是使用 Decoder 解码为输出序列即可 对Encoder-Decoder的改进 1 ：加入注意力机制两种注意力机制：一种叫Hard Attention ，一种叫 Soft Attention 对 Encoder-Decoder的改进 2 ：加入高层语义使用CNN在最终分类前将卷积特征作为图像语义，而最终的分类信息也就是高层语义其实与最终生成的语句非常相关，不能轻易舍弃 在 TensorFlow 中实现 Image Caption下载代码1git clone https://github.com/tensorflow/models.git 环境准备机器中没有Bazel的需要安装Bazel，这里以Ubuntu系统为例，其他系统可以参考其官方网站https://docs.bazel.build/versions/master/install.html 进行安装。 在Ubuntu 系统上安装Bazel，首先要添加Bazel 对应的源： 12echo &quot;deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8&quot; | sudo tee /etc/apt/sources.list.d/bazel.listcurl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add - apt-get安装： 1sudo apt-get update &amp;&amp; sudo apt-get install bazel 此外还需要安装nltk： 1pip install nltk 编译和数据准备编译源码： 123bazel build //im2txt:download_and_preprocess_mscocobazel build -c opt //im2txt/...bazel build -c opt //im2txt:run_inference 下载训练数据(请保证网络畅通，并确保至少有150GB 的硬盘空间可以使用)： 1bazel-bin/im2txt/download_and_preprocess_mscoco &quot;data/mscoco&quot; 下载http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz ，解压后得到inception_v3.ckpt。在data目录下新建一个pretrained目录，并将inception_v3.ckpt复制进去。 最后，在data 目录下新建model 文件夹。并在该目录下新建train 和eval 两个文件夹，这两个文件夹分别用来保存训练时的模型、日志和验证时的日志。最终，文件夹结构应该是： 12345678im2txt/ data/ mscoco/ pretrained/ inception_v3.ckpt model/ train/ eval/ 训练和验证训练： 123456bazel-bin/im2txt/train \ --input_file_pattern=&quot;data/mscoco/train-?????-of-00256&quot; \ --inception_checkpoint_file=&quot;data/pretrained/inception_v3.ckpt&quot; \ --train_dir=&quot;data/model/train&quot; \ --train_inception=false \ --number_of_steps=1000000 打开TensorBoard： 1tensorboard –logdir data/model/train 验证困惑度指标： 1234bazel-bin/im2txt/evaluate \ --input_file_pattern=&quot;data/mscoco/val-?????-of-00004&quot; \ --checkpoint_dir=&quot;data/model/train&quot; \ --eval_dir=&quot;data/model/eval&quot; 打开TensorBoard 观察验证数据集上困惑度的变化： 1tensorboard --logdir data/model/eval 测试单张图片 1234bazel-bin/im2txt/run_inference \ --checkpoint_path=data/model/train \ --vocab_file=data/mscoco/word_counts.txt \ --input_files=data/test.jpg]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[16.21个项目玩转深度学习之神经网络机器翻译技术]]></title>
    <url>%2Ftensorflow%2F16.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%8A%80%E6%9C%AF.html</url>
    <content type="text"><![CDATA[Encoder-Decoder模型的原理一般也称作 Seq2Seq 模型 ,机器翻译问题 ，原语言和目标语言的句子往往并没有相同的长度 注意力机制在机器翻译问题中 ， 当被翻译的句子较长时，一个 c 可能无法存储如此多的信息，翻译精度会下降 注意力机制 Attention 通过在每个时间输入不同的 c 来解决这个问题 使用TensorFlow NMT 搭建神经网络翻译引擎示例：将越南语翻译为英语首先下载 TensorFlow NMT 的代码1git clone https://github.com/tensorflow/nmt.git 下载一个越南语与英语的平行语料库1nmt/scripts/download_iwslt15.sh /tmp/nmt_data 训练模型1234567891011121314mkdir /tmp/nmt_modelpython -m nmt. nmt \--src=vi --tgt=en \--vocab_prefix=/tmp/nmt_data/vocab \--train_prefix=/tmp/nmt_data/train \--dev_prefix=/tmp/nmt_data/tst2012 \--test_prefix=/tmp/nmt_data/tst2013 \--out_dir=/tmp/nmt model \--num_train_steps=l2000 \--steps_per_stats=lOO \--num_layers=2 \--num_units=128 \--dropout=0.2 \--metrics=bleu]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[15.21个项目玩转深度学习之在Tensorflow中进行时间序列预测]]></title>
    <url>%2Ftensorflow%2F15.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9C%A8Tensorflow%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B.html</url>
    <content type="text"><![CDATA[在TensorFlow 中进行时间序列预测从Numpy 数组中读入时间序列数据1python test_input_array.py从CSV 文件中读入时间序列数据1python test_input_csv.pyAR 模型的训练1python train_array.pyLSTM 模型中的单变量时间序列预测 1python train_lstm.py LSTM 模型中的多变量时间序列预测1python train_lstm_multivariate.py test_input_array.py1234567891011121314151617181920212223242526272829303132333435363738394041# coding: utf-8from __future__ import print_functionimport numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import NumpyReaderx = np.array(range(1000))noise = np.random.uniform(-0.2, 0.2, 1000)y = np.sin(np.pi * x / 100) + x / 200. + noiseplt.plot(x, y)plt.savefig('timeseries_y.jpg')data = &#123; tf.contrib.timeseries.TrainEvalFeatures.TIMES: x, tf.contrib.timeseries.TrainEvalFeatures.VALUES: y,&#125;reader = NumpyReader(data)with tf.Session() as sess: full_data = reader.read_full() coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) print(sess.run(full_data)) coord.request_stop()train_input_fn = tf.contrib.timeseries.RandomWindowInputFn( reader, batch_size=2, window_size=10)with tf.Session() as sess: batch_data = train_input_fn.create_batch() coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) one_batch = sess.run(batch_data[0]) coord.request_stop()print('one_batch_data:', one_batch) test_input_csv.py12345678910111213141516171819202122232425# coding: utf-8from __future__ import print_functionimport tensorflow as tfcsv_file_name = './data/period_trend.csv'reader = tf.contrib.timeseries.CSVReader(csv_file_name)with tf.Session() as sess: data = reader.read_full() coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) print(sess.run(data)) coord.request_stop()train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=16)with tf.Session() as sess: data = train_input_fn.create_batch() coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(sess=sess, coord=coord) batch1 = sess.run(data[0]) batch2 = sess.run(data[0]) coord.request_stop()print('batch1:', batch1)print('batch2:', batch2) train_array.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# coding: utf-8from __future__ import print_functionimport numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import NumpyReaderdef main(_): x = np.array(range(1000)) noise = np.random.uniform(-0.2, 0.2, 1000) y = np.sin(np.pi * x / 100) + x / 200. + noise plt.plot(x, y) plt.savefig('timeseries_y.jpg') data = &#123; tf.contrib.timeseries.TrainEvalFeatures.TIMES: x, tf.contrib.timeseries.TrainEvalFeatures.VALUES: y, &#125; reader = NumpyReader(data) train_input_fn = tf.contrib.timeseries.RandomWindowInputFn( reader, batch_size=16, window_size=40) ar = tf.contrib.timeseries.ARRegressor( periodicities=200, input_window_size=30, output_window_size=10, num_features=1, loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS) ar.train(input_fn=train_input_fn, steps=6000) evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader) # keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step'] evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1) (predictions,) = tuple(ar.predict( input_fn=tf.contrib.timeseries.predict_continuation_input_fn( evaluation, steps=250))) plt.figure(figsize=(15, 5)) plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin') plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation') plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction') plt.xlabel('time_step') plt.ylabel('values') plt.legend(loc=4) plt.savefig('predict_result.jpg')if __name__ == '__main__': tf.logging.set_verbosity(tf.logging.INFO) tf.app.run() train_csv.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# coding: utf-8from __future__ import print_functionimport numpy as npimport matplotlibmatplotlib.use('agg')import matplotlib.pyplot as pltimport tensorflow as tfdef main(_): csv_file_name = './data/period_trend.csv' reader = tf.contrib.timeseries.CSVReader(csv_file_name) train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=16, window_size=16) with tf.Session() as sess: data = reader.read_full() coord = tf.train.Coordinator() tf.train.start_queue_runners(sess=sess, coord=coord) data = sess.run(data) coord.request_stop() ar = tf.contrib.timeseries.ARRegressor( periodicities=100, input_window_size=10, output_window_size=6, num_features=1, loss=tf.contrib.timeseries.ARModel.NORMAL_LIKELIHOOD_LOSS) ar.train(input_fn=train_input_fn, steps=1000) evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader) # keys of evaluation: ['covariance', 'loss', 'mean', 'observed', 'start_tuple', 'times', 'global_step'] evaluation = ar.evaluate(input_fn=evaluation_input_fn, steps=1) (predictions,) = tuple(ar.predict( input_fn=tf.contrib.timeseries.predict_continuation_input_fn( evaluation, steps=250))) plt.figure(figsize=(15, 5)) plt.plot(data['times'].reshape(-1), data['values'].reshape(-1), label='origin') plt.plot(evaluation['times'].reshape(-1), evaluation['mean'].reshape(-1), label='evaluation') plt.plot(predictions['times'].reshape(-1), predictions['mean'].reshape(-1), label='prediction') plt.xlabel('time_step') plt.ylabel('values') plt.legend(loc=4) plt.savefig('predict_result.jpg')if __name__ == '__main__': tf.logging.set_verbosity(tf.logging.INFO) tf.app.run() train_lstm.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199# Copyright 2017 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom os import pathimport numpy as npimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimatorsfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_modelfrom tensorflow.contrib.timeseries.python.timeseries import NumpyReaderimport matplotlibmatplotlib.use("agg")import matplotlib.pyplot as pltclass _LSTMModel(ts_model.SequentialTimeSeriesModel): """A time series model-building example using an RNNCell.""" def __init__(self, num_units, num_features, dtype=tf.float32): """Initialize/configure the model object. Note that we do not start graph building here. Rather, this object is a configurable factory for TensorFlow graphs which are run by an Estimator. Args: num_units: The number of units in the model's LSTMCell. num_features: The dimensionality of the time series (features per timestep). dtype: The floating point data type to use. """ super(_LSTMModel, self).__init__( # Pre-register the metrics we'll be outputting (just a mean here). train_output_names=["mean"], predict_output_names=["mean"], num_features=num_features, dtype=dtype) self._num_units = num_units # Filled in by initialize_graph() self._lstm_cell = None self._lstm_cell_run = None self._predict_from_lstm_output = None def initialize_graph(self, input_statistics): """Save templates for components, which can then be used repeatedly. This method is called every time a new graph is created. It's safe to start adding ops to the current default graph here, but the graph should be constructed from scratch. Args: input_statistics: A math_utils.InputStatistics object. """ super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics) self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units) # Create templates so we don't have to worry about variable reuse. self._lstm_cell_run = tf.make_template( name_="lstm_cell", func_=self._lstm_cell, create_scope_now_=True) # Transforms LSTM output into mean predictions. self._predict_from_lstm_output = tf.make_template( name_="predict_from_lstm_output", func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features), create_scope_now_=True) def get_start_state(self): """Return initial state for the time series model.""" return ( # Keeps track of the time associated with this state for error checking. tf.zeros([], dtype=tf.int64), # The previous observation or prediction. tf.zeros([self.num_features], dtype=self.dtype), # The state of the RNNCell (batch dimension removed since this parent # class will broadcast). [tf.squeeze(state_element, axis=0) for state_element in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)]) def _transform(self, data): """Normalize data based on input statistics to encourage stable training.""" mean, variance = self._input_statistics.overall_feature_moments return (data - mean) / variance def _de_transform(self, data): """Transform data back to the input scale.""" mean, variance = self._input_statistics.overall_feature_moments return data * variance + mean def _filtering_step(self, current_times, current_values, state, predictions): """Update model state based on observations. Note that we don't do much here aside from computing a loss. In this case it's easier to update the RNN state in _prediction_step, since that covers running the RNN both on observations (from this method) and our own predictions. This distinction can be important for probabilistic models, where repeatedly predicting without filtering should lead to low-confidence predictions. Args: current_times: A [batch size] integer Tensor. current_values: A [batch size, self.num_features] floating point Tensor with new observations. state: The model's state tuple. predictions: The output of the previous `_prediction_step`. Returns: A tuple of new state and a predictions dictionary updated to include a loss (note that we could also return other measures of goodness of fit, although only "loss" will be optimized). """ state_from_time, prediction, lstm_state = state with tf.control_dependencies( [tf.assert_equal(current_times, state_from_time)]): transformed_values = self._transform(current_values) # Use mean squared error across features for the loss. predictions["loss"] = tf.reduce_mean( (prediction - transformed_values) ** 2, axis=-1) # Keep track of the new observation in model state. It won't be run # through the LSTM until the next _imputation_step. new_state_tuple = (current_times, transformed_values, lstm_state) return (new_state_tuple, predictions) def _prediction_step(self, current_times, state): """Advance the RNN state using a previous observation or prediction.""" _, previous_observation_or_prediction, lstm_state = state lstm_output, new_lstm_state = self._lstm_cell_run( inputs=previous_observation_or_prediction, state=lstm_state) next_prediction = self._predict_from_lstm_output(lstm_output) new_state_tuple = (current_times, next_prediction, new_lstm_state) return new_state_tuple, &#123;"mean": self._de_transform(next_prediction)&#125; def _imputation_step(self, current_times, state): """Advance model state across a gap.""" # Does not do anything special if we're jumping across a gap. More advanced # models, especially probabilistic ones, would want a special case that # depends on the gap size. return state def _exogenous_input_step( self, current_times, current_exogenous_regressors, state): """Update model state based on exogenous regressors.""" raise NotImplementedError( "Exogenous inputs are not implemented for this example.")if __name__ == '__main__': tf.logging.set_verbosity(tf.logging.INFO) x = np.array(range(1000)) noise = np.random.uniform(-0.2, 0.2, 1000) y = np.sin(np.pi * x / 50 ) + np.cos(np.pi * x / 50) + np.sin(np.pi * x / 25) + noise data = &#123; tf.contrib.timeseries.TrainEvalFeatures.TIMES: x, tf.contrib.timeseries.TrainEvalFeatures.VALUES: y, &#125; reader = NumpyReader(data) train_input_fn = tf.contrib.timeseries.RandomWindowInputFn( reader, batch_size=4, window_size=100) estimator = ts_estimators.TimeSeriesRegressor( model=_LSTMModel(num_features=1, num_units=128), optimizer=tf.train.AdamOptimizer(0.001)) estimator.train(input_fn=train_input_fn, steps=2000) evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader) evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1) # Predict starting after the evaluation (predictions,) = tuple(estimator.predict( input_fn=tf.contrib.timeseries.predict_continuation_input_fn( evaluation, steps=200))) observed_times = evaluation["times"][0] observed = evaluation["observed"][0, :, :] evaluated_times = evaluation["times"][0] evaluated = evaluation["mean"][0] predicted_times = predictions['times'] predicted = predictions["mean"] plt.figure(figsize=(15, 5)) plt.axvline(999, linestyle="dotted", linewidth=4, color='r') observed_lines = plt.plot(observed_times, observed, label="observation", color="k") evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g") predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r") plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]], loc="upper left") plt.savefig('predict_result.jpg') train_lstm_multivariate.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190# Copyright 2017 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom os import pathimport numpyimport tensorflow as tffrom tensorflow.contrib.timeseries.python.timeseries import estimators as ts_estimatorsfrom tensorflow.contrib.timeseries.python.timeseries import model as ts_modelimport matplotlibmatplotlib.use("agg")import matplotlib.pyplot as pltclass _LSTMModel(ts_model.SequentialTimeSeriesModel): """A time series model-building example using an RNNCell.""" def __init__(self, num_units, num_features, dtype=tf.float32): """Initialize/configure the model object. Note that we do not start graph building here. Rather, this object is a configurable factory for TensorFlow graphs which are run by an Estimator. Args: num_units: The number of units in the model's LSTMCell. num_features: The dimensionality of the time series (features per timestep). dtype: The floating point data type to use. """ super(_LSTMModel, self).__init__( # Pre-register the metrics we'll be outputting (just a mean here). train_output_names=["mean"], predict_output_names=["mean"], num_features=num_features, dtype=dtype) self._num_units = num_units # Filled in by initialize_graph() self._lstm_cell = None self._lstm_cell_run = None self._predict_from_lstm_output = None def initialize_graph(self, input_statistics): """Save templates for components, which can then be used repeatedly. This method is called every time a new graph is created. It's safe to start adding ops to the current default graph here, but the graph should be constructed from scratch. Args: input_statistics: A math_utils.InputStatistics object. """ super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics) self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units) # Create templates so we don't have to worry about variable reuse. self._lstm_cell_run = tf.make_template( name_="lstm_cell", func_=self._lstm_cell, create_scope_now_=True) # Transforms LSTM output into mean predictions. self._predict_from_lstm_output = tf.make_template( name_="predict_from_lstm_output", func_=lambda inputs: tf.layers.dense(inputs=inputs, units=self.num_features), create_scope_now_=True) def get_start_state(self): """Return initial state for the time series model.""" return ( # Keeps track of the time associated with this state for error checking. tf.zeros([], dtype=tf.int64), # The previous observation or prediction. tf.zeros([self.num_features], dtype=self.dtype), # The state of the RNNCell (batch dimension removed since this parent # class will broadcast). [tf.squeeze(state_element, axis=0) for state_element in self._lstm_cell.zero_state(batch_size=1, dtype=self.dtype)]) def _transform(self, data): """Normalize data based on input statistics to encourage stable training.""" mean, variance = self._input_statistics.overall_feature_moments return (data - mean) / variance def _de_transform(self, data): """Transform data back to the input scale.""" mean, variance = self._input_statistics.overall_feature_moments return data * variance + mean def _filtering_step(self, current_times, current_values, state, predictions): """Update model state based on observations. Note that we don't do much here aside from computing a loss. In this case it's easier to update the RNN state in _prediction_step, since that covers running the RNN both on observations (from this method) and our own predictions. This distinction can be important for probabilistic models, where repeatedly predicting without filtering should lead to low-confidence predictions. Args: current_times: A [batch size] integer Tensor. current_values: A [batch size, self.num_features] floating point Tensor with new observations. state: The model's state tuple. predictions: The output of the previous `_prediction_step`. Returns: A tuple of new state and a predictions dictionary updated to include a loss (note that we could also return other measures of goodness of fit, although only "loss" will be optimized). """ state_from_time, prediction, lstm_state = state with tf.control_dependencies( [tf.assert_equal(current_times, state_from_time)]): transformed_values = self._transform(current_values) # Use mean squared error across features for the loss. predictions["loss"] = tf.reduce_mean( (prediction - transformed_values) ** 2, axis=-1) # Keep track of the new observation in model state. It won't be run # through the LSTM until the next _imputation_step. new_state_tuple = (current_times, transformed_values, lstm_state) return (new_state_tuple, predictions) def _prediction_step(self, current_times, state): """Advance the RNN state using a previous observation or prediction.""" _, previous_observation_or_prediction, lstm_state = state lstm_output, new_lstm_state = self._lstm_cell_run( inputs=previous_observation_or_prediction, state=lstm_state) next_prediction = self._predict_from_lstm_output(lstm_output) new_state_tuple = (current_times, next_prediction, new_lstm_state) return new_state_tuple, &#123;"mean": self._de_transform(next_prediction)&#125; def _imputation_step(self, current_times, state): """Advance model state across a gap.""" # Does not do anything special if we're jumping across a gap. More advanced # models, especially probabilistic ones, would want a special case that # depends on the gap size. return state def _exogenous_input_step( self, current_times, current_exogenous_regressors, state): """Update model state based on exogenous regressors.""" raise NotImplementedError( "Exogenous inputs are not implemented for this example.")if __name__ == '__main__': tf.logging.set_verbosity(tf.logging.INFO) csv_file_name = path.join("./data/multivariate_periods.csv") reader = tf.contrib.timeseries.CSVReader( csv_file_name, column_names=((tf.contrib.timeseries.TrainEvalFeatures.TIMES,) + (tf.contrib.timeseries.TrainEvalFeatures.VALUES,) * 5)) train_input_fn = tf.contrib.timeseries.RandomWindowInputFn( reader, batch_size=4, window_size=32) estimator = ts_estimators.TimeSeriesRegressor( model=_LSTMModel(num_features=5, num_units=128), optimizer=tf.train.AdamOptimizer(0.001)) estimator.train(input_fn=train_input_fn, steps=200) evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader) evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1) # Predict starting after the evaluation (predictions,) = tuple(estimator.predict( input_fn=tf.contrib.timeseries.predict_continuation_input_fn( evaluation, steps=100))) observed_times = evaluation["times"][0] observed = evaluation["observed"][0, :, :] evaluated_times = evaluation["times"][0] evaluated = evaluation["mean"][0] predicted_times = predictions['times'] predicted = predictions["mean"] plt.figure(figsize=(15, 5)) plt.axvline(99, linestyle="dotted", linewidth=4, color='r') observed_lines = plt.plot(observed_times, observed, label="observation", color="k") evaluated_lines = plt.plot(evaluated_times, evaluated, label="evaluation", color="g") predicted_lines = plt.plot(predicted_times, predicted, label="prediction", color="r") plt.legend(handles=[observed_lines[0], evaluated_lines[0], predicted_lines[0]], loc="upper left") plt.savefig('predict_result.jpg')]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[14.21个项目玩转深度学习之词的向量表示]]></title>
    <url>%2Ftensorflow%2F14.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA.html</url>
    <content type="text"><![CDATA[为什么需要做词嵌入实际应用中 ， 每一步只输入一个字母显然是不太合适的 ， 更加高效的方法是每一步输入一个单词，如果还继续使用独热表示 ，那么每一步输入的向量维数会非常大 ，独热表示实际上完全平等看待了单词表中的所有单词 ，忽略了单词之间的联系 词嵌入的原理如何学习到映射 关系？一般有两种方法： 一种方法是基于“计数” 的 ，大型语料库中 ， 计算一个词语和另一个词语同时出现的概率，将经常同时出现的词映射到向量空间的中相近位置 一种方法是基于“预测” 的，从一个词或几个词出发 ， 预测它们可能的相邻词，在预测过程中自然而然地学习到了词嵌入的映射 两种基于预测的方法，分别叫 CBOW 和 Skip-Gram CBOW 实现词嵌入的原理CBOW 的全称为 Continuous Bag of Words ，即连续词袋模型 ，它的核心思想是利用某个词语的上下文预测这个词语 通过优化二分类损失函数来训练模型后，最后得到的模型中的隐含层可以看作是 word2vec 中的“vec”向量 。 对于一个单词，先将独热表示输入模型，隐含层的值是对应的词嵌入表示 。 另外，在 TensorFlow 中，这里使用的损失被称为 NCE 损失，对应的函数为 tf.nn.nce loss 。 Skip-Gram 实现词嵌入的原理Skip-Gram 方法和 CBOW 方法正好相反：使用“出现的词”来预测 “上下文文中词 在损失的选择上，和 CBOW 一样，不使用 V 类分类的 Softmax 交叉摘损失， 而是取出一些“噪声词”，训练一个两类分类器（即同样使用 NCE损失）。 在 TensorFlow 中实现词嵌入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315# coding: utf-8# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Basic word2vec example."""# 导入一些需要的库from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport collectionsimport mathimport osimport randomimport zipfileimport numpy as npfrom six.moves import urllibfrom six.moves import xrange # pylint: disable=redefined-builtinimport tensorflow as tf# 第一步: 在下面这个地址下载语料库url = 'http://mattmahoney.net/dc/'def maybe_download(filename, expected_bytes): """ 这个函数的功能是： 如果filename不存在，就在上面的地址下载它。 如果filename存在，就跳过下载。 最终会检查文字的字节数是否和expected_bytes相同。 """ if not os.path.exists(filename): print('start downloading...') filename, _ = urllib.request.urlretrieve(url + filename, filename) statinfo = os.stat(filename) if statinfo.st_size == expected_bytes: print('Found and verified', filename) else: print(statinfo.st_size) raise Exception( 'Failed to verify ' + filename + '. Can you get to it with a browser?') return filename# 下载语料库text8.zip并验证下载filename = maybe_download('text8.zip', 31344016)# 将语料库解压，并转换成一个word的listdef read_data(filename): """ 这个函数的功能是： 将下载好的zip文件解压并读取为word的list """ with zipfile.ZipFile(filename) as f: data = tf.compat.as_str(f.read(f.namelist()[0])).split() return datavocabulary = read_data(filename)print('Data size', len(vocabulary)) # 总长度为1700万左右# 输出前100个词。print(vocabulary[0:100])# 第二步: 制作一个词表，将不常见的词变成一个UNK标识符# 词表的大小为5万（即我们只考虑最常出现的5万个词）vocabulary_size = 50000def build_dataset(words, n_words): """ 函数功能：将原始的单词表示变成index """ count = [['UNK', -1]] count.extend(collections.Counter(words).most_common(n_words - 1)) dictionary = dict() for word, _ in count: dictionary[word] = len(dictionary) data = list() unk_count = 0 for word in words: if word in dictionary: index = dictionary[word] else: index = 0 # UNK的index为0 unk_count += 1 data.append(index) count[0][1] = unk_count reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return data, count, dictionary, reversed_dictionarydata, count, dictionary, reverse_dictionary = build_dataset(vocabulary, vocabulary_size)del vocabulary # 删除已节省内存# 输出最常出现的5个单词print('Most common words (+UNK)', count[:5])# 输出转换后的数据库data，和原来的单词（前10个）print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])# 我们下面就使用data来制作训练集data_index = 0# 第三步：定义一个函数，用于生成skip-gram模型用的batchdef generate_batch(batch_size, num_skips, skip_window): # data_index相当于一个指针，初始为0 # 每次生成一个batch，data_index就会相应地往后推 global data_index assert batch_size % num_skips == 0 assert num_skips &lt;= 2 * skip_window batch = np.ndarray(shape=(batch_size), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) span = 2 * skip_window + 1 # [ skip_window target skip_window ] buffer = collections.deque(maxlen=span) # data_index是当前数据开始的位置 # 产生batch后就往后推1位（产生batch） for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) for i in range(batch_size // num_skips): # 利用buffer生成batch # buffer是一个长度为 2 * skip_window + 1长度的word list # 一个buffer生成num_skips个数的样本# print([reverse_dictionary[i] for i in buffer]) target = skip_window # target label at the center of the buffer# targets_to_avoid保证样本不重复 targets_to_avoid = [skip_window] for j in range(num_skips): while target in targets_to_avoid: target = random.randint(0, span - 1) targets_to_avoid.append(target) batch[i * num_skips + j] = buffer[skip_window] labels[i * num_skips + j, 0] = buffer[target] buffer.append(data[data_index]) # 每利用buffer生成num_skips个样本，data_index就向后推进一位 data_index = (data_index + 1) % len(data) data_index = (data_index + len(data) - span) % len(data) return batch, labels# 默认情况下skip_window=1, num_skips=2# 此时就是从连续的3(3 = skip_window*2 + 1)个词中生成2(num_skips)个样本。# 如连续的三个词['used', 'against', 'early']# 生成两个样本：against -&gt; used, against -&gt; earlybatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)for i in range(8): print(batch[i], reverse_dictionary[batch[i]], '-&gt;', labels[i, 0], reverse_dictionary[labels[i, 0]])# 第四步: 建立模型.batch_size = 128embedding_size = 128 # 词嵌入空间是128维的。即word2vec中的vec是一个128维的向量skip_window = 1 # skip_window参数和之前保持一致num_skips = 2 # num_skips参数和之前保持一致# 在训练过程中，会对模型进行验证 # 验证的方法就是找出和某个词最近的词。# 只对前valid_window的词进行验证，因为这些词最常出现valid_size = 16 # 每次验证16个词valid_window = 100 # 这16个词是在前100个最常见的词中选出来的valid_examples = np.random.choice(valid_window, valid_size, replace=False)# 构造损失时选取的噪声词的数量num_sampled = 64graph = tf.Graph()with graph.as_default(): # 输入的batch train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) # 用于验证的词 valid_dataset = tf.constant(valid_examples, dtype=tf.int32) # 下面采用的某些函数还没有gpu实现，所以我们只在cpu上定义模型 with tf.device('/cpu:0'): # 定义1个embeddings变量，相当于一行存储一个词的embedding embeddings = tf.Variable( tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # 利用embedding_lookup可以轻松得到一个batch内的所有的词嵌入 embed = tf.nn.embedding_lookup(embeddings, train_inputs) # 创建两个变量用于NCE Loss（即选取噪声词的二分类损失） nce_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocabulary_size])) # tf.nn.nce_loss会自动选取噪声词，并且形成损失。 # 随机选取num_sampled个噪声词 loss = tf.reduce_mean( tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels=train_labels, inputs=embed, num_sampled=num_sampled, num_classes=vocabulary_size)) # 得到loss后，我们就可以构造优化器了 optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss) # 计算词和词的相似度（用于验证） norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) normalized_embeddings = embeddings / norm # 找出和验证词的embedding并计算它们和所有单词的相似度 valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) similarity = tf.matmul( valid_embeddings, normalized_embeddings, transpose_b=True) # 变量初始化步骤 init = tf.global_variables_initializer()# 第五步：开始训练num_steps = 100001with tf.Session(graph=graph) as session: # 初始化变量 init.run() print('Initialized') average_loss = 0 for step in xrange(num_steps): batch_inputs, batch_labels = generate_batch( batch_size, num_skips, skip_window) feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125; # 优化一步 _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict) average_loss += loss_val if step % 2000 == 0: if step &gt; 0: average_loss /= 2000 # 2000个batch的平均损失 print('Average loss at step ', step, ': ', average_loss) average_loss = 0 # 每1万步，我们进行一次验证 if step % 10000 == 0: # sim是验证词与所有词之间的相似度 sim = similarity.eval() # 一共有valid_size个验证词 for i in xrange(valid_size): valid_word = reverse_dictionary[valid_examples[i]] top_k = 8 # 输出最相邻的8个词语 nearest = (-sim[i, :]).argsort()[1:top_k + 1] log_str = 'Nearest to %s:' % valid_word for k in xrange(top_k): close_word = reverse_dictionary[nearest[k]] log_str = '%s %s,' % (log_str, close_word) print(log_str) # final_embeddings是我们最后得到的embedding向量 # 它的形状是[vocabulary_size, embedding_size] # 每一行就代表着对应index词的词嵌入表示 final_embeddings = normalized_embeddings.eval()# Step 6: 可视化# 可视化的图片会保存为“tsne.png”def plot_with_labels(low_dim_embs, labels, filename='tsne.png'): assert low_dim_embs.shape[0] &gt;= len(labels), 'More labels than embeddings' plt.figure(figsize=(18, 18)) # in inches for i, label in enumerate(labels): x, y = low_dim_embs[i, :] plt.scatter(x, y) plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom') plt.savefig(filename)try: # pylint: disable=g-import-not-at-top from sklearn.manifold import TSNE import matplotlib matplotlib.use('agg') import matplotlib.pyplot as plt # 因为我们的embedding的大小为128维，没有办法直接可视化 # 所以我们用t-SNE方法进行降维 tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000) # 只画出500个词的位置 plot_only = 500 low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :]) labels = [reverse_dictionary[i] for i in xrange(plot_only)] plot_with_labels(low_dim_embs, labels)except ImportError: print('Please install sklearn, matplotlib, and scipy to show embeddings.')]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[13.21个项目玩转深度学习之序列分类问题详解]]></title>
    <url>%2Ftensorflow%2F13.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BA%8F%E5%88%97%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195# coding: utf-8from __future__ import print_functionimport tensorflow as tfimport randomimport numpy as np# 这个类用于产生序列样本class ToySequenceData(object): """ 生成序列数据。每个数量可能具有不同的长度。 一共生成下面两类数据 - 类别 0: 线性序列 (如 [0, 1, 2, 3,...]) - 类别 1: 完全随机的序列 (i.e. [1, 3, 10, 7,...]) 注意: max_seq_len是最大的序列长度。对于长度小于这个数值的序列，我们将会补0。 在送入RNN计算时，会借助sequence_length这个属性来进行相应长度的计算。 """ def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3, max_value=1000): self.data = [] self.labels = [] self.seqlen = [] for i in range(n_samples): # 序列的长度是随机的，在min_seq_len和max_seq_len之间。 len = random.randint(min_seq_len, max_seq_len) # self.seqlen用于存储所有的序列。 self.seqlen.append(len) # 以50%的概率，随机添加一个线性或随机的训练 if random.random() &lt; .5: # 生成一个线性序列 rand_start = random.randint(0, max_value - len) s = [[float(i)/max_value] for i in range(rand_start, rand_start + len)] # 长度不足max_seq_len的需要补0 s += [[0.] for i in range(max_seq_len - len)] self.data.append(s) # 线性序列的label是[1, 0]（因为我们一共只有两类） self.labels.append([1., 0.]) else: # 生成一个随机序列 s = [[float(random.randint(0, max_value))/max_value] for i in range(len)] # 长度不足max_seq_len的需要补0 s += [[0.] for i in range(max_seq_len - len)] self.data.append(s) self.labels.append([0., 1.]) self.batch_id = 0 def next(self, batch_size): """ 生成batch_size的样本。 如果使用完了所有样本，会重新从头开始。 """ if self.batch_id == len(self.data): self.batch_id = 0 batch_data = (self.data[self.batch_id:min(self.batch_id + batch_size, len(self.data))]) batch_labels = (self.labels[self.batch_id:min(self.batch_id + batch_size, len(self.data))]) batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id + batch_size, len(self.data))]) self.batch_id = min(self.batch_id + batch_size, len(self.data)) return batch_data, batch_labels, batch_seqlen# 这一部分只是测试一下如何使用上面定义的ToySequenceDatatmp = ToySequenceData()# 生成样本batch_data, batch_labels, batch_seqlen = tmp.next(32)# batch_data是序列数据，它是一个嵌套的list，形状为(batch_size, max_seq_len, 1)print(np.array(batch_data).shape) # (32, 20, 1)# 我们之前调用tmp.next(32)，因此一共有32个序列# 我们可以打出第一个序列print(batch_data[0])# batch_labels是label，它也是一个嵌套的list，形状为(batch_size, 2)# (batch_size, 2)中的“2”表示为两类分类print(np.array(batch_labels).shape) # (32, 2)# 我们可以打出第一个序列的labelprint(batch_labels[0])# batch_seqlen一个长度为batch_size的list，表示每个序列的实际长度print(np.array(batch_seqlen).shape) # (32,)# 我们可以打出第一个序列的长度print(batch_seqlen[0])# 运行的参数learning_rate = 0.01training_iters = 1000000batch_size = 128display_step = 10# 网络定义时的参数seq_max_len = 20 # 最大的序列长度n_hidden = 64 # 隐层的sizen_classes = 2 # 类别数trainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)# x为输入，y为输出# None的位置实际为batch_sizex = tf.placeholder("float", [None, seq_max_len, 1])y = tf.placeholder("float", [None, n_classes])# 这个placeholder存储了输入的x中，每个序列的实际长度seqlen = tf.placeholder(tf.int32, [None])# weights和bias在输出时会用到weights = &#123; 'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))&#125;biases = &#123; 'out': tf.Variable(tf.random_normal([n_classes]))&#125;def dynamicRNN(x, seqlen, weights, biases): # 输入x的形状： (batch_size, max_seq_len, n_input) # 输入seqlen的形状：(batch_size, ) # 定义一个lstm_cell，隐层的大小为n_hidden（之前的参数） lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden) # 使用tf.nn.dynamic_rnn展开时间维度 # 此外sequence_length=seqlen也很重要，它告诉TensorFlow每一个序列应该运行多少步 outputs, states = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen) # outputs的形状为(batch_size, max_seq_len, n_hidden) # 如果有疑问可以参考上一章内容 # 我们希望的是取出与序列长度相对应的输出。如一个序列长度为10，我们就应该取出第10个输出 # 但是TensorFlow不支持直接对outputs进行索引，因此我们用下面的方法来做： batch_size = tf.shape(outputs)[0] # 得到每一个序列真正的index index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1) outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index) # 给最后的输出 return tf.matmul(outputs, weights['out']) + biases['out']# 这里的pred是logits而不是概率pred = dynamicRNN(x, seqlen, weights, biases)# 因为pred是logits，因此用tf.nn.softmax_cross_entropy_with_logits来定义损失cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)# 分类准确率correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))# 初始化init = tf.global_variables_initializer()# 训练with tf.Session() as sess: sess.run(init) step = 1 while step * batch_size &lt; training_iters: batch_x, batch_y, batch_seqlen = trainset.next(batch_size) # 每run一次就会更新一次参数 sess.run(optimizer, feed_dict=&#123;x: batch_x, y: batch_y, seqlen: batch_seqlen&#125;) if step % display_step == 0: # 在这个batch内计算准确度 acc = sess.run(accuracy, feed_dict=&#123;x: batch_x, y: batch_y, seqlen: batch_seqlen&#125;) # 在这个batch内计算损失 loss = sess.run(cost, feed_dict=&#123;x: batch_x, y: batch_y, seqlen: batch_seqlen&#125;) print("Iter " + str(step*batch_size) + ", Minibatch Loss= " + \ "&#123;:.6f&#125;".format(loss) + ", Training Accuracy= " + \ "&#123;:.5f&#125;".format(acc)) step += 1 print("Optimization Finished!") # 最终，我们在测试集上计算一次准确度 test_data = testset.data test_label = testset.labels test_seqlen = testset.seqlen print("Testing Accuracy:", \ sess.run(accuracy, feed_dict=&#123;x: test_data, y: test_label, seqlen: test_seqlen&#125;))]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[12.21个项目玩转深度学习之RNN基本结构与Char_RNN文本生成]]></title>
    <url>%2Ftensorflow%2F12.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BRNN%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84%E4%B8%8EChar_RNN%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90.html</url>
    <content type="text"><![CDATA[RNN基本结构与Char_RNN文本生成RNN原理RNN的英文全称是Recurrent Neural Networks，即循环神经网络，它是对序列型数据进行建模的深度模型。为了处理建模序列问题，RNN引入了隐状态的概念，隐状态可以对序列型的数据提取特征，接着再转换为输出简单地说就是，第一个数据激活+权重+偏置+初始化隐状态得到的结果作为第二个数据的隐状态，这样每个步骤的参数都是共享的，这是RNN的重要特点 N对1的RNN结构问题的输入是一个序列，输出是一个单独的值而不是序列，就在输出的时候对最后一个隐状态做运算即可 1对N的RNN结构输入不是序列而输出是序列的情况下，可以只在序列开始进行输入计算 LSTM的原理RNN的每一层的隐状态都是前一层的隐状态经过变换和激活函数得到，反向传播求导最终得到的导数会包含每一步的梯度的连乘，这会引起梯度爆炸或者梯度消失，所以RNN很难处理长程依赖的问题，LSTM在隐状态计算时以加法代替了这里的迭代变换，可以避免梯度消失的问题，使网络学到长程规律 Char RNN的原理使用的是RNN最经典的N vs N的模型，即输入是N的序列，输出是与之相等的序列，可以用来生成文章，诗歌甚至代码 英文可以使用独热编码来表示 汉字种类多，可能导致模型过大，优化办法： 取常用的N个汉字，剩下的汉字单独设为一类，并用一个特殊的字符进行标注 输入的时候加入一层embedding层，这个层可以将汉字转换为较为稠密的表示 TensorFlow 的 RNN 实现方式实现 RNN 的基本单元： RNNCell两个可以直接使用的子类，一个是 BasicRNNCell ，还有一个是 BasicLSTMCell 学习 RNNCell 要重点关注三个地方： 类方法 call 类属性 state size 类属性 output_size 定义一个基本 RNN 单元的方法为：1234import tensorflow as tfrnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128)print(rnn_cell.state_size) # state_size=128 定义一个基本 LSTM 单元的方法为：1234import tensorflow as tflstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)print(rnn_cell.state_size) # state_size=128 LSTM 可以看作h 和 C两个隐层 ,LSTM 基本单元的 state size 由两部分组成，一部分是 c ，另一部分是 h 12345678910import tensorflow as tfimport numpy as nplstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)inputs = tf.placeholder(np.float32,shape=(32,100)) # 32是batch_sizeh0 = lstm_cell.zero_state(32,np.float32) #通过zero_state得到一个全0的初始状态output,h1 = lstm_cell.call(inputs,h0)print(h1.h) # shape=(32,128)print(h1.c) # shape=(32,128) 对 RNN 进行堆叠： MultiRNNCell12345678910111213141516import tensorflow as tfimport numpy as np# 每调用一次这个函数返回一个BasicRNNCelldef get_a_cell(): return tf.nn.rnn_cell.BasicRNNCell(num_uints=128)# 用tf.nn.rnn_cell.MultiRNNCell 创建3层RNNcell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)])print(cell.state_size) # (128,128,128) 这里的意思是共有3个隐层状态，每个状态大小为128inputs = tf.placeholder(np.float32,shape=(32,100))h0 = cell.zero_state(32,np.float32)output , h1 = cell.call(inputs,h0)print(h1) BasicRNNCell 和 BasicLSTMCell 的 output在 BasicRNNCell 中， output其实和隐状态的值是一样的。因此，还需要额外对输出定义新的变换，才能得到图中真正的输出 y 使用 tf.nn.dynamic_rnn 展开时间维度设输入数据的格式为（batch_size, time steps, input_size）其中 batch_size 表示 batch 的大小，即一个 batch 中序列的个数 。 time_steps 表示序列本身的长度 ， 如在 CharRNN 中，长度为 10 的句子对应的 time_steps等于 10 便用 TensorFlow 实现 CharRNN定义输入数据定义多层 LSTM 模型定义损失训练模型与生成文字训练生成英文的模型：1234567python train.py \ --input_file data/shakespeare.txt \ --name shakespeare \ --num_steps 50 \ --num_seqs 32 \ --learning_rate 0.01 \ --max_steps 20000 测试模型：1234python sample.py \ --converter_path model/shakespeare/converter.pkl \ --checkpoint_path model/shakespeare/ \ --max_length 1000 训练写诗模型：12345678python train.py \ --use_embedding \ --input_file data/poetry.txt \ --name poetry \ --learning_rate 0.005 \ --num_steps 26 \ --num_seqs 32 \ --max_steps 10000 测试模型：12345python sample.py \ --use_embedding \ --converter_path model/poetry/converter.pkl \ --checkpoint_path model/poetry/ \ --max_length 300 训练生成C代码的模型：1234567python train.py \ --input_file data/linux.txt \ --num_steps 100 \ --name linux \ --learning_rate 0.01 \ --num_seqs 32 \ --max_steps 20000 测试模型：1234python sample.py \ --converter_path model/linux/converter.pkl \ --checkpoint_path model/linux \ --max_length 1000]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[11.21个项目玩转深度学习之CycleGAN与非配对图像转换]]></title>
    <url>%2Ftensorflow%2F11.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BCycleGAN%E4%B8%8E%E9%9D%9E%E9%85%8D%E5%AF%B9%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2.html</url>
    <content type="text"><![CDATA[CycleGAN与非配对图像转换下载数据集并训练下载一个事先准备好的数据集：1bash download_dataset.sh apple2orange将图片转换成tfrecords格式：12345python build_data.py \ --X_input_dir data/apple2orange/trainA \ --Y_input_dir data/apple2orange/trainB \ --X_output_file data/tfrecords/apple.tfrecords \ --Y_output_file data/tfrecords/orange.tfrecords 训练模型：1234python train.py \ --X data/tfrecords/apple.tfrecords \ --Y data/tfrecords/orange.tfrecords \ --image_size 256 打开TensorBoard(需要将–logdir checkpoints/20170715-1622 中的目录替换为自己机器中的对应目录)： 1tensorboard --logdir checkpoints/20170715-1622 导出模型(同样要注意将20170715-1622 替换为自己机器中的对应目录)： 12345python export_graph.py \ --checkpoint_dir checkpoints/20170715-1622 \ --XtoY_model apple2orange.pb \ --YtoX_model orange2apple.pb \ --image_size 256 使用测试集中的图片进行测试：12345python inference.py \ --model pretrained/apple2orange.pb \ --input data/apple2orange/testA/n07740461_1661.jpg \ --output data/apple2orange/output_sample.jpg \ --image_size 256 转换生成的图片保存在data/apple2orange/output_sample. jpg。 使用自己的数据进行训练在chapter_11_data/中，事先提供了一个数据集man2woman.zip。，解压后共包含两个文件夹：a_resized 和b_resized。将它们放到目录~/datasets/man2woman/下。使用下面的命令将数据集转换为tfrecords： 12345python build_data.py \ --X_input_dir ~/datasets/man2woman/a_resized/ \ --Y_input_dir ~/datasets/man2woman/b_resized/ \ --X_output_file ~/datasets/man2woman/man.tfrecords \ --Y_output_file ~/datasets/man2woman/woman.tfrecords 训练：1234python train.py \ --X ~/datasets/man2woman/man.tfrecords \ --Y ~/datasets/man2woman/woman.tfrecords \ --image_size 256]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[10.21个项目玩转深度学习之超分辨让图像变得更清晰]]></title>
    <url>%2Ftensorflow%2F10.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B6%85%E5%88%86%E8%BE%A8%E8%AE%A9%E5%9B%BE%E5%83%8F%E5%8F%98%E5%BE%97%E6%9B%B4%E6%B8%85%E6%99%B0.html</url>
    <content type="text"><![CDATA[让图像变得更清晰去除错误图片在地址下载COCO数据集，将所有图片文件放在目录~/datasets/super-resolution/mscoco下。使用chapter_10中的delete_broken_img.py脚本删除一些错误图像：1python delete_broken_img.py -p ~/datasets/super-resolution/mscoco/ 将图像裁剪到统一大小接着将图像缩放到统一大小： 1234python tools/process.py \ --input_dir ~/datasets/super-resolution/mscoco/ \ --operation resize \ --output_dir ~/datasets/super-resolution/mscoco/resized 为代码添加新的操作添加新的blur操作，然后对图像进行模糊处理： 123python tools/process.py --operation blur \ --input_dir ~/datasets/super-resolution/mscoco_resized/ \ --output_dir ~/datasets/super-resolution/mscoco_blur/ 合并图像：12345python tools/process.py \ --input_dir ~/datasets/super-resolution/mscoco_resized/ \ --b_dir ~/datasets/super-resolution/mscoco_blur/ \ --operation combine \ --output_dir ~/datasets/super-resolution/mscoco_combined/ 划分训练集和测试集：12python tools/split.py \ --dir ~/datasets/super-resolution/mscoco_combined/ 模型训练：12345python pix2pix.py --mode train \ --output_dir super_resolution \ --max_epochs 20 \ --input_dir ~/datasets/super-resolution/mscoco_combined/train \ --which_direction BtoA 模型测试：1234python pix2pix.py --mode test \--output_dir super_resolution_test \--input_dir ~/datasets/super-resolution/mscoco_combined/val \--checkpoint super_resolution/ 结果在super_resolution_test文件夹中。]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[9.21个项目玩转深度学习之pip2pix模型与自动上色技术]]></title>
    <url>%2Ftensorflow%2F9.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8Bpip2pix%E6%A8%A1%E5%9E%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E4%B8%8A%E8%89%B2%E6%8A%80%E6%9C%AF.html</url>
    <content type="text"><![CDATA[pip2pix模型与自动上色技术与原始GAN使用随机躁声生成样本不同， cGAN可以根据指定标签生成样本 。 接着会介绍 pix2pix模型，可以看作是 cGAN的一种特殊形式cGAN 的原理cGAN 的全称为 Conditional Generative Adversarial Networks ， 即条件对抗生成网络，它为生成器、判别器都额外加入了一个条件，这个条件实际是希望生成的标签 cGAN 的输入输出为： 生成器 G ， 输入一个噪声 z ， 一个条件 y，输出符合该条件的图像 G(z|y) 判别器 D ，输入一张图像x，一个条件 y，输出该图像在该条件下的真实概率 D(x|y） pix2pix 模型的原理都知道所谓的“机器翻译’比如将一段中文翻译成英文 。在图像领域也有类似的“图像翻译”（ Image-to-Image Translation ）问题： 将街景的标注图像变为真实照片 将建筑标注图像转换为照片 将卫星图像转换为地图 将白天的图片转换为夜晚的图片 将边缘轮廓线转段为真实物体 TensorFlow 的 pix2pix 模型执行已有的数据集代码文件： 文件 作用 pix2pix.py 项目的入口文件。训练模型和执行已有模型都要通过这个文件进行 tools/download-dataset.py 下载已有的几个数据集 tools/process.py 用于创建自己的数据集，将图片整理为给定格式 tools/split.py 将数据集随机分割为训练集和验证集 下载Facades 数据集:1python tools/download-dataset.py facades 训练模型：123456python pix2pix.py \ --mode train \ --output_dir facades_train \ --max_epochs 200 \ --input_dir facades/train \ --which_direction BtoA 测试模型：12345python pix2pix.py \ --mode test \ --output_dir facades_test \ --input_dir facades/val \ --checkpoint facades_train 创建自己的数据集将图片缩放到同样的大小1234python tools/process.py \ --input_dir photos/original \ --operation resize \ --output_dir photos/resized 转换图像合并12345678910python tools/process.py \ --input_dir photos/resized \ --operation blank \ --output_dir photos/blank \ python tools/process.py \ --input_dir photos/reiszed \ --b_dir photos/blank \ --operation combine \ --output_dir photos/combined 划分训练集和测试集：12python tools/split.py \ --dir ~/datasets/super-resolution/mscoco_combined 使用 TensorFlow 为灰度图像自动上色为食物图片上色解压数据food resized.zip 文件解压到目录～/datasets/colorlization/下 执行训练123456python pix2pix.py \ --mode train \ --output_dir colorlization_food \ --max_epochs 70 \ --input_dir ~/datasets/colorlization/food_resized/train \ --lab_colorization 执行测试12345python pix2pix.py \ --mode test \ --output_dir colorlization_food_test \ --input_dir ~/datasets/colorlization/food_resized/val \ --checkpoint colorlization_food 为动漫图片上色解压数据将动漫图像数据集 anime_reized.zip解压到~/datasets/colorlization/ 目录下 执行训练123456python pix2pix.py \ --mode train \ --output_dir colorlization_anime \ --max_epochs 5 \ --input_dir ~/datasets/colorlization/anime_resized/train \ --lab_colorization 执行测试12345python pix2pix.py \ --mode test \ --output_dir colorlization_anime_test \ --input_dir ~/datasets/colorlization/anime_resized/val \ --checkpoint colorlization_anime]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[8.21个项目玩转深度学习之GAN与DCGAN入门]]></title>
    <url>%2Ftensorflow%2F8.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BGAN%E4%B8%8EDCGAN%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[GAN与DCGAN入门GAN 的全称为 Generative Adversarial Networks ，意为对抗生成网络，一旦训练完成后可以生成全新的数据样本，DCGAN 将 GAN 的概念扩展到卷积神经网络中，可以生成质量较高的图片样本GAN 的原理假设有两个网络 ， 生成网络 G( Generator ）和判别网络Ｄ( Discriminator ） G 负责生成图片 ，接收一个随机的噪声 D 负责判别一张图片是不是“真实的” DCGAN 的原理DCGAN 的全称是 Deep Convolutional Generative Adversarial Networks ,意即深度卷积对抗生成网络 G 、D 还有一些其他的实现细节： 不采用任何池化层（ Pooling Layer ），在判别器 D 中，用带有步长（ Stride)的卷积来代替池化层。 在 G 、D 中均使用 Batch Normalization 帮助模型收敛 。 在 G 中，激活函数除了最后一层都使用 ReLU 函数，而最后一层使用 tanh函数。使用 tanh 函数的原因在于最后一层要输出图像 ，而图像的像素值是高一个取值范围的，如 0～255 。 ReLU 函数的输出可能会很大，而 tanh函数的输出是在-1～1 之间的，只要将 tanh 函数的输出加 l 再乘以 127.5可以得到 0～255 的像素值。 生成MNIST图像下载MNIST数据集：1python download.py mnist 训练：1python main.py --dataset mnist --input_height=28 --output_height=28 --train 生成图像保存在samples文件夹中。 使用自己的数据集训练在数据目录chapter_8_data/中已经准备好了一个动漫人物头像数据集faces.zip。在源代码的data目录中再新建一个anime目录（如果没有data目录可以自行新建），并将faces.zip中所有的图像文件解压到anime目录中。 训练命令：1234567python main.py --input_height 96 --input_width 96 \ --output_height 48 --output_width 48 \ --dataset anime --crop -–train \ --epoch 300 --input_fname_pattern "*.jpg" 生成图像保存在samples文件夹中。 依赖一个名为moviepy的库 。 可以使用 pip install moviepy安装 相比一般的神经网络，训练GAN 往往会更加困难。Github 用户 Soumith Chintala 收集了一份训练GAN 的技巧清单]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[7.21个项目玩转深度学习之图像风格迁移]]></title>
    <url>%2Ftensorflow%2F7.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB.html</url>
    <content type="text"><![CDATA[图像风格迁移VGGNet 的本意是输入图像 ， 提取特征，并输出图像类别 。 图像风格迁移正好与它相反输入的是特征，输出对应这种特征的图片 利用内容损失还原图像内容 利用风格损失还原图像风格 原始图像风格迁移和快速图像风格迁移的比较 类型 损失定义 是否需要训练新网络 生成图像的方法 原始图像风格迁移 组合内容损失Ｌ_content与风格损失Ｌ_style 否。只需要预训练好的VGGNet 利用损失，通过梯度下降法计算适合的图像 快速图像风格迁移 组合内容损失Ｌ_content与风格损失Ｌ_style 是。除了预训练好的VGGNet，还需要训练图像生成网络 利用训练好的图像生成网络直接生成 在 TensorFlow 中实现快速凤格迁移快速图像风格迁移的项目结构 文件／文件夹名 用 途 conf/ 文件夹中训练模型时的配置文件。每个生成模型都对应一个配置文件，配置文件中包含原始图像风格图像的位置、各个损失的平衡项、最大训练步数等 img/ 保存了一些原始图像风格图像。也有一些用于测试的图像 nets/preprocessing/ 复制自 TensorFlow Slim 项目的原始文件。 TensorFlow Slim 中定义了一些lmageNet 预训练模型，在本项目中主要使用其中的 VGGI6 模型 eval.py 用于使用已经训练好的模型进行图像的快速风格转移。 提供 7 个预训练的模型 losses.py 用于定义风格损失、内容损失 model.py 用于定义图像生成网络。 reader.py IO 接口 。将训练图片读入 TensorFlow train.py 用于训练模型 utils.py 定义了其他一些方便使用的函数 export.py 将模型导出为.pb 文件。初学者可以不用关注 export.py 的用途。 README.md 说明文件 使用预训练模型在chapter_7_data/ 中提供了7 个预训练模型： wave.ckpt-done、cubist.ckpt-done、denoised_starry.ckpt-done、mosaic.ckpt-done、scream.ckpt-done、feathers.ckpt-done。 以wave.ckpt-done的为例，在chapter_7/中新建一个models文件夹， 然后wave.ckpt-done复制到这个文件夹下，运行命令： 1python eval.py --model_file models/wave.ckpt-done --image_file img/test.jpg 成功风格化的图像会被写到generated/res.jpg。 训练自己的模型准备工作：下载VGG16模型将下载到的压缩包解压后会得到一个vgg16.ckpt 文件。在chapter_7/中新建一个文件夹pretrained，并将vgg16.ckpt复制到pretrained文件夹中。最后的文件路径是pretrained/vgg16.ckpt。这个vgg16.ckpt文件在chapter_7_data/中也有提供。 下载COCO数据集将该数据集解压后会得到一个train2014文件夹，其中应该含有大量jpg格式的图片。在chapter_7中建立到这个文件夹的符号链接： 1ln –s &lt;到train2014 文件夹的路径&gt; train2014 ln –s &lt;到train2014 文件夹的路径&gt; train2014 训练wave模型：1python train.py -c conf/wave.yml 打开TensorBoard：1tensorboard --logdir models/wave/ 训练中保存的模型在文件夹models/wave/中。]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[6.21个项目玩转深度学习之人脸检测与人脸识别]]></title>
    <url>%2Ftensorflow%2F6.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E4%B8%8E%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB.html</url>
    <content type="text"><![CDATA[人脸检测与人脸识别MTCNN 的原理人脸检测在图片中找到人脸的位置，在这个过程中 ，系统的输入是一张可能含有人脸的图片，输出是人脸位置的矩形框人脸对齐（ Face Alignment ）原始图片中人脸的姿态 、位置可能有较大的区别，为了之后统一处理，要把人脸“摆正”需要检测人脸中关键点（landmark)，如眼睛位置，鼻子位置，嘴巴位置，脸的轮廓点等，根据关键点使用仿射变换将人脸统一校准，消除姿势不同带来的误差 MT 是英文单词 Multi-task的简写，意即这种方法可以同时完成人脸检测和人脸对齐两项任务 MTCNN 由三个神经网络组成: P-Net R-Net O-Net P-Net输入：宽和高皆为 12 像素，同时是 3 通道的 RGB 图像 作用：该网络要判断这个图像中是否含有人脸，并且给出人脸框和关键点的位置 输出： 第一个部分判断图像是否是人脸 第二个部分给出框的精确位置 第三个部分给出人脸５个关键点位置 R-Net输入：宽和高皆为 24 像素，同时是 3 通道的 RGB 图像 作用：R-Net 的输出和P-Net 完全一样，同样由人脸判别 、框回归 、关键点位置预测三部分组成 Ｏ-Net输入：宽和高皆为 48 像素，同时是 3 通道的 RGB 图像 作用：一样的 其实原理简单来说就是先用简单的粗筛选一遍，再针对性得放大区域进行预测 MTCNN的损失定义损失由三部分组成， 针对人脸判别部分，直接使用交叉熵损失，针对框回归相关键点判定，直接使用 L2 损失 。 最后这三部分损失各自乘以自身的权重再加起来，就形成最后的总损失了 MTCNN训练过程在训练 P-Net 和 R-Net时更关心框位置的准确性，而较少关注关键点判定的损失 ， 因此关键点判定损失的权重很小 。对于 O-Net ，关键点判定损失的权重较大。 使用深度卷积网络提取特征人脸识别这一步一般是使用深度卷积网络 ，将输入的人脸图像转换成一个向量的表示也就是所谓的 “特征” 三元组损失（ Triplet Loss ）三元组损失直接对距离进行优化，因此可以解决人脸的特征表示问题 。但是在训练过程中，三元组的选择非常地有技巧性，使用三元组损失训练人脸模型通常还需要非常大的人脸数据集，才能取得较好的效果。 中心损失（ Center Loss )不直接对距离进行优化，保留了原有的分类模型，但又为每个类（在人脸模型中，一个类就对应一个人）指定了一个类别中心。同一类的图像对应的特征都应该尽量靠近自己的类别中心，不同类的类别中心尽量远离，利用较少的图像就可以达到与三元组损失相似的效果 在 TensorFlow 中实现人脸识别LFW 人脸数据库[下载 LFW 数据库](http ://vis-www.cs.umass.edu/lfw/lfw.tgz) LFW 数据库上的人脸检测和对齐对LFW进行人脸检测和对齐： 12345python src/align/align_dataset_mtcnn.py \ ~/datasets/lfw/raw \ ~/datasets/lfw/lfw_mtcnnpy_160 \ --image_size 160 --margin 32 \ --random_order 使用已有模型验证LFW 数据库准确率在百度网盘的chapter_6_data/目录或者地址 下载解压得到4个模型文件夹，将它们拷贝到~/models/facenet/20170512-110547/中。 之后运行代码： 123python src/validate_on_lfw.py \ ~/datasets/lfw/lfw_mtcnnpy_160 \ ~/models/facenet/20170512-110547/ 即可验证该模型在已经裁剪好的lfw数据集上的准确率。 在自己的数据上使用已有模型计算人脸两两之间的距离： 123python src/compare.py \ ~/models/facenet/20170512-110547/ \ ./test_imgs/1.jpg ./test_imgs/2.jpg ./test_imgs/3.jpg 重新训练新模型以CASIA-WebFace数据集为例，读者需自行申请该数据集，申请地址。获得CASIA-WebFace数据集后，将它解压到~/datasets/casia/raw目录中。此时文件夹~/datasets/casia/raw/中的数据结构应该类似于： 12345678910110000045 001.jpg 002.jpg 003.jpg ……0000099 001.jpg 002.jpg 003.jpg ………… 先用MTCNN进行检测和对齐： 1234python src/align/align_dataset_mtcnn.py \ ~/datasets/casia/raw/ \ ~/datasets/casia/casia_maxpy_mtcnnpy_182 \ --image_size 182 --margin 44 再进行训练： 1234567891011121314151617python src/train_softmax.py \ --logs_base_dir ~/logs/facenet/ \ --models_base_dir ~/models/facenet/ \ --data_dir ~/datasets/casia/casia_maxpy_mtcnnpy_182 \ --image_size 160 \ --model_def models.inception_resnet_v1 \ --lfw_dir ~/datasets/lfw/lfw_mtcnnpy_160 \ --optimizer RMSPROP \ --learning_rate -1 \ --max_nrof_epochs 80 \ --keep_probability 0.8 \ --random_crop --random_flip \ --learning_rate_schedule_file data/learning_rate_schedule_classifier_casia.txt \ --weight_decay 5e-5 \ --center_loss_factor 1e-2 \ --center_loss_alfa 0.9 打开TensorBoard的命令(&lt;开始训练时间&gt;需要进行替换)： 1tensorboard --logdir ~/logs/facenet/&lt;开始训练时间&gt;/]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[5.21个项目玩转深度学习之深度学习中的目标检测]]></title>
    <url>%2Ftensorflow%2F5.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.html</url>
    <content type="text"><![CDATA[深度学习中的目标检测深度学习中目标检测的原理R-CNN 的原理R-CNN 的全称是 Region-CNN,传统的目标检测方法大多以图像识别为基础。 一般可以在图片上使用穷举法选出所有物体可能出现的区域框，对这些区域框提取特征并使用图像识别方法分类， 得到所有分类成功的区域后，通过非极大值抑制（Non-maximum suppression）输出结果 R-CNN 遵循传统目标检测的思路，只是把传统特征SIFT,HOG特征换成了深度卷积网络提取的特征 提取框,对每个框提取特征 图像分类 非极大值抑制 过程 对原始图像使用Selective Search搜寻可能存在的区域，相比穷举，减少了很多计算量 将取出的可能含有物体的区域送入 CNN 中提取特征 取出的区域大小却不同 。对此，R-CNＮ的做法是将区域缩放到统一大小 ， 再提取特征 取出特征后使用 SVM 进行分类 通过非极大值抑制输出结果 R-CNN的训练、可以分成下面四步： 在数据集上训练 CNN 在目标检测的数据集上，对训练好的 CNN做微调 用 Selective Search 搜索候选区域，统一使用微调后的 CNN对这些区域提取特征，将提取到的特征存储起来 使用存储起来的特征 ，训练 SVM 分类器 R-CNN缺点 通过 Selective Search 得到的有效区域往往在 1000 个以上，这意昧着要重复计算 1000 多次神经网络 ，非常耗时 在训练、阶段，还需要把所有特征保存起来 ，再通过 SVM进行训练，这也是非常耗时且麻烦的 SPPNet 的原理SPPNet 的英文全称是 Spatial Pyramid Pooling Convolutional Networks 翻译成中文是“空间金字塔池化卷积网络” SPPNet 主要做了一件事情：将 CNN 的输入从固定尺寸改进为任意尺寸。SPPNet 在普通的 CNN 结构中加入了 ROI 池化层（ ROI Pooling ），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。非常简单，就是按比例做池化操作 比较： Ｒ-CNN必须把框中的图像缩放到统一大小，再对每一张缩放后的图片提取特征，使用ROI池化层后，就可以先对图像进行一遍卷积计算，得到整个图像的卷积特征，对于原始图像的各种候选框就可以只在卷积特征中找到对应的位置框，再对位置框中的卷积提取特征 Ｒ-CNN要对每个区域计算卷积，而SPPNet只需要计算一次 它们都遵循提取候选框，提取特征，分类的步骤，都使用SVM Fast R-CNN 的原理Fast R-CNN 相比 SPPNet 更进一步，不再使用 SVM作为分类器，而是使用神经网络进行分类，这样就可以同时训练特征提取网络和分类网络，从而取得比 SPPNet 更高的准确度 SPPNet 是使用 SVM 对特征进行分类，而 Fast R-CNN 则是直接使用全连接层。全连接层高两个输出，一个输出负责分类即 Softmax ，另一个输出负责框回归 假设要在图像中检测 K 类物体，那么最终的输出应该是 K+I个数，还有一个背景类，针对该区域没有目标物体的情况 框回归实际上要做的是对原始的检测框进行某种程度的“校准”。因为使用 Selective Search 获得的框有时存在一定偏差 。 设通过Selective Search 得到的框的四个参数为$$(x,y,w,h)$$真正的框参数：$$（ｘ’,y’,w’,h’)$$框回归就是学习：$$(\frac{x’-x}{w},\frac{y’-y}{h},ln\frac{w’}{w},ln\frac{h’}{h})$$ $$\frac{x’-x}{w},\frac{y’-y}{h}:表示与尺度无关的平移量\ln\frac{w’}{w},ln\frac{h’}{h}:表示与尺度无关的缩放量$$ Faster R-CNN 的原理Fast R-CNN需要先使用 Selective Search 提取框，这个方法比较慢，有时 ，检测一张图片，大部分时间不是花在计算神经网络分类上，而是花在 Selective Search 提取框上，Faster R-CNN 中，用 RPN 网络( Region Proposal Network ）取代了 Selective Search ，不仅速度得到大大提高 ，而且还获得了更加精确的结果 。 训练过程： 先使用一个CNN对原始图片提取特征，设这个前置的CNN提取的特征为:$$51\times39\times256$$那么新的卷积特征一共有：$$51\times39$$个位置，每个位置都负责原图中对应位置９种尺寸的框的检测，检测的目标是判断框中是否存在一个物体，因此共有：$$51\times39\times9$$个框，论文中将这些框都统称为’anchor’,分别是：128,256,512,又分为三种长宽比，2:1,1:2,1:1 使用RPN生成候选框后，剩下的和Fast R-CNN一样 训练过程，需要训练两个网络，一个是RPN,一个是分类网络 通常是交替训练，即在一个batch,先训练RPN一次，再训练分类网络一次 对比表 项目 R-CNN Fast R-CNN Faster R-CNN 提取候选框 Selective Search Selective Search RPN网络 提取特征 卷积神经网络 卷积＋ROI池化 卷积＋ROI池化 特征分类 SVM 卷积＋ROI池化 卷积＋ROI池化 TensorFlow Object Detection API安装 TensorFlow Object Detection API1git clone https://github.com/tensorflow/models.git models文件夹中还高一个 research文件夹 。 下面的安装命令都是以 research文件夹为根目录执行的 安装 TensorFlow Object Detection API的步骤如下 （ 以 research 文件夹为相对目录）： 安装或升级 protoc，可以使用命令 protoc --version查看 protoc 的版本 。 如果发现版本低于 2 .6 .0 或运行命令错误，就需要安装或升级 protoc，[protobuf 的发布页面](https ://github.com/google/protobuf/releases) 1sudo cp bin/protoc /usr/bin/protoc 编译 proto 文件 12# From models/researchprotoc object_detection/protos/*.proto --python_out=. 如果每个 proto文件都生成了对应的以 py为后缀的 python 源代码，就说明编译成功了 将 Slim 加入 PYTHON PATH TensorFlow Object Detection API是以 Slim 为基础实现的，需要将 Slim的目录加入 PYTHONPATH 1export PYTHONPATH=$PYTHONPATH:'pwd':'pwd'/slim 检查设置 123pythonimport slim# 如果不报错，说明正确配置了 进行测试 1python object_detection/builders/model_builde_test.py 自动检查是否正确安装，如果出现Ran 7 tests in 0.019s ok，说明安装成功 执行已经训练好的模型默认提供了 5 个预训练模型，都是使用 coco 数据集训练完成的，结构分别为： SSD+MobileNet SSD+Inception R-FCN+ResNet101 Faster RCNN+ResNet101 Faster RCNN+Inception_ResNet 训练新的模型1.下载数据集，并将真转换为 tfrecord 恪式 voc 2012 数据集的下载地址 2.object_detection文件夹中 ， 再新建一个voc文件夹，并将下载的数据集压缩包复制至 voc中,最终形成的目录为： 1234567891011121314151617181920research/ object_detection/ voc/ VOCdevkit/ VOC2012/ JPEGImages/ 2007_000027.jpg 2007_000032.jpg 2007_000033.jpg 2007_000039.jpg 2007_000042.jpg ……………… Annotations/ 2007_000027.xml 2007_000032.xml 2007_000033.xml 2007_000039.xml 2007_000042.xml ……………… ……………… 3.在object_detection目录中执行如下命令将数据集转换为tfrecord： 12python create_pascal_tf_record.py --data_dir voc/VOCdevkit/ --year=VOC2012 --set=train --output_path=voc/pascal_train.recordpython create_pascal_tf_record.py --data_dir voc/VOCdevkit/ --year=VOC2012 --set=val --output_path=voc/pascal_val.record 4.此外，将pascal_label_map.pbtxt数据复制到voc 文件夹下： 1cp data/pascal_label_map.pbtxt voc/ 5.下载模型文件并解压，解压后得到frozen_inference_graph.pb、graph.pbtxt、model.ckpt.data-00000-of-00001、model.ckpt.index、model.ckpt.meta 5 个文件。在voc文件夹中新建一个pretrained文件夹，并将这5个文件复制进去。 复制一份config文件： 1cp samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config \voc/voc.config 并在voc/voc.config中修改7处需要重新配置的地方： num_classes:改为VOC2012中的物体类别数，即20类 eval_config中的num_examples表示验证阶段需要执行的图片数量，修改为5823 还有５处含有PATH_TO_BE_CONFIGURED需要改为自己的目录 6.在 voc文件夹中新建一个 train_dir作为保存模型和日志的目录 7.训练模型的命令： 1python train.py --train_dir voc/train_dir/ --pipeline_config_path voc/voc.config 8.使用TensorBoard： 1tensorboard --logdir voc/train_dir/ 注意的是，如果发生内存和显存不足报错的情况，除了换用较小的模型进行训练外，还可以修改配置艾件中的以下部分： 123456image_resizer&#123; keep_aspect_ratio_resize&#123; min_dimension:600 max_dimension:1024 &#125;&#125; 表示将输入图像进行等比例缩放再开始训练，缩放后最大边长为 1024 ，最小边长为 600 可以将这两个数值改小（如分别改成 512 和 300 ) 导出模型并预测单张图片运行(需要根据voc/train_dir/里实际保存的checkpoint，将1582改为合适的数值)： 12345python export_inference_graph.py \ --input_type image_tensor \ --pipeline_config_path voc/voc.config \ --trained_checkpoint_prefix voc/train_dir/model.ckpt-1582 --output_directory voc/export/ 导出的模型是voc/export/frozen_inference_graph.pb文件。 预测可以参考前面的Jupyter Notebook代码]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4.21个项目玩转深度学习之Deep_Dream模型]]></title>
    <url>%2Ftensorflow%2F4.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BDeep_Dream%E6%A8%A1%E5%9E%8B.html</url>
    <content type="text"><![CDATA[Deep_Dream模型技术原理在卷积网络中，输入一般是一张图像，中间层是若干卷积运算，输出是图像的类别。在训练阶段，会使用大量的训练图片计算梯度，网络根据梯度不断地调整和学习最佳的参数。对此，通常会再一些疑问，例如: 卷积层究竟学习到了什么内窑? 卷积层的参数代表的意义是什么？ 浅层的卷积和深层的卷积学习到的内容哪些区别？ Deep Dream可以解答上述问题 我们可以使用一张原始的随机噪声图片，经过神经网络的不断调整，让某一个类别概率最大，得到极大化对应的图像，这就是神经网络眼中最具有这个类别特点的图像 TensorFlow 中的 Deep Dream 模型实践导入 Inception 模型load_inception.py12345678910111213141516171819202122232425262728293031323334# coding:utf-8# 导入要用到的基本模块。from __future__ import print_functionimport numpy as npimport tensorflow as tf# 创建图和Sessiongraph = tf.Graph()sess = tf.InteractiveSession(graph=graph)# tensorflow_inception_graph.pb文件中，既存储了inception的网络结构也存储了对应的数据# 使用下面的语句将之导入model_fn = 'tensorflow_inception_graph.pb'with tf.gfile.FastGFile(model_fn, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read())# 定义t_input为我们输入的图像t_input = tf.placeholder(np.float32, name='input')imagenet_mean = 117.0# 输入图像需要经过处理才能送入网络中# expand_dims是加一维，从[height, width, channel]变成[1, height, width, channel]# t_input - imagenet_mean是减去一个均值t_preprocessed = tf.expand_dims(t_input - imagenet_mean, 0)tf.import_graph_def(graph_def, &#123;'input': t_preprocessed&#125;)# 找到所有卷积层layers = [op.name for op in graph.get_operations() if op.type == 'Conv2D' and 'import/' in op.name]# 输出卷积层层数print('Number of layers', len(layers))# 特别地，输出mixed4d_3x3_bottleneck_pre_relu的形状name = 'mixed4d_3x3_bottleneck_pre_relu'print('shape of %s: %s' % (name, str(graph.get_tensor_by_name('import/' + name + ':0').get_shape()))) 生成原胎的 Deep Dream 图像gen_naive.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# coding: utf-8from __future__ import print_functionimport osfrom io import BytesIOimport numpy as npfrom functools import partialimport PIL.Imageimport scipy.miscimport tensorflow as tfgraph = tf.Graph()model_fn = 'tensorflow_inception_graph.pb'sess = tf.InteractiveSession(graph=graph)with tf.gfile.FastGFile(model_fn, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read())t_input = tf.placeholder(np.float32, name='input')imagenet_mean = 117.0t_preprocessed = tf.expand_dims(t_input - imagenet_mean, 0)tf.import_graph_def(graph_def, &#123;'input': t_preprocessed&#125;)def savearray(img_array, img_name): scipy.misc.toimage(img_array).save(img_name) print('img saved: %s' % img_name)def render_naive(t_obj, img0, iter_n=20, step=1.0): # t_score是优化目标。它是t_obj的平均值 # 结合调用处看，实际上就是layer_output[:, :, :, channel]的平均值 t_score = tf.reduce_mean(t_obj) # 计算t_score对t_input的梯度 t_grad = tf.gradients(t_score, t_input)[0] # 创建新图 img = img0.copy() for i in range(iter_n): # 在sess中计算梯度，以及当前的score g, score = sess.run([t_grad, t_score], &#123;t_input: img&#125;) # 对img应用梯度。step可以看做“学习率” g /= g.std() + 1e-8 img += g * step print('score(mean)=%f' % (score)) # 保存图片 savearray(img, 'naive.jpg')# 定义卷积层、通道数，并取出对应的tensorname = 'mixed4d_3x3_bottleneck_pre_relu'channel = 139layer_output = graph.get_tensor_by_name("import/%s:0" % name)# 定义原始的图像噪声img_noise = np.random.uniform(size=(224, 224, 3)) + 100.0# 调用render_naive函数渲染render_naive(layer_output[:, :, :, channel], img_noise, iter_n=20) 生成更大只寸的 Deep Dream 图像每次不对整张图片做优化，而是把图片分成几个部分，每次只对某中的一个部分做优化，这样每次优化时只会消耗固定大小的内存 gen_multiscale.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# coding:utf-8from __future__ import print_functionimport osfrom io import BytesIOimport numpy as npfrom functools import partialimport PIL.Imageimport scipy.miscimport tensorflow as tfgraph = tf.Graph()model_fn = 'tensorflow_inception_graph.pb'sess = tf.InteractiveSession(graph=graph)with tf.gfile.FastGFile(model_fn, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read())t_input = tf.placeholder(np.float32, name='input')imagenet_mean = 117.0t_preprocessed = tf.expand_dims(t_input - imagenet_mean, 0)tf.import_graph_def(graph_def, &#123;'input': t_preprocessed&#125;)def savearray(img_array, img_name): scipy.misc.toimage(img_array).save(img_name) print('img saved: %s' % img_name)def resize_ratio(img, ratio): min = img.min() max = img.max() img = (img - min) / (max - min) * 255 img = np.float32(scipy.misc.imresize(img, ratio)) img = img / 255 * (max - min) + min return imgdef calc_grad_tiled(img, t_grad, tile_size=512): # 每次只对tile_size×tile_size大小的图像计算梯度，避免内存问题 sz = tile_size h, w = img.shape[:2] # img_shift：先在行上做整体移动，再在列上做整体移动 # 防止在tile的边缘产生边缘效应 sx, sy = np.random.randint(sz, size=2) img_shift = np.roll(np.roll(img, sx, 1), sy, 0) grad = np.zeros_like(img) # y, x是开始位置的像素 for y in range(0, max(h - sz // 2, sz), sz): for x in range(0, max(w - sz // 2, sz), sz): # 每次对sub计算梯度。sub的大小是tile_size×tile_size sub = img_shift[y:y + sz, x:x + sz] g = sess.run(t_grad, &#123;t_input: sub&#125;) grad[y:y + sz, x:x + sz] = g # 使用np.roll移动回去 return np.roll(np.roll(grad, -sx, 1), -sy, 0)def render_multiscale(t_obj, img0, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4): # 同样定义目标和梯度 t_score = tf.reduce_mean(t_obj) t_grad = tf.gradients(t_score, t_input)[0] img = img0.copy() for octave in range(octave_n): if octave &gt; 0: # 每次将将图片放大octave_scale倍 # 共放大octave_n - 1 次 img = resize_ratio(img, octave_scale) for i in range(iter_n): # 调用calc_grad_tiled计算任意大小图像的梯度 g = calc_grad_tiled(img, t_grad) g /= g.std() + 1e-8 img += g * step print('.', end=' ') savearray(img, 'multiscale.jpg')if __name__ == '__main__': name = 'mixed4d_3x3_bottleneck_pre_relu' channel = 139 img_noise = np.random.uniform(size=(224, 224, 3)) + 100.0 layer_output = graph.get_tensor_by_name("import/%s:0" % name) render_multiscale(layer_output[:, :, :, channel], img_noise, iter_n=20) 如果直接计算梯度,可能会发生比较明显的“边缘效应”,使用np.roll(np.roll(img, sx, 1), sy, 0）对图片“整体移动,这样原先在图像边缘的像素就会被移动到中间，从而避免边缘效应 实际工程中，为了加快图像的收敛速度，采用先生成小尺寸，再将图片放大的方法 生成更高质量的 Deep Dream 图像拉普拉斯金字塔（Laplacian Pyramid）对图像进行分解。这种算法可以把图片分解为多层，底层的levell，level2就对应图像的高频成分，而上层的level3，level4 对应图像的低频成分。 可以对梯度也做这样的分解 gen_lapnorm.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138# coding:utf-8from __future__ import print_functionimport osfrom io import BytesIOimport numpy as npfrom functools import partialimport PIL.Imageimport scipy.miscimport tensorflow as tfgraph = tf.Graph()model_fn = 'tensorflow_inception_graph.pb'sess = tf.InteractiveSession(graph=graph)with tf.gfile.FastGFile(model_fn, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read())t_input = tf.placeholder(np.float32, name='input')imagenet_mean = 117.0t_preprocessed = tf.expand_dims(t_input - imagenet_mean, 0)tf.import_graph_def(graph_def, &#123;'input': t_preprocessed&#125;)def savearray(img_array, img_name): scipy.misc.toimage(img_array).save(img_name) print('img saved: %s' % img_name)def resize_ratio(img, ratio): min = img.min() max = img.max() img = (img - min) / (max - min) * 255 img = np.float32(scipy.misc.imresize(img, ratio)) img = img / 255 * (max - min) + min return imgdef calc_grad_tiled(img, t_grad, tile_size=512): sz = tile_size h, w = img.shape[:2] sx, sy = np.random.randint(sz, size=2) img_shift = np.roll(np.roll(img, sx, 1), sy, 0) # 先在行上做整体移动，再在列上做整体移动 grad = np.zeros_like(img) for y in range(0, max(h - sz // 2, sz), sz): for x in range(0, max(w - sz // 2, sz), sz): sub = img_shift[y:y + sz, x:x + sz] g = sess.run(t_grad, &#123;t_input: sub&#125;) grad[y:y + sz, x:x + sz] = g return np.roll(np.roll(grad, -sx, 1), -sy, 0)k = np.float32([1, 4, 6, 4, 1])k = np.outer(k, k)k5x5 = k[:, :, None, None] / k.sum() * np.eye(3, dtype=np.float32)# 这个函数将图像分为低频和高频成分def lap_split(img): with tf.name_scope('split'): # 做过一次卷积相当于一次“平滑”，因此lo为低频成分 lo = tf.nn.conv2d(img, k5x5, [1, 2, 2, 1], 'SAME') # 低频成分放缩到原始图像一样大小得到lo2，再用原始图像img减去lo2，就得到高频成分hi lo2 = tf.nn.conv2d_transpose(lo, k5x5 * 4, tf.shape(img), [1, 2, 2, 1]) hi = img - lo2 return lo, hi# 这个函数将图像img分成n层拉普拉斯金字塔def lap_split_n(img, n): levels = [] for i in range(n): # 调用lap_split将图像分为低频和高频部分 # 高频部分保存到levels中 # 低频部分再继续分解 img, hi = lap_split(img) levels.append(hi) levels.append(img) return levels[::-1]# 将拉普拉斯金字塔还原到原始图像def lap_merge(levels): img = levels[0] for hi in levels[1:]: with tf.name_scope('merge'): img = tf.nn.conv2d_transpose(img, k5x5 * 4, tf.shape(hi), [1, 2, 2, 1]) + hi return img# 对img做标准化。def normalize_std(img, eps=1e-10): with tf.name_scope('normalize'): std = tf.sqrt(tf.reduce_mean(tf.square(img))) return img / tf.maximum(std, eps)# 拉普拉斯金字塔标准化def lap_normalize(img, scale_n=4): img = tf.expand_dims(img, 0) tlevels = lap_split_n(img, scale_n) # 每一层都做一次normalize_std tlevels = list(map(normalize_std, tlevels)) out = lap_merge(tlevels) return out[0, :, :, :]def tffunc(*argtypes): placeholders = list(map(tf.placeholder, argtypes)) def wrap(f): out = f(*placeholders) def wrapper(*args, **kw): return out.eval(dict(zip(placeholders, args)), session=kw.get('session')) return wrapper return wrapdef render_lapnorm(t_obj, img0, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4): # 同样定义目标和梯度 t_score = tf.reduce_mean(t_obj) t_grad = tf.gradients(t_score, t_input)[0] # 将lap_normalize转换为正常函数 lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n)) img = img0.copy() for octave in range(octave_n): if octave &gt; 0: img = resize_ratio(img, octave_scale) for i in range(iter_n): g = calc_grad_tiled(img, t_grad) # 唯一的区别在于我们使用lap_norm_func来标准化g！ g = lap_norm_func(g) img += g * step print('.', end=' ') savearray(img, 'lapnorm.jpg')if __name__ == '__main__': name = 'mixed4d_3x3_bottleneck_pre_relu' channel = 139 img_noise = np.random.uniform(size=(224, 224, 3)) + 100.0 layer_output = graph.get_tensor_by_name("import/%s:0" % name) render_lapnorm(layer_output[:, :, :, channel], img_noise, iter_n=20) 最终的 Deep Dream 模型gen_deepdream.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117# coding:utf-8from __future__ import print_functionimport osfrom io import BytesIOimport numpy as npfrom functools import partialimport PIL.Imageimport scipy.miscimport tensorflow as tfgraph = tf.Graph()model_fn = 'tensorflow_inception_graph.pb'sess = tf.InteractiveSession(graph=graph)with tf.gfile.FastGFile(model_fn, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read())t_input = tf.placeholder(np.float32, name='input') # define the input tensorimagenet_mean = 117.0t_preprocessed = tf.expand_dims(t_input - imagenet_mean, 0)tf.import_graph_def(graph_def, &#123;'input': t_preprocessed&#125;)def savearray(img_array, img_name): scipy.misc.toimage(img_array).save(img_name) print('img saved: %s' % img_name)def visstd(a, s=0.1): return (a - a.mean()) / max(a.std(), 1e-4) * s + 0.5def resize_ratio(img, ratio): min = img.min() max = img.max() img = (img - min) / (max - min) * 255 img = np.float32(scipy.misc.imresize(img, ratio)) img = img / 255 * (max - min) + min return imgdef resize(img, hw): min = img.min() max = img.max() img = (img - min) / (max - min) * 255 img = np.float32(scipy.misc.imresize(img, hw)) img = img / 255 * (max - min) + min return imgdef calc_grad_tiled(img, t_grad, tile_size=512): sz = tile_size h, w = img.shape[:2] sx, sy = np.random.randint(sz, size=2) img_shift = np.roll(np.roll(img, sx, 1), sy, 0) # 先在行上做整体移动，再在列上做整体移动 grad = np.zeros_like(img) for y in range(0, max(h - sz // 2, sz), sz): for x in range(0, max(w - sz // 2, sz), sz): sub = img_shift[y:y + sz, x:x + sz] g = sess.run(t_grad, &#123;t_input: sub&#125;) grad[y:y + sz, x:x + sz] = g return np.roll(np.roll(grad, -sx, 1), -sy, 0)def tffunc(*argtypes): placeholders = list(map(tf.placeholder, argtypes)) def wrap(f): out = f(*placeholders) def wrapper(*args, **kw): return out.eval(dict(zip(placeholders, args)), session=kw.get('session')) return wrapper return wrapdef render_deepdream(t_obj, img0, iter_n=10, step=1.5, octave_n=4, octave_scale=1.4): t_score = tf.reduce_mean(t_obj) t_grad = tf.gradients(t_score, t_input)[0] img = img0 # 同样将图像进行金字塔分解 # 此时提取高频、低频的方法比较简单。直接缩放就可以 octaves = [] for i in range(octave_n - 1): hw = img.shape[:2] lo = resize(img, np.int32(np.float32(hw) / octave_scale)) hi = img - resize(lo, hw) img = lo octaves.append(hi) # 先生成低频的图像，再依次放大并加上高频 for octave in range(octave_n): if octave &gt; 0: hi = octaves[-octave] img = resize(img, hi.shape[:2]) + hi for i in range(iter_n): g = calc_grad_tiled(img, t_grad) img += g * (step / (np.abs(g).mean() + 1e-7)) print('.', end=' ') img = img.clip(0, 255) savearray(img, 'deepdream.jpg')if __name__ == '__main__': img0 = PIL.Image.open('test.jpg') img0 = np.float32(img0) name = 'mixed4d_3x3_bottleneck_pre_relu' channel = 139 layer_output = graph.get_tensor_by_name("import/%s:0" % name) render_deepdream(layer_output[:, :, :, channel], img0) # name = 'mixed4c' # layer_output = graph.get_tensor_by_name("import/%s:0" % name) # render_deepdream(tf.square(layer_output), img0)]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3.21个项目玩转深度学习之打造自己的图像识别模型]]></title>
    <url>%2Ftensorflow%2F3.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B.html</url>
    <content type="text"><![CDATA[打造自己的图像识别模型数据准备 数据集划分 转换为 tfrecord 使用data_convert.py将图片转换为tfrecord格式12345python data_convert.py -t pic/ \ --train-shards 2 \ --validation-shards 2 \ --num-threads 2 \ --dataset-name satellite -t pic/ : 表示转换pic下的数据，文件夹必须有一个train目录，和一个validation目录 –train-shards 2 : 表示将数据分为两块，数据集更多可以分更多块 –validation-shards 2 : 表示将数据分为两块，数据集更多可以分更多块 –num-threads 2 : 采用两个线程产生数据，线程数必须整除train-shards和validation-shards –dataset-name satellite : 给数据集起名字，最后生成的就是satellite_train和satellite_validation使用TensorFlow Slim 微调模型下载TensorFlow Slim 源代码1git clone https://github.com/tensorflow/models.git TensorFlow Slim 代码结构 文件夹 用途 datasets 定义了一些训练使用的数据集 nets 常用网络 preprocessing 预处理和数据增强 scripts 训练的示例脚本 train_image_classifier.py 训练模型的入口代码 eval_image_classifier.py 验证模型的入口代码 download_and_convert_data.py 下载并转换数据集格式的入口代码 ### 定义新的datasets文件 + datasets/下新建satellite.py，将flowers.py复制进来，修改以下几处： 12345_FILE_PATTERN = 'satellite_%s_*.tfrecord'SPLITS_TO_SIZES = &#123;'train':4800,'validation':1200&#125;_NUM_CLASSES = 6 因为数据是jpg，修改image/format 1'image/format': tf.FixedLenFeature((),tf.string,default_value='jpg'), 在dataset_factory.py中注册satellite数据库 123456789from datasets import satellitedatasets_map = &#123; 'cifar10':cifar10, 'flowers':flowers, 'imagenet':imagenet, 'mnist':mnist, 'satellite':satellite,&#125; 准备训练文件夹 在slim中再建一个satellite目录 新建一个data目录，将5个转好格式的训练数据复制进去 新建一个train_dir，用来保存训练过程中的日志和模型 新建一个pretrained目录，在slim找到Inception V3模型，下载解压得到一个inception_v3.ckpt文件，复制进去 开始训练在slim文件夹下，运行下面命令就可以开始训练了 123456789101112131415161718python train_image_classifier.py \ --train_dir=satellite/train_dir \ --dataset_name=satellite \ --dataset_split_name=train \ --dataset_dir=satellite/data \ --model_name=inception_v3 \ --checkpoint_path=satellite/pretrained/inception_v3.ckpt \ --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \ --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \ --max_number_of_steps=100000 \ --batch_size=32 \ --learning_rate=0.001 \ --learning_rate_decay_type=fixed \ --save_interval_secs=300 \ --save_summaries_secs=2 \ --log_every_n_steps=10 \ --optimizer=rmsprop \ --weight_decay=0.00004 --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits:规定了模型微调变量的范围，这里只对InceptionV3/Logits,InceptionV3/AuxLogits进行微调，相当于末端层，不设定的话，会对所有参数进行训练 --train_dir=satellite/train_dir:保存日志和checkpoint --dataset_name=satellite，--dataset_split_name=train:指定训练的数据集 --dataset_dir=satellite/data:指定训练数据集保存的位置 --model_name=inception_v3:使用模型 --checkpoint_path=satellite/pretrained/inception_v3.ckpt:预训练模型 --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits:恢复预训练模型不恢复这两层 --max_number_of_steps=100000:最大步数 --batch_size=32:每步使用的batch数量 --learning_rate=0.001:学习率 --learning_rate_decay_type=fixed: 学习率是否自动下降，这里是固定学习率 --save_interval_secs=300: 每隔300s保存到train_dir --save_summaries_secs=2: 每隔2s，写日志到train_dir --log_every_n_steps=10: 每隔10步，屏幕上打印出训练信息 --optimizer=rmsprop: 表示选定的优化器 --weight_decay=0.00004:选定的weight_decay值，即模型所有参数的二次正则化超参数 还可以对所有层进行训练： 1234567891011121314151617python train_image_classifier.py \ --train_dir=satellite/train_dir \ --dataset_name=satellite \ --dataset_split_name=train \ --dataset_dir=satellite/data \ --model_name=inception_v3 \ --checkpoint_path=satellite/pretrained/inception_v3.ckpt \ --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \ --max_number_of_steps=100000 \ --batch_size=32 \ --learning_rate=0.001 \ --learning_rate_decay_type=fixed \ --save_interval_secs=300 \ --save_summaries_secs=2 \ --log_every_n_steps=10 \ --optimizer=rmsprop \ --weight_decay=0.00004 验证模型准确率可以使用eval_image_classifier.py进行验证 1234567python eval_image_classifier.py \ --checkpoint_path=satellite/train_dir \ --eval_dir=satellite/eval_dir \ --dataset_name=satellite \ --dataset_split_name=validation \ --dataset_dir=satellite/data \ --model_name=inception_v3 TensorBoard可视化使用以下命令打开TensorBoard 1tensorboard --logdir satellite/train_dir 导出模型并对单张图片进行识别 freeze_graph.py: 导出一个用于识别的模型 classify_image_inception_v3.py:使用inception_v3对单张图片做识别Slim 提供了导出网络结构的脚本export_inference_graph.py,slim文件夹下运行：12345python export_inference_graph.py \ --alsologtostderr \ --model_name=inception_v3 \ --output_file=satellite/inception_v3_inf_graph.pb \ --dataset_name satellite 这个pb文件只会保存网络结构，不包含训练的参数，所以需要将checkpoint模型参数保存进来 使用freeze_graph.py:123456python freeze_graph.py \ --input_graph slim/satellite/inception_v3_inf_graph.pb \ --input_checkpoint slim/satellite/train_dir/model.ckpt-5700 \ --input_binary true \ --output_node_names InceptionV3/Predictions/Reshape_1 \ --output_graph slim/satellite/frozen_graph.pb 使用classify_image_inception_v3.py对单张图片进行预测：1234python classify_image_inception_v3.py \ --model_path slim/satellite/frozen_graph.pb \ --label_path data_prepare/pic/label.txt \ --image_file test_image.jpg 附录data_convert.py1234567891011121314151617181920212223242526272829303132333435# coding:utf-8from __future__ import absolute_importimport argparseimport osimport loggingfrom src.tfrecord import maindef parse_args(): parser = argparse.ArgumentParser() parser.add_argument('-t', '--tensorflow-data-dir', default='pic/') parser.add_argument('--train-shards', default=2, type=int) parser.add_argument('--validation-shards', default=2, type=int) parser.add_argument('--num-threads', default=2, type=int) parser.add_argument('--dataset-name', default='satellite', type=str) return parser.parse_args()if __name__ == '__main__': logging.basicConfig(level=logging.INFO) args = parse_args() args.tensorflow_dir = args.tensorflow_data_dir args.train_directory = os.path.join(args.tensorflow_dir, 'train') args.validation_directory = os.path.join(args.tensorflow_dir, 'validation') args.output_directory = args.tensorflow_dir args.labels_file = os.path.join(args.tensorflow_dir, 'label.txt') if os.path.exists(args.labels_file) is False: logging.warning('Can\'t find label.txt. Now create it.') all_entries = os.listdir(args.train_directory) dirnames = [] for entry in all_entries: if os.path.isdir(os.path.join(args.train_directory, entry)): dirnames.append(entry) with open(args.labels_file, 'w') as f: for dirname in dirnames: f.write(dirname + '\n') main(args) tfrecord.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411# coding:utf-8# Copyright 2016 Google Inc. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Converts image data to TFRecords file format with Example protos.The image data set is expected to reside in JPEG files located in thefollowing directory structure. data_dir/label_0/image0.jpeg data_dir/label_0/image1.jpg ... data_dir/label_1/weird-image.jpeg data_dir/label_1/my-image.jpeg ...where the sub-directory is the unique label associated with these images.This TensorFlow script converts the training and evaluation data intoa sharded data set consisting of TFRecord files train_directory/train-00000-of-01024 train_directory/train-00001-of-01024 ... train_directory/train-00127-of-01024and validation_directory/validation-00000-of-00128 validation_directory/validation-00001-of-00128 ... validation_directory/validation-00127-of-00128where we have selected 1024 and 128 shards for each data set. Each recordwithin the TFRecord file is a serialized Example proto. The Example protocontains the following fields: image/encoded: string containing JPEG encoded image in RGB colorspace image/height: integer, image height in pixels image/width: integer, image width in pixels image/colorspace: string, specifying the colorspace, always 'RGB' image/channels: integer, specifying the number of channels, always 3 image/format: string, specifying the format, always'JPEG' image/filename: string containing the basename of the image file e.g. 'n01440764_10026.JPEG' or 'ILSVRC2012_val_00000293.JPEG' image/class/label: integer specifying the index in a classification layer. start from "class_label_base" image/class/text: string specifying the human-readable version of the label e.g. 'dog'If you data set involves bounding boxes, please look at build_imagenet_data.py."""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom datetime import datetimeimport osimport randomimport sysimport threadingimport numpy as npimport tensorflow as tfimport loggingdef _int64_feature(value): """Wrapper for inserting int64 features into Example proto.""" if not isinstance(value, list): value = [value] return tf.train.Feature(int64_list=tf.train.Int64List(value=value))def _bytes_feature(value): """Wrapper for inserting bytes features into Example proto.""" return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))def _convert_to_example(filename, image_buffer, label, text, height, width): """Build an Example proto for an example. Args: filename: string, path to an image file, e.g., '/path/to/example.JPG' image_buffer: string, JPEG encoding of RGB image label: integer, identifier for the ground truth for the network text: string, unique human-readable, e.g. 'dog' height: integer, image height in pixels width: integer, image width in pixels Returns: Example proto """ colorspace = 'RGB' channels = 3 image_format = 'JPEG' example = tf.train.Example(features=tf.train.Features(feature=&#123; 'image/height': _int64_feature(height), 'image/width': _int64_feature(width), 'image/colorspace': _bytes_feature(colorspace), 'image/channels': _int64_feature(channels), 'image/class/label': _int64_feature(label), 'image/class/text': _bytes_feature(text), 'image/format': _bytes_feature(image_format), 'image/filename': _bytes_feature(os.path.basename(filename)), 'image/encoded': _bytes_feature(image_buffer)&#125;)) return exampleclass ImageCoder(object): """Helper class that provides TensorFlow image coding utilities.""" def __init__(self): # Create a single Session to run all image coding calls. self._sess = tf.Session() # Initializes function that converts PNG to JPEG data. self._png_data = tf.placeholder(dtype=tf.string) image = tf.image.decode_png(self._png_data, channels=3) self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100) # Initializes function that decodes RGB JPEG data. self._decode_jpeg_data = tf.placeholder(dtype=tf.string) self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3) def png_to_jpeg(self, image_data): return self._sess.run(self._png_to_jpeg, feed_dict=&#123;self._png_data: image_data&#125;) def decode_jpeg(self, image_data): image = self._sess.run(self._decode_jpeg, feed_dict=&#123;self._decode_jpeg_data: image_data&#125;) assert len(image.shape) == 3 assert image.shape[2] == 3 return imagedef _is_png(filename): """Determine if a file contains a PNG format image. Args: filename: string, path of the image file. Returns: boolean indicating if the image is a PNG. """ return '.png' in filenamedef _process_image(filename, coder): """Process a single image file. Args: filename: string, path to an image file e.g., '/path/to/example.JPG'. coder: instance of ImageCoder to provide TensorFlow image coding utils. Returns: image_buffer: string, JPEG encoding of RGB image. height: integer, image height in pixels. width: integer, image width in pixels. """ # Read the image file. with open(filename, 'r') as f: image_data = f.read() # Convert any PNG to JPEG's for consistency. if _is_png(filename): logging.info('Converting PNG to JPEG for %s' % filename) image_data = coder.png_to_jpeg(image_data) # Decode the RGB JPEG. image = coder.decode_jpeg(image_data) # Check that image converted to RGB assert len(image.shape) == 3 height = image.shape[0] width = image.shape[1] assert image.shape[2] == 3 return image_data, height, widthdef _process_image_files_batch(coder, thread_index, ranges, name, filenames, texts, labels, num_shards, command_args): """Processes and saves list of images as TFRecord in 1 thread. Args: coder: instance of ImageCoder to provide TensorFlow image coding utils. thread_index: integer, unique batch to run index is within [0, len(ranges)). ranges: list of pairs of integers specifying ranges of each batches to analyze in parallel. name: string, unique identifier specifying the data set filenames: list of strings; each string is a path to an image file texts: list of strings; each string is human readable, e.g. 'dog' labels: list of integer; each integer identifies the ground truth num_shards: integer number of shards for this data set. """ # Each thread produces N shards where N = int(num_shards / num_threads). # For instance, if num_shards = 128, and the num_threads = 2, then the first # thread would produce shards [0, 64). num_threads = len(ranges) assert not num_shards % num_threads num_shards_per_batch = int(num_shards / num_threads) shard_ranges = np.linspace(ranges[thread_index][0], ranges[thread_index][1], num_shards_per_batch + 1).astype(int) num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0] counter = 0 for s in xrange(num_shards_per_batch): # Generate a sharded version of the file name, e.g. 'train-00002-of-00010' shard = thread_index * num_shards_per_batch + s output_filename = '%s_%s_%.5d-of-%.5d.tfrecord' % (command_args.dataset_name, name, shard, num_shards) output_file = os.path.join(command_args.output_directory, output_filename) writer = tf.python_io.TFRecordWriter(output_file) shard_counter = 0 files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int) for i in files_in_shard: filename = filenames[i] label = labels[i] text = texts[i] image_buffer, height, width = _process_image(filename, coder) example = _convert_to_example(filename, image_buffer, label, text, height, width) writer.write(example.SerializeToString()) shard_counter += 1 counter += 1 if not counter % 1000: logging.info('%s [thread %d]: Processed %d of %d images in thread batch.' % (datetime.now(), thread_index, counter, num_files_in_thread)) sys.stdout.flush() writer.close() logging.info('%s [thread %d]: Wrote %d images to %s' % (datetime.now(), thread_index, shard_counter, output_file)) sys.stdout.flush() shard_counter = 0 logging.info('%s [thread %d]: Wrote %d images to %d shards.' % (datetime.now(), thread_index, counter, num_files_in_thread)) sys.stdout.flush()def _process_image_files(name, filenames, texts, labels, num_shards, command_args): """Process and save list of images as TFRecord of Example protos. Args: name: string, unique identifier specifying the data set filenames: list of strings; each string is a path to an image file texts: list of strings; each string is human readable, e.g. 'dog' labels: list of integer; each integer identifies the ground truth num_shards: integer number of shards for this data set. """ assert len(filenames) == len(texts) assert len(filenames) == len(labels) # Break all images into batches with a [ranges[i][0], ranges[i][1]]. spacing = np.linspace(0, len(filenames), command_args.num_threads + 1).astype(np.int) ranges = [] for i in xrange(len(spacing) - 1): ranges.append([spacing[i], spacing[i + 1]]) # Launch a thread for each batch. logging.info('Launching %d threads for spacings: %s' % (command_args.num_threads, ranges)) sys.stdout.flush() # Create a mechanism for monitoring when all threads are finished. coord = tf.train.Coordinator() # Create a generic TensorFlow-based utility for converting all image codings. coder = ImageCoder() threads = [] for thread_index in xrange(len(ranges)): args = (coder, thread_index, ranges, name, filenames, texts, labels, num_shards, command_args) t = threading.Thread(target=_process_image_files_batch, args=args) t.start() threads.append(t) # Wait for all the threads to terminate. coord.join(threads) logging.info('%s: Finished writing all %d images in data set.' % (datetime.now(), len(filenames))) sys.stdout.flush()def _find_image_files(data_dir, labels_file, command_args): """Build a list of all images files and labels in the data set. Args: data_dir: string, path to the root directory of images. Assumes that the image data set resides in JPEG files located in the following directory structure. data_dir/dog/another-image.JPEG data_dir/dog/my-image.jpg where 'dog' is the label associated with these images. labels_file: string, path to the labels file. The list of valid labels are held in this file. Assumes that the file contains entries as such: dog cat flower where each line corresponds to a label. We map each label contained in the file to an integer starting with the integer 0 corresponding to the label contained in the first line. Returns: filenames: list of strings; each string is a path to an image file. texts: list of strings; each string is the class, e.g. 'dog' labels: list of integer; each integer identifies the ground truth. """ logging.info('Determining list of input files and labels from %s.' % data_dir) unique_labels = [l.strip() for l in tf.gfile.FastGFile( labels_file, 'r').readlines()] labels = [] filenames = [] texts = [] # Leave label index 0 empty as a background class. """非常重要，这里我们调整label从0开始以符合定义""" label_index = command_args.class_label_base # Construct the list of JPEG files and labels. for text in unique_labels: jpeg_file_path = '%s/%s/*' % (data_dir, text) matching_files = tf.gfile.Glob(jpeg_file_path) labels.extend([label_index] * len(matching_files)) texts.extend([text] * len(matching_files)) filenames.extend(matching_files) if not label_index % 100: logging.info('Finished finding files in %d of %d classes.' % ( label_index, len(labels))) label_index += 1 # Shuffle the ordering of all image files in order to guarantee # random ordering of the images with respect to label in the # saved TFRecord files. Make the randomization repeatable. shuffled_index = range(len(filenames)) random.seed(12345) random.shuffle(shuffled_index) filenames = [filenames[i] for i in shuffled_index] texts = [texts[i] for i in shuffled_index] labels = [labels[i] for i in shuffled_index] logging.info('Found %d JPEG files across %d labels inside %s.' % (len(filenames), len(unique_labels), data_dir)) # print(labels) return filenames, texts, labelsdef _process_dataset(name, directory, num_shards, labels_file, command_args): """Process a complete data set and save it as a TFRecord. Args: name: string, unique identifier specifying the data set. directory: string, root path to the data set. num_shards: integer number of shards for this data set. labels_file: string, path to the labels file. """ filenames, texts, labels = _find_image_files(directory, labels_file, command_args) _process_image_files(name, filenames, texts, labels, num_shards, command_args)def check_and_set_default_args(command_args): if not(hasattr(command_args, 'train_shards')) or command_args.train_shards is None: command_args.train_shards = 5 if not(hasattr(command_args, 'validation_shards')) or command_args.validation_shards is None: command_args.validation_shards = 5 if not(hasattr(command_args, 'num_threads')) or command_args.num_threads is None: command_args.num_threads = 5 if not(hasattr(command_args, 'class_label_base')) or command_args.class_label_base is None: command_args.class_label_base = 0 if not(hasattr(command_args, 'dataset_name')) or command_args.dataset_name is None: command_args.dataset_name = '' assert not command_args.train_shards % command_args.num_threads, ( 'Please make the command_args.num_threads commensurate with command_args.train_shards') assert not command_args.validation_shards % command_args.num_threads, ( 'Please make the command_args.num_threads commensurate with ' 'command_args.validation_shards') assert command_args.train_directory is not None assert command_args.validation_directory is not None assert command_args.labels_file is not None assert command_args.output_directory is not Nonedef main(command_args): """ command_args:需要有以下属性： command_args.train_directory 训练集所在的文件夹。这个文件夹下面，每个文件夹的名字代表label名称，再下面就是图片。 command_args.validation_directory 验证集所在的文件夹。这个文件夹下面，每个文件夹的名字代表label名称，再下面就是图片。 command_args.labels_file 一个文件。每一行代表一个label名称。 command_args.output_directory 一个文件夹，表示最后输出的位置。 command_args.train_shards 将训练集分成多少份。 command_args.validation_shards 将验证集分成多少份。 command_args.num_threads 线程数。必须是上面两个参数的约数。 command_args.class_label_base 很重要！真正的tfrecord中，每个class的label号从多少开始，默认为0（在models/slim中就是从0开始的） command_args.dataset_name 字符串，输出的时候的前缀。 图片不可以有损坏。否则会导致线程提前退出。 """ check_and_set_default_args(command_args) logging.info('Saving results to %s' % command_args.output_directory) # Run it! _process_dataset('validation', command_args.validation_directory, command_args.validation_shards, command_args.labels_file, command_args) _process_dataset('train', command_args.train_directory, command_args.train_shards, command_args.labels_file, command_args) freeze_graph.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================r"""Converts checkpoint variables into Const ops in a standalone GraphDef file.This script is designed to take a GraphDef proto, a SaverDef proto, and a set ofvariable values stored in a checkpoint file, and output a GraphDef with all ofthe variable ops converted into const ops containing the values of thevariables.It's useful to do this when we need to load a single file in C++, especially inenvironments like mobile or embedded where we may not have access to theRestoreTensor ops and file loading calls that they rely on.An example of command-line usage is:bazel build tensorflow/python/tools:freeze_graph &amp;&amp; \bazel-bin/tensorflow/python/tools/freeze_graph \--input_graph=some_graph_def.pb \--input_checkpoint=model.ckpt-8361242 \--output_graph=/tmp/frozen_graph.pb --output_node_names=softmaxYou can also look at freeze_graph_test.py for an example of how to use it."""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport argparseimport sysfrom google.protobuf import text_formatfrom tensorflow.core.framework import graph_pb2from tensorflow.core.protobuf import saver_pb2from tensorflow.python import pywrap_tensorflowfrom tensorflow.python.client import sessionfrom tensorflow.python.framework import graph_utilfrom tensorflow.python.framework import importerfrom tensorflow.python.platform import appfrom tensorflow.python.platform import gfilefrom tensorflow.python.training import saver as saver_libFLAGS = Nonedef freeze_graph_with_def_protos( input_graph_def, input_saver_def, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_blacklist=""): """Converts all variables in a graph and checkpoint into constants.""" del restore_op_name, filename_tensor_name # Unused by updated loading code. # 'input_checkpoint' may be a prefix if we're using Saver V2 format if not saver_lib.checkpoint_exists(input_checkpoint): print("Input checkpoint '" + input_checkpoint + "' doesn't exist!") return -1 if not output_node_names: print("You need to supply the name of a node to --output_node_names.") return -1 # Remove all the explicit device specifications for this node. This helps to # make the graph more portable. if clear_devices: for node in input_graph_def.node: node.device = "" _ = importer.import_graph_def(input_graph_def, name="") with session.Session() as sess: if input_saver_def: saver = saver_lib.Saver(saver_def=input_saver_def) saver.restore(sess, input_checkpoint) else: var_list = &#123;&#125; reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint) var_to_shape_map = reader.get_variable_to_shape_map() for key in var_to_shape_map: try: tensor = sess.graph.get_tensor_by_name(key + ":0") except KeyError: # This tensor doesn't exist in the graph (for example it's # 'global_step' or a similar housekeeping element) so skip it. continue var_list[key] = tensor saver = saver_lib.Saver(var_list=var_list) saver.restore(sess, input_checkpoint) if initializer_nodes: sess.run(initializer_nodes) variable_names_blacklist = (variable_names_blacklist.split(",") if variable_names_blacklist else None) output_graph_def = graph_util.convert_variables_to_constants( sess, input_graph_def, output_node_names.split(","), variable_names_blacklist=variable_names_blacklist) # Write GraphDef to file if output path has been given. if output_graph: with gfile.GFile(output_graph, "wb") as f: f.write(output_graph_def.SerializeToString()) print("%d ops in the final graph." % len(output_graph_def.node)) return output_graph_defdef _parse_input_graph_proto(input_graph, input_binary): """Parser input tensorflow graph into GraphDef proto.""" if not gfile.Exists(input_graph): print("Input graph file '" + input_graph + "' does not exist!") return -1 input_graph_def = graph_pb2.GraphDef() mode = "rb" if input_binary else "r" with gfile.FastGFile(input_graph, mode) as f: if input_binary: input_graph_def.ParseFromString(f.read()) else: text_format.Merge(f.read(), input_graph_def) return input_graph_defdef _parse_input_saver_proto(input_saver, input_binary): """Parser input tensorflow Saver into SaverDef proto.""" if not gfile.Exists(input_saver): print("Input saver file '" + input_saver + "' does not exist!") return -1 mode = "rb" if input_binary else "r" with gfile.FastGFile(input_saver, mode) as f: saver_def = saver_pb2.SaverDef() if input_binary: saver_def.ParseFromString(f.read()) else: text_format.Merge(f.read(), saver_def) return saver_defdef freeze_graph(input_graph, input_saver, input_binary, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_blacklist=""): """Converts all variables in a graph and checkpoint into constants.""" input_graph_def = _parse_input_graph_proto(input_graph, input_binary) input_saver_def = None if input_saver: input_saver_def = _parse_input_saver_proto(input_saver, input_binary) freeze_graph_with_def_protos( input_graph_def, input_saver_def, input_checkpoint, output_node_names, restore_op_name, filename_tensor_name, output_graph, clear_devices, initializer_nodes, variable_names_blacklist)def main(unused_args): freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary, FLAGS.input_checkpoint, FLAGS.output_node_names, FLAGS.restore_op_name, FLAGS.filename_tensor_name, FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes, FLAGS.variable_names_blacklist)if __name__ == "__main__": parser = argparse.ArgumentParser() parser.register("type", "bool", lambda v: v.lower() == "true") parser.add_argument( "--input_graph", type=str, default="", help="TensorFlow \'GraphDef\' file to load.") parser.add_argument( "--input_saver", type=str, default="", help="TensorFlow saver file to load.") parser.add_argument( "--input_checkpoint", type=str, default="", help="TensorFlow variables file to load.") parser.add_argument( "--output_graph", type=str, default="", help="Output \'GraphDef\' file name.") parser.add_argument( "--input_binary", nargs="?", const=True, type="bool", default=False, help="Whether the input files are in binary format.") parser.add_argument( "--output_node_names", type=str, default="", help="The name of the output nodes, comma separated.") parser.add_argument( "--restore_op_name", type=str, default="save/restore_all", help="The name of the master restore operator.") parser.add_argument( "--filename_tensor_name", type=str, default="save/Const:0", help="The name of the tensor holding the save path.") parser.add_argument( "--clear_devices", nargs="?", const=True, type="bool", default=True, help="Whether to remove device specifications.") parser.add_argument( "--initializer_nodes", type=str, default="", help="comma separated list of initializer nodes to run before freezing.") parser.add_argument( "--variable_names_blacklist", type=str, default="", help="""\ comma separated list of variables to skip converting to constants\ """) FLAGS, unparsed = parser.parse_known_args() app.run(main=main, argv=[sys.argv[0]] + unparsed) classify_image_inception_v3.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport argparseimport os.pathimport reimport sysimport tarfileimport numpy as npfrom six.moves import urllibimport tensorflow as tfFLAGS = Noneclass NodeLookup(object): def __init__(self, label_lookup_path=None): self.node_lookup = self.load(label_lookup_path) def load(self, label_lookup_path): node_id_to_name = &#123;&#125; with open(label_lookup_path) as f: for index, line in enumerate(f): node_id_to_name[index] = line.strip() return node_id_to_name def id_to_string(self, node_id): if node_id not in self.node_lookup: return '' return self.node_lookup[node_id]def create_graph(): """Creates a graph from saved GraphDef file and returns a saver.""" # Creates graph from saved graph_def.pb. with tf.gfile.FastGFile(FLAGS.model_path, 'rb') as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) _ = tf.import_graph_def(graph_def, name='')def preprocess_for_eval(image, height, width, central_fraction=0.875, scope=None): with tf.name_scope(scope, 'eval_image', [image, height, width]): if image.dtype != tf.float32: image = tf.image.convert_image_dtype(image, dtype=tf.float32) # Crop the central region of the image with an area containing 87.5% of # the original image. if central_fraction: image = tf.image.central_crop(image, central_fraction=central_fraction) if height and width: # Resize the image to the specified height and width. image = tf.expand_dims(image, 0) image = tf.image.resize_bilinear(image, [height, width], align_corners=False) image = tf.squeeze(image, [0]) image = tf.subtract(image, 0.5) image = tf.multiply(image, 2.0) return imagedef run_inference_on_image(image): """Runs inference on an image. Args: image: Image file name. Returns: Nothing """ with tf.Graph().as_default(): image_data = tf.gfile.FastGFile(image, 'rb').read() image_data = tf.image.decode_jpeg(image_data) image_data = preprocess_for_eval(image_data, 299, 299) image_data = tf.expand_dims(image_data, 0) with tf.Session() as sess: image_data = sess.run(image_data) # Creates graph from saved GraphDef. create_graph() with tf.Session() as sess: softmax_tensor = sess.graph.get_tensor_by_name('InceptionV3/Logits/SpatialSqueeze:0') predictions = sess.run(softmax_tensor, &#123;'input:0': image_data&#125;) predictions = np.squeeze(predictions) # Creates node ID --&gt; English string lookup. node_lookup = NodeLookup(FLAGS.label_path) top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1] for node_id in top_k: human_string = node_lookup.id_to_string(node_id) score = predictions[node_id] print('%s (score = %.5f)' % (human_string, score))def main(_): image = FLAGS.image_file run_inference_on_image(image)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument( '--model_path', type=str, ) parser.add_argument( '--label_path', type=str, ) parser.add_argument( '--image_file', type=str, default='', help='Absolute path to image file.' ) parser.add_argument( '--num_top_predictions', type=int, default=5, help='Display this many predictions.' ) FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2.21个项目玩转深度学习之CIFAR10与ImageNet图像识别]]></title>
    <url>%2Ftensorflow%2F2.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BCIFAR10%E4%B8%8EImageNet%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB.html</url>
    <content type="text"><![CDATA[CIFAR-10图像识别CIFAR-10数据集下载建议使用欧洲网络代理，下载地址，解压后： 文件名 文件用途 batches.meta.txt 文本文件，存储类别名称 data_batch_1~5.bin 5个以二进制存储10000张彩色图像的训练数据 test_batch.bin 测试图像和测试图像的标签 readme.html 数据集介绍文件 Tensorflow 的数据读取机制TensorFlow使用“文件名队列 + 内存队列” 的双队列形式读入文件，如何在TensorFlow创建两个队列？ 文件名队列: 使用 tf.train.string_input_producer 传入：文件名list 重要参数：num_epochs , shuffle 内存队列: 不需要自己建立，只需要使用reader对象从文件名队列中读取数据就可以了 tf.train.start_queue_runners:使用tf.train.string_input_producer创建文件名队列后，整个系统还是停滞的，需要使用 tf.train.start_queue_runner才会启动填充队列的进程 读取示例假设当前文件夹已有：A.jpg,B.jpg，C.jpg,读取5个epoch并把读取的结果重新存入read文件夹 1234567891011121314151617181920212223242526272829# coding:utf-8import osif not os.path.exists('read'): os.makedirs('read/')# 导入TensorFlowimport tensorflow as tf # 新建一个Sessionwith tf.Session() as sess: # 我们要读三幅图片A.jpg, B.jpg, C.jpg filename = ['A.jpg', 'B.jpg', 'C.jpg'] # string_input_producer会产生一个文件名队列 filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5) # reader从文件名队列中读数据。对应的方法是reader.read reader = tf.WholeFileReader() key, value = reader.read(filename_queue) # tf.train.string_input_producer定义了一个epoch变量，要对它进行初始化 tf.local_variables_initializer().run() # 使用start_queue_runners之后，才会开始填充队列 threads = tf.train.start_queue_runners(sess=sess) i = 0 while True: i += 1 # 获取图片数据并保存 image_data = sess.run(value) with open('read/test_%d.jpg' % i, 'wb') as f: f.write(image_data)# 程序最后会抛出一个OutOfRangeError，这是epoch跑完，队列关闭的标志 结果如下： 将CIFAR-10数据保存为图片一个样本由3073个字节组成，第一个是标签，剩下的是图像数据。 如何使用TensorFlow读取数据？ 1.用tf.train.string_input_producer建立队列 2.通过reader.read读数据，一个文件就是一张图片，所以用tf.WholeFileReader(),这里的数据是固定字节的，所以要用tf.FixedLengthRecordReader() 3.调用tf.train.start_queue_runner 4.通过sess.run()取出图片结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#coding: utf-8# 导入当前目录的cifar10_input，这个模块负责读入cifar10数据import cifar10_input# 导入TensorFlow和其他一些可能用到的模块。import tensorflow as tfimport osimport scipy.miscdef inputs_origin(data_dir): # filenames一共5个，从data_batch_1.bin到data_batch_5.bin # 读入的都是训练图像 filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in range(1, 6)] # 判断文件是否存在 for f in filenames: if not tf.gfile.Exists(f): raise ValueError('Failed to find file: ' + f) # 将文件名的list包装成TensorFlow中queue的形式 filename_queue = tf.train.string_input_producer(filenames) # cifar10_input.read_cifar10是事先写好的从queue中读取文件的函数 # 返回的结果read_input的属性uint8image就是图像的Tensor read_input = cifar10_input.read_cifar10(filename_queue) # 将图片转换为实数形式 reshaped_image = tf.cast(read_input.uint8image, tf.float32) # 返回的reshaped_image是一张图片的tensor # 我们应当这样理解reshaped_image：每次使用sess.run(reshaped_image)，就会取出一张图片 return reshaped_imageif __name__ == '__main__': # 创建一个会话sess with tf.Session() as sess: # 调用inputs_origin。cifar10_data/cifar-10-batches-bin是我们下载的数据的文件夹位置 reshaped_image = inputs_origin('cifar10_data/cifar-10-batches-bin') # 这一步start_queue_runner很重要。 # 我们之前有filename_queue = tf.train.string_input_producer(filenames) # 这个queue必须通过start_queue_runners才能启动 # 缺少start_queue_runners程序将不能执行 threads = tf.train.start_queue_runners(sess=sess) # 变量初始化 sess.run(tf.global_variables_initializer()) # 创建文件夹cifar10_data/raw/ if not os.path.exists('cifar10_data/raw/'): os.makedirs('cifar10_data/raw/') # 保存30张图片 for i in range(30): # 每次sess.run(reshaped_image)，都会取出一张图片 image_array = sess.run(reshaped_image) # 将图片保存 scipy.misc.toimage(image_array).save('cifar10_data/raw/%d.jpg' % i) TensorFlow 训练 CIFAR-10数据增强常用方法 方法 介绍 平移： 一定尺度范围的平移 旋转： 一定尺度范围的旋转 翻转： 水平或垂直翻转 裁剪： 原有图像裁剪 缩放： 一定尺度的放大缩小 颜色变换 对图像的RGB颜色空间进行变换 噪声扰动 给图像加入一些人工产生的噪声 TensorFlow中数据增强的实现123456789101112131415# Randomly crop a [height, width] section of the image.随机裁剪distorted_image = tf.random_crop(reshaped_image, [height, width, 3])随机翻转# Randomly flip the image horizontally.distorted_image = tf.image.random_flip_left_right(distorted_image)# Because these operations are not commutative, consider randomizing# the order their operation.随机改变亮度和对比度distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8) CIFAR-10识别模型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def inference(images): """Build the CIFAR-10 model. Args: images: Images returned from distorted_inputs() or inputs(). Returns: Logits. """ # We instantiate all variables using tf.get_variable() instead of # tf.Variable() in order to share variables across multiple GPU training runs. # If we only ran this model on a single GPU, we could simplify this function # by replacing all instances of tf.get_variable() with tf.Variable(). # # conv1 with tf.variable_scope('conv1') as scope: kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0) conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME') biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0)) pre_activation = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(pre_activation, name=scope.name) _activation_summary(conv1) # pool1 pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1') # norm1 norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1') # conv2 with tf.variable_scope('conv2') as scope: kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0) conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME') biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1)) pre_activation = tf.nn.bias_add(conv, biases) conv2 = tf.nn.relu(pre_activation, name=scope.name) _activation_summary(conv2) # norm2 norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2') # pool2 pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2') # local3 with tf.variable_scope('local3') as scope: # Move everything into depth so we can perform a single matrix multiply. reshape = tf.reshape(pool2, [FLAGS.batch_size, -1]) dim = reshape.get_shape()[1].value weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004) biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1)) local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name) _activation_summary(local3) # local4 with tf.variable_scope('local4') as scope: weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004) biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1)) local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name) _activation_summary(local4) # linear layer(WX + b), # We don't apply softmax here because # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits # and performs the softmax internally for efficiency. with tf.variable_scope('softmax_linear') as scope: weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1/192.0, wd=0.0) biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0)) softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name) _activation_summary(softmax_linear) return softmax_linear 训练模型1python cifar10_train.py --train_dir cifar10_train/ --data_dir 在TensorFlow中查看训练进度要使用TensorBoard,打开另一个命令窗口，切换到当前目录，输入以下命令 1tensorboard --logdir cifar10_train 默认运行在6006端口，打开浏览器，输入http://127.0.0.1:6006或http://localhost:6006 注意：如果展开global_step对应的训练速度发生较大的变化，或者越来越慢，可能是程序出现了错误，需要进行检查 TensorBoard工作原理：在指定训练文件夹cifar10_train下，可以找到一个以events.out开头的文件，在训练过程中，程序将日志信息写入这个文件，运行TensorBoard只要指定训练文件夹，就会自动搜索这个文件，并在网页中显示 测试模型效果cifar10_eval.py --data_dir cifar10_data/ --eval_dir cifar10_eval/ --checkpoint_dir cifar10_train/```123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408&gt; 训练测试不能在同一块GPU上，可能会显存不足，使用另一张显卡的方法是设置不同的`CUDA_VISIBLE_DEVICES`,比如训练先运行：&gt; `export CUDA_VISIBLE_DEVICES=0`再进行训练，测试使用1就可以了# 附录## cifar10.py```python# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Builds the CIFAR-10 network.Summary of available functions: # Compute input images and labels for training. If you would like to run # evaluations, use inputs() instead. inputs, labels = distorted_inputs() # Compute inference on the model inputs to make a prediction. predictions = inference(inputs) # Compute the total loss of the prediction with respect to the labels. loss = loss(predictions, labels) # Create a graph to run one step of training with respect to the loss. train_op = train(loss, global_step)"""# pylint: disable=missing-docstringfrom __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport osimport reimport sysimport tarfilefrom six.moves import urllibimport tensorflow as tfimport cifar10_inputFLAGS = tf.app.flags.FLAGS# Basic model parameters.tf.app.flags.DEFINE_integer('batch_size', 128, """Number of images to process in a batch.""")tf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data', """Path to the CIFAR-10 data directory.""")tf.app.flags.DEFINE_boolean('use_fp16', False, """Train the model using fp16.""")# Global constants describing the CIFAR-10 data set.IMAGE_SIZE = cifar10_input.IMAGE_SIZENUM_CLASSES = cifar10_input.NUM_CLASSESNUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAINNUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL# Constants describing the training process.MOVING_AVERAGE_DECAY = 0.9999 # The decay to use for the moving average.NUM_EPOCHS_PER_DECAY = 350.0 # Epochs after which learning rate decays.LEARNING_RATE_DECAY_FACTOR = 0.1 # Learning rate decay factor.INITIAL_LEARNING_RATE = 0.1 # Initial learning rate.# If a model is trained with multiple GPUs, prefix all Op names with tower_name# to differentiate the operations. Note that this prefix is removed from the# names of the summaries when visualizing a model.TOWER_NAME = 'tower'DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'def _activation_summary(x): """Helper to create summaries for activations. Creates a summary that provides a histogram of activations. Creates a summary that measures the sparsity of activations. Args: x: Tensor Returns: nothing """ # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training # session. This helps the clarity of presentation on tensorboard. tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name) tf.summary.histogram(tensor_name + '/activations', x) tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))def _variable_on_cpu(name, shape, initializer): """Helper to create a Variable stored on CPU memory. Args: name: name of the variable shape: list of ints initializer: initializer for Variable Returns: Variable Tensor """ with tf.device('/cpu:0'): dtype = tf.float16 if FLAGS.use_fp16 else tf.float32 var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype) return vardef _variable_with_weight_decay(name, shape, stddev, wd): """Helper to create an initialized Variable with weight decay. Note that the Variable is initialized with a truncated normal distribution. A weight decay is added only if one is specified. Args: name: name of the variable shape: list of ints stddev: standard deviation of a truncated Gaussian wd: add L2Loss weight decay multiplied by this float. If None, weight decay is not added for this Variable. Returns: Variable Tensor """ dtype = tf.float16 if FLAGS.use_fp16 else tf.float32 var = _variable_on_cpu( name, shape, tf.truncated_normal_initializer(stddev=stddev, dtype=dtype)) if wd is not None: weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss') tf.add_to_collection('losses', weight_decay) return vardef distorted_inputs(): """Construct distorted input for CIFAR training using the Reader ops. Returns: images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size. labels: Labels. 1D tensor of [batch_size] size. Raises: ValueError: If no data_dir """ if not FLAGS.data_dir: raise ValueError('Please supply a data_dir') data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.distorted_inputs(data_dir=data_dir, batch_size=FLAGS.batch_size) if FLAGS.use_fp16: images = tf.cast(images, tf.float16) labels = tf.cast(labels, tf.float16) return images, labelsdef inputs(eval_data): """Construct input for CIFAR evaluation using the Reader ops. Args: eval_data: bool, indicating if one should use the train or eval data set. Returns: images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size. labels: Labels. 1D tensor of [batch_size] size. Raises: ValueError: If no data_dir """ if not FLAGS.data_dir: raise ValueError('Please supply a data_dir') data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin') images, labels = cifar10_input.inputs(eval_data=eval_data, data_dir=data_dir, batch_size=FLAGS.batch_size) if FLAGS.use_fp16: images = tf.cast(images, tf.float16) labels = tf.cast(labels, tf.float16) return images, labelsdef inference(images): """Build the CIFAR-10 model. Args: images: Images returned from distorted_inputs() or inputs(). Returns: Logits. """ # We instantiate all variables using tf.get_variable() instead of # tf.Variable() in order to share variables across multiple GPU training runs. # If we only ran this model on a single GPU, we could simplify this function # by replacing all instances of tf.get_variable() with tf.Variable(). # # conv1 with tf.variable_scope('conv1') as scope: kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0) conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME') biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0)) pre_activation = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(pre_activation, name=scope.name) _activation_summary(conv1) # pool1 pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1') # norm1 norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1') # conv2 with tf.variable_scope('conv2') as scope: kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0) conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME') biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1)) pre_activation = tf.nn.bias_add(conv, biases) conv2 = tf.nn.relu(pre_activation, name=scope.name) _activation_summary(conv2) # norm2 norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2') # pool2 pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2') # local3 with tf.variable_scope('local3') as scope: # Move everything into depth so we can perform a single matrix multiply. reshape = tf.reshape(pool2, [FLAGS.batch_size, -1]) dim = reshape.get_shape()[1].value weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004) biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1)) local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name) _activation_summary(local3) # local4 with tf.variable_scope('local4') as scope: weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004) biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1)) local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name) _activation_summary(local4) # linear layer(WX + b), # We don't apply softmax here because # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits # and performs the softmax internally for efficiency. with tf.variable_scope('softmax_linear') as scope: weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1/192.0, wd=0.0) biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0)) softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name) _activation_summary(softmax_linear) return softmax_lineardef loss(logits, labels): """Add L2Loss to all the trainable variables. Add summary for "Loss" and "Loss/avg". Args: logits: Logits from inference(). labels: Labels from distorted_inputs or inputs(). 1-D tensor of shape [batch_size] Returns: Loss tensor of type float. """ # Calculate the average cross entropy loss across the batch. labels = tf.cast(labels, tf.int64) cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits( labels=labels, logits=logits, name='cross_entropy_per_example') cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy') tf.add_to_collection('losses', cross_entropy_mean) # The total loss is defined as the cross entropy loss plus all of the weight # decay terms (L2 loss). return tf.add_n(tf.get_collection('losses'), name='total_loss')def _add_loss_summaries(total_loss): """Add summaries for losses in CIFAR-10 model. Generates moving average for all losses and associated summaries for visualizing the performance of the network. Args: total_loss: Total loss from loss(). Returns: loss_averages_op: op for generating moving averages of losses. """ # Compute the moving average of all individual losses and the total loss. loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg') losses = tf.get_collection('losses') loss_averages_op = loss_averages.apply(losses + [total_loss]) # Attach a scalar summary to all individual losses and the total loss; do the # same for the averaged version of the losses. for l in losses + [total_loss]: # Name each loss as '(raw)' and name the moving average version of the loss # as the original loss name. tf.summary.scalar(l.op.name + ' (raw)', l) tf.summary.scalar(l.op.name, loss_averages.average(l)) return loss_averages_opdef train(total_loss, global_step): """Train CIFAR-10 model. Create an optimizer and apply to all trainable variables. Add moving average for all trainable variables. Args: total_loss: Total loss from loss(). global_step: Integer Variable counting the number of training steps processed. Returns: train_op: op for training. """ # Variables that affect learning rate. num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY) # Decay the learning rate exponentially based on the number of steps. lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, decay_steps, LEARNING_RATE_DECAY_FACTOR, staircase=True) tf.summary.scalar('learning_rate', lr) # Generate moving averages of all losses and associated summaries. loss_averages_op = _add_loss_summaries(total_loss) # Compute gradients. with tf.control_dependencies([loss_averages_op]): opt = tf.train.GradientDescentOptimizer(lr) grads = opt.compute_gradients(total_loss) # Apply gradients. apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) # Add histograms for trainable variables. for var in tf.trainable_variables(): tf.summary.histogram(var.op.name, var) # Add histograms for gradients. for grad, var in grads: if grad is not None: tf.summary.histogram(var.op.name + '/gradients', grad) # Track the moving averages of all trainable variables. variable_averages = tf.train.ExponentialMovingAverage( MOVING_AVERAGE_DECAY, global_step) variables_averages_op = variable_averages.apply(tf.trainable_variables()) with tf.control_dependencies([apply_gradient_op, variables_averages_op]): train_op = tf.no_op(name='train') return train_opdef maybe_download_and_extract(): """Download and extract the tarball from Alex's website.""" dest_directory = FLAGS.data_dir if not os.path.exists(dest_directory): os.makedirs(dest_directory) filename = DATA_URL.split('/')[-1] filepath = os.path.join(dest_directory, filename) if not os.path.exists(filepath): def _progress(count, block_size, total_size): sys.stdout.write('\r&gt;&gt; Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0)) sys.stdout.flush() filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress) print() statinfo = os.stat(filepath) print('Successfully downloaded', filename, statinfo.st_size, 'bytes.') extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin') if not os.path.exists(extracted_dir_path): tarfile.open(filepath, 'r:gz').extractall(dest_directory) cifar10_download.py1234567891011121314# coding:utf-8# 引入当前目录中的已经编写好的cifar10模块import cifar10# 引入tensorflowimport tensorflow as tf# tf.app.flags.FLAGS是TensorFlow内部的一个全局变量存储器，同时可以用于命令行参数的处理FLAGS = tf.app.flags.FLAGS# 在cifar10模块中预先定义了f.app.flags.FLAGS.data_dir为CIFAR-10的数据路径# 我们把这个路径改为cifar10_dataFLAGS.data_dir = 'cifar10_data/'# 如果不存在数据文件，就会执行下载cifar10.maybe_download_and_extract() cifar10_eval.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Evaluation for CIFAR-10.Accuracy:cifar10_train.py achieves 83.0% accuracy after 100K steps (256 epochsof data) as judged by cifar10_eval.py.Speed:On a single Tesla K40, cifar10_train.py processes a single batch of 128 imagesin 0.25-0.35 sec (i.e. 350 - 600 images /sec). The model reaches ~86%accuracy after 100K steps in 8 hours of training time.Usage:Please see the tutorial and website for how to download the CIFAR-10data set, compile the program and train the model.http://tensorflow.org/tutorials/deep_cnn/"""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom datetime import datetimeimport mathimport timeimport numpy as npimport tensorflow as tfimport cifar10FLAGS = tf.app.flags.FLAGStf.app.flags.DEFINE_string('eval_dir', '/tmp/cifar10_eval', """Directory where to write event logs.""")tf.app.flags.DEFINE_string('eval_data', 'test', """Either 'test' or 'train_eval'.""")tf.app.flags.DEFINE_string('checkpoint_dir', '/tmp/cifar10_train', """Directory where to read model checkpoints.""")tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5, """How often to run the eval.""")tf.app.flags.DEFINE_integer('num_examples', 10000, """Number of examples to run.""")tf.app.flags.DEFINE_boolean('run_once', False, """Whether to run eval only once.""")def eval_once(saver, summary_writer, top_k_op, summary_op): """Run Eval once. Args: saver: Saver. summary_writer: Summary writer. top_k_op: Top K op. summary_op: Summary op. """ with tf.Session() as sess: ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: # Restores from checkpoint saver.restore(sess, ckpt.model_checkpoint_path) # Assuming model_checkpoint_path looks something like: # /my-favorite-path/cifar10_train/model.ckpt-0, # extract global_step from it. global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1] else: print('No checkpoint file found') return # Start the queue runners. coord = tf.train.Coordinator() try: threads = [] for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS): threads.extend(qr.create_threads(sess, coord=coord, daemon=True, start=True)) num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size)) true_count = 0 # Counts the number of correct predictions. total_sample_count = num_iter * FLAGS.batch_size step = 0 while step &lt; num_iter and not coord.should_stop(): predictions = sess.run([top_k_op]) true_count += np.sum(predictions) step += 1 # Compute precision @ 1. precision = true_count / total_sample_count print('%s: precision @ 1 = %.3f' % (datetime.now(), precision)) summary = tf.Summary() summary.ParseFromString(sess.run(summary_op)) summary.value.add(tag='Precision @ 1', simple_value=precision) summary_writer.add_summary(summary, global_step) except Exception as e: # pylint: disable=broad-except coord.request_stop(e) coord.request_stop() coord.join(threads, stop_grace_period_secs=10)def evaluate(): """Eval CIFAR-10 for a number of steps.""" with tf.Graph().as_default() as g: # Get images and labels for CIFAR-10. eval_data = FLAGS.eval_data == 'test' images, labels = cifar10.inputs(eval_data=eval_data) # Build a Graph that computes the logits predictions from the # inference model. logits = cifar10.inference(images) # Calculate predictions. top_k_op = tf.nn.in_top_k(logits, labels, 1) # Restore the moving average version of the learned variables for eval. variable_averages = tf.train.ExponentialMovingAverage( cifar10.MOVING_AVERAGE_DECAY) variables_to_restore = variable_averages.variables_to_restore() saver = tf.train.Saver(variables_to_restore) # Build the summary operation based on the TF collection of Summaries. summary_op = tf.summary.merge_all() summary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g) while True: eval_once(saver, summary_writer, top_k_op, summary_op) if FLAGS.run_once: break time.sleep(FLAGS.eval_interval_secs)def main(argv=None): # pylint: disable=unused-argument cifar10.maybe_download_and_extract() if tf.gfile.Exists(FLAGS.eval_dir): tf.gfile.DeleteRecursively(FLAGS.eval_dir) tf.gfile.MakeDirs(FLAGS.eval_dir) evaluate()if __name__ == '__main__': tf.app.run() cifar10_extract.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#coding: utf-8# 导入当前目录的cifar10_input，这个模块负责读入cifar10数据import cifar10_input# 导入TensorFlow和其他一些可能用到的模块。import tensorflow as tfimport osimport scipy.miscdef inputs_origin(data_dir): # filenames一共5个，从data_batch_1.bin到data_batch_5.bin # 读入的都是训练图像 filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in xrange(1, 6)] # 判断文件是否存在 for f in filenames: if not tf.gfile.Exists(f): raise ValueError('Failed to find file: ' + f) # 将文件名的list包装成TensorFlow中queue的形式 filename_queue = tf.train.string_input_producer(filenames) # cifar10_input.read_cifar10是事先写好的从queue中读取文件的函数 # 返回的结果read_input的属性uint8image就是图像的Tensor read_input = cifar10_input.read_cifar10(filename_queue) # 将图片转换为实数形式 reshaped_image = tf.cast(read_input.uint8image, tf.float32) # 返回的reshaped_image是一张图片的tensor # 我们应当这样理解reshaped_image：每次使用sess.run(reshaped_image)，就会取出一张图片 return reshaped_imageif __name__ == '__main__': # 创建一个会话sess with tf.Session() as sess: # 调用inputs_origin。cifar10_data/cifar-10-batches-bin是我们下载的数据的文件夹位置 reshaped_image = inputs_origin('cifar10_data/cifar-10-batches-bin') # 这一步start_queue_runner很重要。 # 我们之前有filename_queue = tf.train.string_input_producer(filenames) # 这个queue必须通过start_queue_runners才能启动 # 缺少start_queue_runners程序将不能执行 threads = tf.train.start_queue_runners(sess=sess) # 变量初始化 sess.run(tf.global_variables_initializer()) # 创建文件夹cifar10_data/raw/ if not os.path.exists('cifar10_data/raw/'): os.makedirs('cifar10_data/raw/') # 保存30张图片 for i in range(30): # 每次sess.run(reshaped_image)，都会取出一张图片 image_array = sess.run(reshaped_image) # 将图片保存 scipy.misc.toimage(image_array).save('cifar10_data/raw/%d.jpg' % i) cifar10_input.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Routine for decoding the CIFAR-10 binary file format."""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport osfrom six.moves import xrange # pylint: disable=redefined-builtinimport tensorflow as tf# Process images of this size. Note that this differs from the original CIFAR# image size of 32 x 32. If one alters this number, then the entire model# architecture will change and any model would need to be retrained.IMAGE_SIZE = 24# Global constants describing the CIFAR-10 data set.NUM_CLASSES = 10NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000def read_cifar10(filename_queue): """Reads and parses examples from CIFAR10 data files. Recommendation: if you want N-way read parallelism, call this function N times. This will give you N independent Readers reading different files &amp; positions within those files, which will give better mixing of examples. Args: filename_queue: A queue of strings with the filenames to read from. Returns: An object representing a single example, with the following fields: height: number of rows in the result (32) width: number of columns in the result (32) depth: number of color channels in the result (3) key: a scalar string Tensor describing the filename &amp; record number for this example. label: an int32 Tensor with the label in the range 0..9. uint8image: a [height, width, depth] uint8 Tensor with the image data """ class CIFAR10Record(object): pass result = CIFAR10Record() # Dimensions of the images in the CIFAR-10 dataset. # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the # input format. label_bytes = 1 # 2 for CIFAR-100 result.height = 32 result.width = 32 result.depth = 3 image_bytes = result.height * result.width * result.depth # Every record consists of a label followed by the image, with a # fixed number of bytes for each. record_bytes = label_bytes + image_bytes # Read a record, getting filenames from the filename_queue. No # header or footer in the CIFAR-10 format, so we leave header_bytes # and footer_bytes at their default of 0. reader = tf.FixedLengthRecordReader(record_bytes=record_bytes) result.key, value = reader.read(filename_queue) # Convert from a string to a vector of uint8 that is record_bytes long. record_bytes = tf.decode_raw(value, tf.uint8) # The first bytes represent the label, which we convert from uint8-&gt;int32. result.label = tf.cast( tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32) # The remaining bytes after the label represent the image, which we reshape # from [depth * height * width] to [depth, height, width]. depth_major = tf.reshape( tf.strided_slice(record_bytes, [label_bytes], [label_bytes + image_bytes]), [result.depth, result.height, result.width]) # Convert from [depth, height, width] to [height, width, depth]. result.uint8image = tf.transpose(depth_major, [1, 2, 0]) return resultdef _generate_image_and_label_batch(image, label, min_queue_examples, batch_size, shuffle): """Construct a queued batch of images and labels. Args: image: 3-D Tensor of [height, width, 3] of type.float32. label: 1-D Tensor of type.int32 min_queue_examples: int32, minimum number of samples to retain in the queue that provides of batches of examples. batch_size: Number of images per batch. shuffle: boolean indicating whether to use a shuffling queue. Returns: images: Images. 4D tensor of [batch_size, height, width, 3] size. labels: Labels. 1D tensor of [batch_size] size. """ # Create a queue that shuffles the examples, and then # read 'batch_size' images + labels from the example queue. num_preprocess_threads = 16 if shuffle: images, label_batch = tf.train.shuffle_batch( [image, label], batch_size=batch_size, num_threads=num_preprocess_threads, capacity=min_queue_examples + 3 * batch_size, min_after_dequeue=min_queue_examples) else: images, label_batch = tf.train.batch( [image, label], batch_size=batch_size, num_threads=num_preprocess_threads, capacity=min_queue_examples + 3 * batch_size) # Display the training images in the visualizer. tf.summary.image('images', images) return images, tf.reshape(label_batch, [batch_size])def distorted_inputs(data_dir, batch_size): """Construct distorted input for CIFAR training using the Reader ops. Args: data_dir: Path to the CIFAR-10 data directory. batch_size: Number of images per batch. Returns: images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size. labels: Labels. 1D tensor of [batch_size] size. """ filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in xrange(1, 6)] for f in filenames: if not tf.gfile.Exists(f): raise ValueError('Failed to find file: ' + f) # Create a queue that produces the filenames to read. filename_queue = tf.train.string_input_producer(filenames) # Read examples from files in the filename queue. read_input = read_cifar10(filename_queue) reshaped_image = tf.cast(read_input.uint8image, tf.float32) height = IMAGE_SIZE width = IMAGE_SIZE # Image processing for training the network. Note the many random # distortions applied to the image. # Randomly crop a [height, width] section of the image. distorted_image = tf.random_crop(reshaped_image, [height, width, 3]) # Randomly flip the image horizontally. distorted_image = tf.image.random_flip_left_right(distorted_image) # Because these operations are not commutative, consider randomizing # the order their operation. distorted_image = tf.image.random_brightness(distorted_image, max_delta=63) distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8) # Subtract off the mean and divide by the variance of the pixels. float_image = tf.image.per_image_standardization(distorted_image) # Set the shapes of tensors. float_image.set_shape([height, width, 3]) read_input.label.set_shape([1]) # Ensure that the random shuffling has good mixing properties. min_fraction_of_examples_in_queue = 0.4 min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_examples_in_queue) print('Filling queue with %d CIFAR images before starting to train. ' 'This will take a few minutes.' % min_queue_examples) # Generate a batch of images and labels by building up a queue of examples. return _generate_image_and_label_batch(float_image, read_input.label, min_queue_examples, batch_size, shuffle=True)def inputs(eval_data, data_dir, batch_size): """Construct input for CIFAR evaluation using the Reader ops. Args: eval_data: bool, indicating if one should use the train or eval data set. data_dir: Path to the CIFAR-10 data directory. batch_size: Number of images per batch. Returns: images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size. labels: Labels. 1D tensor of [batch_size] size. """ if not eval_data: filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i) for i in xrange(1, 6)] num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN else: filenames = [os.path.join(data_dir, 'test_batch.bin')] num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL for f in filenames: if not tf.gfile.Exists(f): raise ValueError('Failed to find file: ' + f) # Create a queue that produces the filenames to read. filename_queue = tf.train.string_input_producer(filenames) # Read examples from files in the filename queue. read_input = read_cifar10(filename_queue) reshaped_image = tf.cast(read_input.uint8image, tf.float32) height = IMAGE_SIZE width = IMAGE_SIZE # Image processing for evaluation. # Crop the central [height, width] of the image. resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, height, width) # Subtract off the mean and divide by the variance of the pixels. float_image = tf.image.per_image_standardization(resized_image) # Set the shapes of tensors. float_image.set_shape([height, width, 3]) read_input.label.set_shape([1]) # Ensure that the random shuffling has good mixing properties. min_fraction_of_examples_in_queue = 0.4 min_queue_examples = int(num_examples_per_epoch * min_fraction_of_examples_in_queue) # Generate a batch of images and labels by building up a queue of examples. return _generate_image_and_label_batch(float_image, read_input.label, min_queue_examples, batch_size, shuffle=False) cifar10_input_test.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""Tests for cifar10 input."""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport osimport tensorflow as tfimport cifar10_inputclass CIFAR10InputTest(tf.test.TestCase): def _record(self, label, red, green, blue): image_size = 32 * 32 record = bytes(bytearray([label] + [red] * image_size + [green] * image_size + [blue] * image_size)) expected = [[[red, green, blue]] * 32] * 32 return record, expected def testSimple(self): labels = [9, 3, 0] records = [self._record(labels[0], 0, 128, 255), self._record(labels[1], 255, 0, 1), self._record(labels[2], 254, 255, 0)] contents = b"".join([record for record, _ in records]) expected = [expected for _, expected in records] filename = os.path.join(self.get_temp_dir(), "cifar") open(filename, "wb").write(contents) with self.test_session() as sess: q = tf.FIFOQueue(99, [tf.string], shapes=()) q.enqueue([filename]).run() q.close().run() result = cifar10_input.read_cifar10(q) for i in range(3): key, label, uint8image = sess.run([ result.key, result.label, result.uint8image]) self.assertEqual("%s:%d" % (filename, i), tf.compat.as_text(key)) self.assertEqual(labels[i], label) self.assertAllEqual(expected[i], uint8image) with self.assertRaises(tf.errors.OutOfRangeError): sess.run([result.key, result.uint8image])if __name__ == "__main__": tf.test.main() cifar10_multi_gpu_train.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""A binary to train CIFAR-10 using multiple GPUs with synchronous updates.Accuracy:cifar10_multi_gpu_train.py achieves ~86% accuracy after 100K steps (256epochs of data) as judged by cifar10_eval.py.Speed: With batch_size 128.System | Step Time (sec/batch) | Accuracy--------------------------------------------------------------------1 Tesla K20m | 0.35-0.60 | ~86% at 60K steps (5 hours)1 Tesla K40m | 0.25-0.35 | ~86% at 100K steps (4 hours)2 Tesla K20m | 0.13-0.20 | ~84% at 30K steps (2.5 hours)3 Tesla K20m | 0.13-0.18 | ~84% at 30K steps4 Tesla K20m | ~0.10 | ~84% at 30K stepsUsage:Please see the tutorial and website for how to download the CIFAR-10data set, compile the program and train the model.http://tensorflow.org/tutorials/deep_cnn/"""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom datetime import datetimeimport os.pathimport reimport timeimport numpy as npfrom six.moves import xrange # pylint: disable=redefined-builtinimport tensorflow as tfimport cifar10FLAGS = tf.app.flags.FLAGStf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train', """Directory where to write event logs """ """and checkpoint.""")tf.app.flags.DEFINE_integer('max_steps', 1000000, """Number of batches to run.""")tf.app.flags.DEFINE_integer('num_gpus', 1, """How many GPUs to use.""")tf.app.flags.DEFINE_boolean('log_device_placement', False, """Whether to log device placement.""")def tower_loss(scope): """Calculate the total loss on a single tower running the CIFAR model. Args: scope: unique prefix string identifying the CIFAR tower, e.g. 'tower_0' Returns: Tensor of shape [] containing the total loss for a batch of data """ # Get images and labels for CIFAR-10. images, labels = cifar10.distorted_inputs() # Build inference Graph. logits = cifar10.inference(images) # Build the portion of the Graph calculating the losses. Note that we will # assemble the total_loss using a custom function below. _ = cifar10.loss(logits, labels) # Assemble all of the losses for the current tower only. losses = tf.get_collection('losses', scope) # Calculate the total loss for the current tower. total_loss = tf.add_n(losses, name='total_loss') # Attach a scalar summary to all individual losses and the total loss; do the # same for the averaged version of the losses. for l in losses + [total_loss]: # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training # session. This helps the clarity of presentation on tensorboard. loss_name = re.sub('%s_[0-9]*/' % cifar10.TOWER_NAME, '', l.op.name) tf.summary.scalar(loss_name, l) return total_lossdef average_gradients(tower_grads): """Calculate the average gradient for each shared variable across all towers. Note that this function provides a synchronization point across all towers. Args: tower_grads: List of lists of (gradient, variable) tuples. The outer list is over individual gradients. The inner list is over the gradient calculation for each tower. Returns: List of pairs of (gradient, variable) where the gradient has been averaged across all towers. """ average_grads = [] for grad_and_vars in zip(*tower_grads): # Note that each grad_and_vars looks like the following: # ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN)) grads = [] for g, _ in grad_and_vars: # Add 0 dimension to the gradients to represent the tower. expanded_g = tf.expand_dims(g, 0) # Append on a 'tower' dimension which we will average over below. grads.append(expanded_g) # Average over the 'tower' dimension. grad = tf.concat(axis=0, values=grads) grad = tf.reduce_mean(grad, 0) # Keep in mind that the Variables are redundant because they are shared # across towers. So .. we will just return the first tower's pointer to # the Variable. v = grad_and_vars[0][1] grad_and_var = (grad, v) average_grads.append(grad_and_var) return average_gradsdef train(): """Train CIFAR-10 for a number of steps.""" with tf.Graph().as_default(), tf.device('/cpu:0'): # Create a variable to count the number of train() calls. This equals the # number of batches processed * FLAGS.num_gpus. global_step = tf.get_variable( 'global_step', [], initializer=tf.constant_initializer(0), trainable=False) # Calculate the learning rate schedule. num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size) decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY) # Decay the learning rate exponentially based on the number of steps. lr = tf.train.exponential_decay(cifar10.INITIAL_LEARNING_RATE, global_step, decay_steps, cifar10.LEARNING_RATE_DECAY_FACTOR, staircase=True) # Create an optimizer that performs gradient descent. opt = tf.train.GradientDescentOptimizer(lr) # Calculate the gradients for each model tower. tower_grads = [] with tf.variable_scope(tf.get_variable_scope()): for i in xrange(FLAGS.num_gpus): with tf.device('/gpu:%d' % i): with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope: # Calculate the loss for one tower of the CIFAR model. This function # constructs the entire CIFAR model but shares the variables across # all towers. loss = tower_loss(scope) # Reuse variables for the next tower. tf.get_variable_scope().reuse_variables() # Retain the summaries from the final tower. summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope) # Calculate the gradients for the batch of data on this CIFAR tower. grads = opt.compute_gradients(loss) # Keep track of the gradients across all towers. tower_grads.append(grads) # We must calculate the mean of each gradient. Note that this is the # synchronization point across all towers. grads = average_gradients(tower_grads) # Add a summary to track the learning rate. summaries.append(tf.summary.scalar('learning_rate', lr)) # Add histograms for gradients. for grad, var in grads: if grad is not None: summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad)) # Apply the gradients to adjust the shared variables. apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) # Add histograms for trainable variables. for var in tf.trainable_variables(): summaries.append(tf.summary.histogram(var.op.name, var)) # Track the moving averages of all trainable variables. variable_averages = tf.train.ExponentialMovingAverage( cifar10.MOVING_AVERAGE_DECAY, global_step) variables_averages_op = variable_averages.apply(tf.trainable_variables()) # Group all updates to into a single train op. train_op = tf.group(apply_gradient_op, variables_averages_op) # Create a saver. saver = tf.train.Saver(tf.global_variables()) # Build the summary operation from the last tower summaries. summary_op = tf.summary.merge(summaries) # Build an initialization operation to run below. init = tf.global_variables_initializer() # Start running operations on the Graph. allow_soft_placement must be set to # True to build towers on GPU, as some of the ops do not have GPU # implementations. sess = tf.Session(config=tf.ConfigProto( allow_soft_placement=True, log_device_placement=FLAGS.log_device_placement)) sess.run(init) # Start the queue runners. tf.train.start_queue_runners(sess=sess) summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph) for step in xrange(FLAGS.max_steps): start_time = time.time() _, loss_value = sess.run([train_op, loss]) duration = time.time() - start_time assert not np.isnan(loss_value), 'Model diverged with loss = NaN' if step % 10 == 0: num_examples_per_step = FLAGS.batch_size * FLAGS.num_gpus examples_per_sec = num_examples_per_step / duration sec_per_batch = duration / FLAGS.num_gpus format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f ' 'sec/batch)') print (format_str % (datetime.now(), step, loss_value, examples_per_sec, sec_per_batch)) if step % 100 == 0: summary_str = sess.run(summary_op) summary_writer.add_summary(summary_str, step) # Save the model checkpoint periodically. if step % 1000 == 0 or (step + 1) == FLAGS.max_steps: checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt') saver.save(sess, checkpoint_path, global_step=step)def main(argv=None): # pylint: disable=unused-argument cifar10.maybe_download_and_extract() if tf.gfile.Exists(FLAGS.train_dir): tf.gfile.DeleteRecursively(FLAGS.train_dir) tf.gfile.MakeDirs(FLAGS.train_dir) train()if __name__ == '__main__': tf.app.run() cifar10_train.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# =============================================================================="""A binary to train CIFAR-10 using a single GPU.Accuracy:cifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs ofdata) as judged by cifar10_eval.py.Speed: With batch_size 128.System | Step Time (sec/batch) | Accuracy------------------------------------------------------------------1 Tesla K20m | 0.35-0.60 | ~86% at 60K steps (5 hours)1 Tesla K40m | 0.25-0.35 | ~86% at 100K steps (4 hours)Usage:Please see the tutorial and website for how to download the CIFAR-10data set, compile the program and train the model.http://tensorflow.org/tutorials/deep_cnn/"""from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom datetime import datetimeimport timeimport tensorflow as tfimport cifar10FLAGS = tf.app.flags.FLAGStf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train', """Directory where to write event logs """ """and checkpoint.""")tf.app.flags.DEFINE_integer('max_steps', 1000000, """Number of batches to run.""")tf.app.flags.DEFINE_boolean('log_device_placement', False, """Whether to log device placement.""")tf.app.flags.DEFINE_integer('log_frequency', 10, """How often to log results to the console.""")def train(): """Train CIFAR-10 for a number of steps.""" with tf.Graph().as_default(): global_step = tf.contrib.framework.get_or_create_global_step() # Get images and labels for CIFAR-10. images, labels = cifar10.distorted_inputs() # Build a Graph that computes the logits predictions from the # inference model. logits = cifar10.inference(images) # Calculate loss. loss = cifar10.loss(logits, labels) # Build a Graph that trains the model with one batch of examples and # updates the model parameters. train_op = cifar10.train(loss, global_step) class _LoggerHook(tf.train.SessionRunHook): """Logs loss and runtime.""" def begin(self): self._step = -1 self._start_time = time.time() def before_run(self, run_context): self._step += 1 return tf.train.SessionRunArgs(loss) # Asks for loss value. def after_run(self, run_context, run_values): if self._step % FLAGS.log_frequency == 0: current_time = time.time() duration = current_time - self._start_time self._start_time = current_time loss_value = run_values.results examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration sec_per_batch = float(duration / FLAGS.log_frequency) format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f ' 'sec/batch)') print(format_str % (datetime.now(), self._step, loss_value, examples_per_sec, sec_per_batch)) with tf.train.MonitoredTrainingSession( checkpoint_dir=FLAGS.train_dir, hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps), tf.train.NanTensorHook(loss), _LoggerHook()], config=tf.ConfigProto( log_device_placement=FLAGS.log_device_placement)) as mon_sess: while not mon_sess.should_stop(): mon_sess.run(train_op)def main(argv=None): # pylint: disable=unused-argument cifar10.maybe_download_and_extract() if tf.gfile.Exists(FLAGS.train_dir): tf.gfile.DeleteRecursively(FLAGS.train_dir) tf.gfile.MakeDirs(FLAGS.train_dir) train()if __name__ == '__main__': tf.app.run() test.py1234567891011121314151617181920212223242526272829# coding:utf-8import osif not os.path.exists('read'): os.makedirs('read/')# 导入TensorFlowimport tensorflow as tf # 新建一个Sessionwith tf.Session() as sess: # 我们要读三幅图片A.jpg, B.jpg, C.jpg filename = ['A.jpg', 'B.jpg', 'C.jpg'] # string_input_producer会产生一个文件名队列 filename_queue = tf.train.string_input_producer(filename, shuffle=False, num_epochs=5) # reader从文件名队列中读数据。对应的方法是reader.read reader = tf.WholeFileReader() key, value = reader.read(filename_queue) # tf.train.string_input_producer定义了一个epoch变量，要对它进行初始化 tf.local_variables_initializer().run() # 使用start_queue_runners之后，才会开始填充队列 threads = tf.train.start_queue_runners(sess=sess) i = 0 while True: i += 1 # 获取图片数据并保存 image_data = sess.run(value) with open('read/test_%d.jpg' % i, 'wb') as f: f.write(image_data)# 程序最后会抛出一个OutOfRangeError，这是epoch跑完，队列关闭的标志]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[1.21个项目玩转深度学习之MNIST机器学习入门.md]]></title>
    <url>%2Ftensorflow%2F1.21%E4%B8%AA%E9%A1%B9%E7%9B%AE%E7%8E%A9%E8%BD%AC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8BMNIST%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8.html</url>
    <content type="text"><![CDATA[MNIST 机器学习入门导入数据123456789# 导入TensorFlow和tf.kerasimport tensorflow as tffrom tensorflow import keras# 导入辅助库import numpy as npimport matplotlib.pyplot as pltprint(tf.__version__) 1.14.0123fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 3s 85us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 306s 12us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 29us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 19s 4us/step图像是28x28 NumPy数组，像素值介于0到255之间。labels是一个整数数组，数值介于0到9之间。这对应了图像所代表的服装的类别: 标签 类别 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot 12class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] 探索数据1train_images.shape (60000, 28, 28)1len(train_labels) 600001train_labels array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)1test_images.shape (10000, 28, 28)1len(test_labels) 10000数据预处理12345plt.figure()plt.imshow(train_images[0])plt.colorbar()plt.grid(False)plt.show() 123train_images = train_images / 255.0test_images = test_images / 255.0 123456789plt.figure(figsize=(10,10))for i in range(25): plt.subplot(5,5,i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.binary) plt.xlabel(class_names[train_labels[i]])plt.show() 构建模型设置网络层12345model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)), keras.layers.Dense(128, activation=tf.nn.relu), keras.layers.Dense(10, activation=tf.nn.softmax)]) WARNING: Logging before flag parsing goes to stderr. W0916 22:23:25.715616 9456 deprecation.py:506] From H:\blog_anaconda_use\anaconda\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version. Instructions for updating: Call initializer instance with the dtype argument instead of passing it to the constructor网络中的第一层, tf.keras.layers.Flatten, 将图像格式从一个二维数组(包含着28x28个像素)转换成为一个包含着28 * 28 = 784个像素的一维数组。可以将这个网络层视为它将图像中未堆叠的像素排列在一起。这个网络层没有需要学习的参数;它仅仅对数据进行格式化。 在像素被展平之后，网络由一个包含有两个tf.keras.layers.Dense网络层的序列组成。他们被称作稠密链接层或全连接层。 第一个Dense网络层包含有128个节点(或被称为神经元)。第二个(也是最后一个)网络层是一个包含10个节点的softmax层—它将返回包含10个概率分数的数组，总和为1。每个节点包含一个分数，表示当前图像属于10个类别之一的概率。 编译模型123model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) 训练模型训练神经网络模型需要以下步骤: 将训练数据提供给模型 - 在本案例中，他们是train_images和train_labels数组。模型学习如何将图像与其标签关联我们使用模型对测试集进行预测, 在本案例中为test_images数组。我们验证预测结果是否匹配test_labels数组中保存的标签。通过调用model.fit方法来训练模型 — 模型对训练数据进行”拟合”。 1model.fit(train_images, train_labels, epochs=5) Epoch 1/5 60000/60000 [==============================] - 11s 187us/sample - loss: 0.5008 - acc: 0.8248 Epoch 2/5 60000/60000 [==============================] - 9s 150us/sample - loss: 0.3785 - acc: 0.8651 Epoch 3/5 60000/60000 [==============================] - 10s 167us/sample - loss: 0.3386 - acc: 0.8767 Epoch 4/5 60000/60000 [==============================] - 7s 118us/sample - loss: 0.3147 - acc: 0.8846 Epoch 5/5 60000/60000 [==============================] - 7s 125us/sample - loss: 0.2962 - acc: 0.8915 &lt;tensorflow.python.keras.callbacks.History at 0x1a712362048&gt;评估准确率123test_loss, test_acc = model.evaluate(test_images, test_labels)print('Test accuracy:', test_acc) 10000/10000 [==============================] - 1s 116us/sample - loss: 0.3490 - acc: 0.8744 Test accuracy: 0.8744进行预测1predictions = model.predict(test_images) 1predictions[0] array([2.9816958e-06, 3.6195729e-08, 6.0329114e-08, 1.6594480e-08, 4.0449553e-08, 1.8102061e-03, 9.0025031e-07, 2.4079284e-02, 4.8888058e-05, 9.7405756e-01], dtype=float32)1np.argmax(predictions[0]) 91test_labels[0] 9我们可以用图表来查看全部10个类别 123456789101112131415161718192021222324252627282930def plot_image(i, predictions_array, true_label, img): predictions_array, true_label, img = predictions_array[i], true_label[i], img[i] plt.grid(False) plt.xticks([]) plt.yticks([]) plt.imshow(img, cmap=plt.cm.binary) predicted_label = np.argmax(predictions_array) if predicted_label == true_label: color = 'blue' else: color = 'red' plt.xlabel("&#123;&#125; &#123;:2.0f&#125;% (&#123;&#125;)".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label]), color=color)def plot_value_array(i, predictions_array, true_label): predictions_array, true_label = predictions_array[i], true_label[i] plt.grid(False) plt.xticks([]) plt.yticks([]) thisplot = plt.bar(range(10), predictions_array, color="#777777") plt.ylim([0, 1]) predicted_label = np.argmax(predictions_array) thisplot[predicted_label].set_color('red') thisplot[true_label].set_color('blue') 1234567i = 0plt.figure(figsize=(6,3))plt.subplot(1,2,1)plot_image(i, predictions, test_labels, test_images)plt.subplot(1,2,2)plot_value_array(i, predictions, test_labels)plt.show() 1234567i = 12plt.figure(figsize=(6,3))plt.subplot(1,2,1)plot_image(i, predictions, test_labels, test_images)plt.subplot(1,2,2)plot_value_array(i, predictions, test_labels)plt.show() 123456789101112# 绘制前X个测试图像，预测标签和真实标签# 以蓝色显示正确的预测，红色显示不正确的预测num_rows = 5num_cols = 3num_images = num_rows*num_colsplt.figure(figsize=(2*2*num_cols, 2*num_rows))for i in range(num_images): plt.subplot(num_rows, 2*num_cols, 2*i+1) plot_image(i, predictions, test_labels, test_images) plt.subplot(num_rows, 2*num_cols, 2*i+2) plot_value_array(i, predictions, test_labels)plt.show() 最后，使用训练的模型对单个图像进行预测。 1234# 从测试数据集中获取图像img = test_images[0]print(img.shape) (28, 28)tf.keras模型经过优化，可以一次性对批量,或者一个集合的数据进行预测。因此，即使我们使用单个图像，我们也需要将其添加到列表中: 1234# 将图像添加到批次中，即使它是唯一的成员。img = (np.expand_dims(img,0))print(img.shape) (1, 28, 28)123predictions_single = model.predict(img)print(predictions_single) [[2.9817018e-06 3.6195665e-08 6.0329015e-08 1.6594482e-08 4.0449557e-08 1.8102063e-03 9.0025043e-07 2.4079263e-02 4.8888065e-05 9.7405767e-01]]123plot_value_array(0, predictions_single, test_labels)plt.xticks(range(10), class_names, rotation=45)plt.show() 12prediction_result = np.argmax(predictions_single[0])print(prediction_result) 9]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于OpenCV和YOLOv3深度学习的目标检测]]></title>
    <url>%2Freading%2F%E5%9F%BA%E4%BA%8EOpenCVh%E5%92%8CYOLO-V3%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%AE%9E%E7%8E%B0.html</url>
    <content type="text"><![CDATA[本文翻译自Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ ) 本文搬运自：https://blog.csdn.net/qq_27158179/article/details/81915740 基于OpenCV和YOLOv3深度学习的目标检测 从OpenCV 3.4.2开始，我们可以很容易的在OpenCV应用中使用YOLOv3模型（即OpemCV-3.4.2开始支持YOLOv3这网络框架）。 YOLO是什么原理？我们可以把目标检测看成是目标定位和目标识别的结合。 在传统的计算机视觉方法中，采用滑动窗口查找不同区域和大小的目标。因为这是消耗量较大的算法，通常假定目标的纵横比是固定的。 早期的基于深度学习的目标检测算法，如R-CNN和快速R-CNN，采用选择型搜索（Selective Search）来缩小必须测试的边界框的数量（本文的边界框指的是，在预测到疑似所识别到的目标后，在图片上把对象框出的一个矩形）。 另外一种称为Overfeat的方法，通过卷积地计算滑动窗口，以多个尺度扫描了图像。 然后有人提出了快速R-CNN算法，使用Region Proposal Network(RPN)区别将要测试的边界框。通过巧妙的设计，用于目标识别的特征点，也被RPN用于提出潜在的边界框，因此节省了大量的计算。 然而，YOLO使用了完全不同的方法解决目标检测问题。它将图像进行神经网络的一次性正向处理。SSD是另外一种将图像进行神经网络一次性正向处理的方法，但是YOLOv3比SSD实现了更高的精度，同时又较快的运算速度。YOLOv3在M40，TitanX和1080Ti这类GPU上实时效果更好。 让我们看看YOLO如何在一张图片中检测目标。 首先，它把原图按比例平均分解成一张有13x13网格的图片。这169个单元会根据原图的大小而改变。对于一张416x416像素的图片，每个图片单元的大小是32x32像素。处理图片时，会以图片单元为单位，预测单位中的多个边界框。 对于每个边界框，这个网络会计算所包含物体的边界框的置信度，同时计算所包含的目标是属于一个特定类别的可能性大小。 非最大抑制（non-maximum suppression）可以消除低置信度的边界框，以及把同时包围着单个物体的多个高置信度的边界框消除到只剩下一个。 YOLOv3的作者，Joseph Redmon和Ali Farhadi，让YOLOv3比前一代YOLOv2更加精确和快速。YOLOv3在处理多个不同尺寸图片的场合中得到了优化。他们还通过加大了网络，并添加快捷链接将其引入剩余网络来改进网络。 为什么选择OpenCV的YOLO 这里有三个理由。 容易整合到现有的OpenCV程序中：如果应用程序已经使用了OpenCV，并想简单地使用YOLOv3，完全不需要担心Darknet源代码的编译和建立。 OpenCV的CPU版本的运算速度比Darknet+OpenMP快9倍：OpenCV的DNN模块，其CPU运行是十分快的。举个例子，当用了OpenMP的Darknet在CPU上处理一张图片消耗2秒，OpenCV的实现只需要0.22秒。具体请看下面的表格。 支持Python。Darknet是用C语言写的，因此并不支持Python。相反，OpenCV是支持Python的。会有支持Darknet的编程接口。 在Darknet和OpenCV上跑YOLOv3的速度测试下面的表格展示了在Darknet和OpenCV上YOLOv3的性能差距，输入图片的尺寸是416x416。不出所料，GPU版本的Darknet在性能上比其他方式优越。同时，理所当然的Darknet配合OpenMP会好于没有OpenMP的Darknet，因为OpenMP支持多核的CPU。 意外的是，CPU版本的OpenCV在执行DNN的运算速度，是9倍的快过Darknet和OpenML OS Framework CPU/GPU Time(ms)/Frame Linux 16.04 Darknet 12x Intel Core i7-6850K CPU @ 3.60GHz 9370 Linux 16.04 Darknet + OpenMP 12x Intel Core i7-6850K CPU @ 3.60GHz 1942 Linux 16.04 OpenCV [CPU] 12x Intel Core i7-6850K CPU @ 3.60GHz 220 Linux 16.04 Darknet NVIDIA GeForce 1080 Ti GPU 23 macOS DarkNet 2.5 GHz Intel Core i7 CPU 7260 macOS OpenCV [CPU] 2.5 GHz Intel Core i7 CPU 400 采用YOLOv3的目标检测，C++/Python两种语言第1步：下载模型1234cd 到wget安装目录，执行wget https://pjreddie.com/media/files/yolov3.weightswget https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true -O ./yolov3.cfgwget https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true -O ./coco.names 执行命令后开始下载yolov3.weights文件（包括了提前训练好的网络的权值），和yolov3.cfg文件（包含了网络的配置方式）和coco.names（包括了COCO数据库中使用的80种不同的目标种类名字）。 所以如果是自己训练的模型，也就同样把：权重文件，网络配置文件，标签文件拷贝过来就可以了 第2步：初始化参数YOLOv3算法的预测结果就是边界框。每一个边界框都旁随着一个置信值。第一阶段中，全部低于置信度阀值的都会排除掉。 对剩余的边界框执行非最大抑制算法，以去除重叠的边界框。非最大抑制由一个参数nmsThrehold控制。读者可以尝试改变这个数值，观察输出的边界框的改变。 接下来，设置输入图片的宽度（inpWidth）和高度（inpHeight）。我们设置他们为416，以便对比YOLOv3作者提供的Darknets的C代码。如果想要更快的速度，读者可以把宽度和高度设置为320。如果想要更准确的结果，改变他们到608。 Python代码：12345# Initialize the parametersconfThreshold = 0.5 #Confidence thresholdnmsThreshold = 0.4 #Non-maximum suppression thresholdinpWidth = 416 #Width of network's input imageinpHeight = 416 #Height of network's input image C++代码：12345// Initialize the parametersfloat confThreshold = 0.5; // Confidence thresholdfloat nmsThreshold = 0.4; // Non-maximum suppression thresholdint inpWidth = 416; // Width of network's input imageint inpHeight = 416; // Height of network's input image 第3步：读取模型和类别文件coco.names包含了训练好的模型能识别的所有目标名字。我们读出各个类别的名字。 接着，我们读取了网络，其包含两个部分： yolov3.weights: 预训练得到的权重。 yolov3.cfg：配置文件 我们把DNN的后端设置为OpenCV，目标设置为CPU。可以通过使cv.dnn.DNN_TARGET_OPENCL置为GPU，尝试设定偏好的运行目标为GPU。但是要记住当前的OpenCV版本只在Intel的GPU上测试，如果没有Intel的GPU则程序会自动设置为CPU。 Python:12345678910111213# Load names of classesclassesFile = "coco.names";classes = Nonewith open(classesFile, 'rt') as f: classes = f.read().rstrip('\n').split('\n') # Give the configuration and weight files for the model and load the network using them.modelConfiguration = "yolov3.cfg";modelWeights = "yolov3.weights"; net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU) C++:1234567891011121314// Load names of classesstring classesFile = "coco.names";ifstream ifs(classesFile.c_str());string line;while (getline(ifs, line)) classes.push_back(line); // Give the configuration and weight files for the modelString modelConfiguration = "yolov3.cfg";String modelWeights = "yolov3.weights"; // Load the networkNet net = readNetFromDarknet(modelConfiguration, modelWeights);net.setPreferableBackend(DNN_BACKEND_OPENCV);net.setPreferableTarget(DNN_TARGET_CPU); 第4步：读取输入这一步我们读取图像，视频流或者网络摄像头。另外，我们也使用Videowriter（OpenCV里的一个类）以视频方式保存带有输出边界框的每一帧图片。 Python:123456789101112131415161718192021222324252627outputFile = "yolo_out_py.avi"if (args.image): # Open the image file if not os.path.isfile(args.image): print("Input image file ", args.image, " doesn't exist") sys.exit(1) cap = cv.VideoCapture(args.image) outputFile = args.image[:-4]+'_yolo_out_py.jpg'elif (args.video): # Open the video file if not os.path.isfile(args.video): print("Input video file ", args.video, " doesn't exist") sys.exit(1) cap = cv.VideoCapture(args.video) outputFile = args.video[:-4]+'_yolo_out_py.avi'else: # Webcam input cap = cv.VideoCapture(0) # Get the video writer initialized to save the output videoif (not args.image): vid_writer = cv.VideoWriter ( outputFile, cv.VideoWriter_fourcc('M','J','P','G'), 30, (round(cap.get(cv.CAP_PROP_FRAME_WIDTH)), round(cap.get(cv.CAP_PROP_FRAME_HEIGHT)),) ) C++:12345678910111213141516171819202122232425262728293031323334353637outputFile = "yolo_out_cpp.avi";if (parser.has("image"))&#123; // Open the image file str = parser.get&lt;String&gt;("image"); ifstream ifile(str); if (!ifile) throw("error"); cap.open(str); str.replace(str.end()-4, str.end(), "_yolo_out.jpg"); outputFile = str;&#125;else if (parser.has("video"))&#123; // Open the video file str = parser.get&lt;String&gt;("video"); ifstream ifile(str); if (!ifile) throw("error"); cap.open(str); str.replace(str.end()-4, str.end(), "_yolo_out.avi"); outputFile = str;&#125;// Open the webcaomelse cap.open(parser.get&lt;int&gt;("device")); // Get the video writer initialized to save the output videoif (!parser.has("image")) &#123; video.open( outputFile, VideoWriter::fourcc('M','J','P','G'), 28, Size( cap.get(CAP_PROP_FRAME_WIDTH), cap.get(CAP_PROP_FRAME_HEIGHT) ) );&#125; 第5步：处理每一帧输入到神经网络的图像需要以一种叫bolb的格式保存。 读取了输入图片或者视频流的一帧图像后，这帧图像需要经过bolbFromImage()函数处理为神经网络的输入类型bolb。在这个过程中，图像像素以一个1/255的比例因子，被缩放到0到1之间。同时，图像在不裁剪的情况下，大小调整到416x416。注意我们没有降低图像平均值，因此传递[0,0,0]到函数的平均值输入，保持swapRB参数到默认值1。 输出的bolb传递到网络，经过网络正向处理，网络输出了所预测到的一个边界框清单。这些边界框通过后处理，滤除了低置信值的。我们随后再详细的说明后处理的步骤。我们在每一帧的左上方打印出了推断时间。伴随着最后的边界框的完成，图像保存到硬盘中，之后可以作为图像输入或者通过Videowriter作为视频流输入。 Python：1234567891011121314151617181920212223242526272829303132333435363738394041424344while cv.waitKey(1) &lt; 0: # get frame from the video hasFrame, frame = cap.read() # Stop the program if reached end of video if not hasFrame: print("Done processing !!!") print("Output file is stored as ", outputFile) cv.waitKey(3000) break # Create a 4D blob from a frame. blob = cv.dnn.blobFromImage( frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False) # Sets the input to the network net.setInput(blob) # Runs the forward pass to get output of the output layers outs = net.forward(getOutputsNames(net)) # Remove the bounding boxes with low confidence postprocess(frame, outs) # Put efficiency information. The function getPerfProfile returns the # overall time for inference(t) and the timings for each of the layers(in layersTimes) t, _ = net.getPerfProfile() label = 'Inference time: %.2f ms' % ( t * 1000.0 / cv.getTickFrequency()) cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255)) # Write the frame with the detection boxes if (args.image): cv.imwrite(outputFile, frame.astype(np.uint8)); else: vid_writer.write(frame.astype(np.uint8)) c++:12345678910111213141516171819202122232425262728293031323334353637383940414243// Process frames.while (waitKey(1) &lt; 0)&#123; // get frame from the video cap &gt;&gt; frame; // Stop the program if reached end of video if (frame.empty()) &#123; cout &lt;&lt; "Done processing !!!" &lt;&lt; endl; cout &lt;&lt; "Output file is stored as " &lt;&lt; outputFile &lt;&lt; endl; waitKey(3000); break; &#125; // Create a 4D blob from a frame. blobFromImage(frame, blob, 1/255.0, cvSize(inpWidth, inpHeight), Scalar(0,0,0), true, false); //Sets the input to the network net.setInput(blob); // Runs the forward pass to get output of the output layers vector&lt;Mat&gt; outs; net.forward(outs, getOutputsNames(net)); // Remove the bounding boxes with low confidence postprocess(frame, outs); // Put efficiency information. The function getPerfProfile returns the // overall time for inference(t) and the timings for each of the layers(in layersTimes) vector&lt;double&gt; layersTimes; double freq = getTickFrequency() / 1000; double t = net.getPerfProfile(layersTimes) / freq; string label = format("Inference time for a frame : %.2f ms", t); putText(frame, label, Point(0, 15), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(0, 0, 255)); // Write the frame with the detection boxes Mat detectedFrame; frame.convertTo(detectedFrame, CV_8U); if (parser.has("image")) imwrite(outputFile, detectedFrame); else video.write(detectedFrame); &#125; 现在，让我们详细分析一下上面调用的函数。 第5a步：得到输出层的名字OpenCV的网络类中的前向功能需要结束层，直到它在网络中运行。因为我们需要运行整个网络，所以我们需要识别网络中的最后一层。我们通过使用getUnconnectedOutLayers()获得未连接的输出层的名字，该层基本就是网络的最后层。然后我们运行前向网络，得到输出，如前面的代码片段（net.forward(getOutputsNames(net))）。 python:1234567# Get the names of the output layersdef getOutputsNames(net): # Get the names of all the layers in the network layersNames = net.getLayerNames() # Get the names of the output layers, i.e. the layers with unconnected outputs return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()] C++:12345678910111213141516171819// Get the names of the output layersvector&lt;String&gt; getOutputsNames(const Net&amp; net)&#123; static vector&lt;String&gt; names; if (names.empty()) &#123; //Get the indices of the output layers, i.e. the layers with unconnected outputs vector&lt;int&gt; outLayers = net.getUnconnectedOutLayers(); //get the names of all the layers in the network vector&lt;String&gt; layersNames = net.getLayerNames(); // Get the names of the output layers in names names.resize(outLayers.size()); for (size_t i = 0; i &lt; outLayers.size(); ++i) names[i] = layersNames[outLayers[i] - 1]; &#125; return names;&#125; 第5b步：后处理网络输出网络输出的每个边界框都分别由一个包含着类别名字和5个元素的向量表示。 头四个元素代表center_x, center_y, width和height。第五个元素表示包含着目标的边界框的置信度。 其余的元素是和每个类别（如目标种类）有关的置信度。边界框分配给最高分数对应的那一种类。 一个边界框的最高分数也叫做它的置信度（confidence）。如果边界框的置信度低于规定的阀值，算法上不再处理这个边界框。 置信度大于或等于置信度阀值的边界框，将进行非最大抑制。这会减少重叠的边界框数目。 Python:12345678910111213141516171819202122232425262728293031323334353637383940# Remove the bounding boxes with low confidence using non-maxima suppressiondef postprocess(frame, outs): frameHeight = frame.shape[0] frameWidth = frame.shape[1] classIds = [] confidences = [] boxes = [] # Scan through all the bounding boxes output from the network and keep only the # ones with high confidence scores. Assign the box's class label as the class with the highest score. classIds = [] confidences = [] boxes = [] for out in outs: for detection in out: scores = detection[5:] classId = np.argmax(scores) confidence = scores[classId] if confidence &gt; confThreshold: center_x = int(detection[0] * frameWidth) center_y = int(detection[1] * frameHeight) width = int(detection[2] * frameWidth) height = int(detection[3] * frameHeight) left = int(center_x - width / 2) top = int(center_y - height / 2) classIds.append(classId) confidences.append(float(confidence)) boxes.append([left, top, width, height]) # Perform non maximum suppression to eliminate redundant overlapping boxes with # lower confidences. indices = cv.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold) for i in indices: i = i[0] box = boxes[i] left = box[0] top = box[1] width = box[2] height = box[3] drawPred(classIds[i], confidences[i], left, top, left + width, top + height) c++:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// Remove the bounding boxes with low confidence using non-maxima suppressionvoid postprocess(Mat&amp; frame, const vector&lt;Mat&gt;&amp; outs)&#123; vector&lt;int&gt; classIds; vector&lt;float&gt; confidences; vector&lt;Rect&gt; boxes; for (size_t i = 0; i &lt; outs.size(); ++i) &#123; // Scan through all the bounding boxes output from the network and keep only the // ones with high confidence scores. Assign the box's class label as the class // with the highest score for the box. float* data = (float*)outs[i].data; for (int j = 0; j &lt; outs[i].rows; ++j, data += outs[i].cols) &#123; Mat scores = outs[i].row(j).colRange(5, outs[i].cols); Point classIdPoint; double confidence; // Get the value and location of the maximum score minMaxLoc(scores, 0, &amp;confidence, 0, &amp;classIdPoint); if (confidence &gt; confThreshold) &#123; int centerX = (int)(data[0] * frame.cols); int centerY = (int)(data[1] * frame.rows); int width = (int)(data[2] * frame.cols); int height = (int)(data[3] * frame.rows); int left = centerX - width / 2; int top = centerY - height / 2; classIds.push_back(classIdPoint.x); confidences.push_back((float)confidence); boxes.push_back(Rect(left, top, width, height)); &#125; &#125; &#125; // Perform non maximum suppression to eliminate redundant overlapping boxes with // lower confidences vector&lt;int&gt; indices; NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices); for (size_t i = 0; i &lt; indices.size(); ++i) &#123; int idx = indices[i]; Rect box = boxes[idx]; drawPred(classIds[idx], confidences[idx], box.x, box.y, box.x + box.width, box.y + box.height, frame); &#125;&#125; 非最大抑制由参数nmsThreshold控制。如果nmsThreshold设置太少，比如0.1，我们可能检测不到相同或不同种类的重叠目标。如果设置得太高，比如1，可能出现一个目标有多个边界框包围。 第5c步：画出计算得到的边界框 最后，经过非最大抑制后，得到了边界框。我们把边界框在输入帧上画出，并标出种类名和置信值。 Python: 最后，经过非最大抑制后，得到了边界框。我们把边界框在输入帧上画出，并标出种类名和置信值。 1234567891011121314151617# Draw the predicted bounding boxdef drawPred(classId, conf, left, top, right, bottom): # Draw a bounding box. cv.rectangle(frame, (left, top), (right, bottom), (0, 0, 255)) label = '%.2f' % conf # Get the label for the class name and its confidence if classes: assert(classId &lt; len(classes)) label = '%s:%s' % (classes[classId], label) #Display the label at the top of the bounding box labelSize, baseLine = cv.getTextSize( label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1) top = max(top, labelSize[1]) cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255)) c++:12345678910111213141516171819// Draw the predicted bounding boxvoid drawPred(int classId, float conf, int left, int top, int right, int bottom, Mat&amp; frame)&#123; //Draw a rectangle displaying the bounding box rectangle(frame, Point(left, top), Point(right, bottom), Scalar(0, 0, 255)); //Get the label for the class name and its confidence string label = format("%.2f", conf); if (!classes.empty()) &#123; CV_Assert(classId &lt; (int)classes.size()); label = classes[classId] + ":" + label; &#125; //Display the label at the top of the bounding box int baseLine; Size labelSize = getTextSize(label, FONT_HERSHEY_SIMPLEX, 0.5, 1, &amp;baseLine); top = max(top, labelSize.height); putText(frame, label, Point(left, top), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(255,255,255)); 附上Python完整代码，方便搬运123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181######################## 使用示例 ################################ python3 object_detection_yolo.py --video=run.mp4 #### python3 object_detection_yolo.py --image=bird.jpg ################################################################import cv2 as cvimport argparseimport sysimport numpy as npimport os.path# Initialize the parametersconfThreshold = 0.5 #Confidence thresholdnmsThreshold = 0.4 #Non-maximum suppression thresholdinpWidth = 416 #Width of network's input imageinpHeight = 416 #Height of network's input imageparser = argparse.ArgumentParser( description='Object Detection using YOLO in OPENCV')parser.add_argument('--image', help='Path to image file.')parser.add_argument('--video', help='Path to video file.')args = parser.parse_args() # Load names of classesclassesFile = "voc_MLH.names"classes = Nonewith open(classesFile, 'rt') as f: classes = f.read().rstrip('\n').split('\n')# Give the configuration and weight files for the model and load the network using them.modelConfiguration = "yolov3.cfg"modelWeights = "yolov3.weights"net = cv.dnn.readNetFromDarknet(modelConfiguration, modelWeights)#net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)#net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)# Get the names of the output layersdef getOutputsNames(net): # Get the names of all the layers in the network layersNames = net.getLayerNames() # Get the names of the output layers, i.e. the layers with unconnected outputs return [layersNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]# Draw the predicted bounding boxdef drawPred(classId, conf, left, top, right, bottom): # Draw a bounding box. cv.rectangle( frame, (left, top), (right, bottom), (255, 178, 50), 3) label = '%.2f' % conf # Get the label for the class name and its confidence if classes: assert(classId &lt; len(classes)) label = '%s:%s' % (classes[classId], label) #Display the label at the top of the bounding box labelSize, baseLine = cv.getTextSize( label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1) top = max(top, labelSize[1]) cv.rectangle( frame, (left, top - round(3.5*labelSize[1])), (left + round(3.5*labelSize[0]), top + baseLine), (255, 255, 255), cv.FILLED) cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1)# Remove the bounding boxes with low confidence using non-maxima suppressiondef postprocess(frame, outs): frameHeight = frame.shape[0] frameWidth = frame.shape[1] # Scan through all the bounding boxes output from the network and keep only the # ones with high confidence scores. Assign the box's class label as the class with the highest score. classIds = [] confidences = [] boxes = [] for out in outs: for detection in out: scores = detection[5:] classId = np.argmax(scores) confidence = scores[classId] if confidence &gt; confThreshold: center_x = int(detection[0] * frameWidth) center_y = int(detection[1] * frameHeight) width = int(detection[2] * frameWidth) height = int(detection[3] * frameHeight) left = int(center_x - width / 2) top = int(center_y - height / 2) classIds.append(classId) confidences.append(float(confidence)) boxes.append([left, top, width, height]) # Perform non maximum suppression to eliminate redundant overlapping boxes with # lower confidences. indices = cv.dnn.NMSBoxes( boxes, confidences, confThreshold, nmsThreshold) for i in indices: i = i[0] box = boxes[i] left = box[0] top = box[1] width = box[2] height = box[3] drawPred( classIds[i], confidences[i], left, top, left + width, top + height)# Process inputswinName = 'Deep learning object detection in OpenCV'cv.namedWindow(winName, cv.WINDOW_NORMAL)outputFile = "yolo_out_py.avi"if (args.image): # Open the image file if not os.path.isfile(args.image): print("Input image file ", args.image, " doesn't exist") sys.exit(1) cap = cv.VideoCapture(args.image) outputFile = args.image[:-4]+'_out.jpg'elif (args.video): # Open the video file if not os.path.isfile(args.video): print("Input video file ", args.video, " doesn't exist") sys.exit(1) cap = cv.VideoCapture(args.video) outputFile = args.video[:-4]+'_yolo_out_py.avi'else: # Webcam input cap = cv.VideoCapture(0)# Get the video writer initialized to save the output videoif (not args.image): vid_writer = cv.VideoWriter(outputFile, cv.VideoWriter_fourcc('M','J','P','G'), 30, (round(cap.get(cv.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv.CAP_PROP_FRAME_HEIGHT))))while cv.waitKey(1) &lt; 0: # get frame from the video hasFrame, frame = cap.read() # Stop the program if reached end of video if not hasFrame: print("Done processing !!!") print("Output file is stored as ", outputFile) cv.waitKey(3000) # Release device cap.release() break # Create a 4D blob from a frame. blob = cv.dnn.blobFromImage( frame, 1/255, (inpWidth, inpHeight), [0,0,0], 1, crop=False) # Sets the input to the network net.setInput(blob) # Runs the forward pass to get output of the output layers outs = net.forward(getOutputsNames(net)) # Remove the bounding boxes with low confidence postprocess(frame, outs) # Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes) t, _ = net.getPerfProfile() label = 'Inference time: %.2f ms' % ( t * 1000.0 / cv.getTickFrequency()) cv.putText(frame, label, (0, 15), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255)) # Write the frame with the detection boxes if (args.image): cv.imwrite(outputFile, frame.astype(np.uint8)) else: vid_writer.write(frame.astype(np.uint8)) cv.imshow(winName, frame) 扩展内容OpenCV自3.1版本其就在contrib中加入了DNN模块 该DNN模块除了libprotobuf，不依赖任何第三方库；并且libprotobuf已经包含在了OpenCV的ThirdParty，安装OpenCV时会一并安装 目前，该DNN模块支持加载训练好的模型 支持的深度学习库： Caffe 1 TensorFlow Torch/PyTorch 主要的层及函数： AbsVal（caffe/layers/absval_layer.hpp这一层比较简单：主要就是求绝对值） AveragePooling（平均值池化） BatchNormalization（就像激活函数层、卷积层、全连接层、池化层一样，BN也属于网络的一层；在网络中间层数据做一个归一化处理） Concatenation（Caffe中通过Concatenation层，可以把多个的blobs链接成一个blob） Convolution (including dilated convolution) Crop Deconvolution, a.k.a. transposed convolution or full convolution DetectionOutput (SSD-specific layer) Dropout Eltwise (+, *, max)（caffe提供的按元素操作层。它支持3种基本操作：PROD按元素乘积；SUM按元素求和；MAX保存最大元素 ） Flatten（Caffe中Flattening层是把一个输入的大小为n * c * h * w变成一个简单的向量，其大小为 n * (chw) * 1 * 1） FullyConnected LRN（Local Response Normalization，caffe中LRN是对一个局部的输入区域进行的归一化） LSTM MaxPooling（最大池化） MaxUnpooling MVN NormalizeBBox (SSD-specific layer) Padding Permute Power PReLU (including ChannelPReLU with channel-specific slopes) PriorBox (SSD-specific layer) ReLU RNN Scale Shift Sigmoid Slice（Caffe中Slice layer 的作用是将bottom按照需要分解成多个tops） Softmax（激活函数） Split（Caffe中Splitting层可以把一个输入blob分离成多个输出blobs） TanH（激活函数） 一些已经经过测试的网络： AlexNet GoogLeNet v1 (also referred to as Inception-5h) ResNet-34/50/… SqueezeNet v1.1 VGG-based FCN (semantical segmentation network) ENet (lightweight semantical segmentation network) VGG-based SSD (object detection network) MobileNet-based SSD (light-weight object detection network 官方示例]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[0.tensorflow介绍]]></title>
    <url>%2Ftensorflow%2F0.tensorflow%E4%BB%8B%E7%BB%8D.html</url>
    <content type="text"><![CDATA[#TensorFlow是一个基于数据流编程（dataflow programming）的符号数学系统，被广泛应用于各类机器学习（machine learning）算法的编程实现，其前身是谷歌的神经网络算法库DistBelief 谷歌出品，必属精品。不用介绍，学就完事儿了 ##组件与工作原理###核心组件分布式TensorFlow的核心组件（core runtime）包括：分发中心（distributed master）、执行器（dataflow executor/worker service）、内核应用（kernel implementation）和最底端的设备层（device layer）/网络层（networking layer） ####分发中心从输入的数据流图中剪取子图（subgraph），将其划分为操作片段并启动执行器。分发中心处理数据流图时会进行预设定的操作优化，包括公共子表达式消去（common subexpression elimination）、常量折叠（constant folding）等 ####执行器负责图操作（graph operation）在进程和设备中的运行、收发其它执行器的结果。分布式TensorFlow拥有参数器（parameter server）以汇总和更新其它执行器返回的模型参数。执行器在调度本地设备时会选择进行并行计算和GPU加速 ####内核应用负责单一的图操作，包括数学计算、数组操作（array manipulation）、控制流（control flow）和状态管理操作（state management operations）。内核应用使用Eigen执行张量的并行计算、cuDNN库等执行GPU加速、gemmlowp执行低数值精度计算，此外用户可以在内核应用中注册注册额外的内核（fused kernels）以提升基础操作，例如激励函数和其梯度计算的运行效率 单进程版本的TensorFlow没有分发中心和执行器，而是使用特殊的会话应用（Session implementation）联系本地设备。TensorFlow的C语言API是核心组件和用户代码的分界，其它组件/API均通过C语言API与核心组件进行交互 ###低阶API1.张量（tf.Tensor） 2.变量（tf.Variable） 3.数据流图（tf.Graph） 4.会话（tf.Session） 5.保存和恢复对张量的保存和恢复使用tf.train.Saver 6.使用检查点工具tf.python.tools.inspect_checkpoint可以查看文件中保存的张量 ###高阶API ####EstimatorsEstimators是TensorFlow自带的高阶神经网络API Estimators封装了神经网络的训练、评估、预测、导出等操作 工作流程如下： 1.建立数据集导入函数：可以使用TensorFlow的数据导入工具tf.data.Dataset或从NumPy数组创建数据集导入函数。 2.定义特征列：特征列（tf.feature_column）包含了训练数据的特征名称、特征类型和输入预处理操作。 3.调出预创建的Estimator模型：可用的模型包括基础统计学（baseline）、梯度提升决策树（boosting desicion tree）和深度神经网络的回归、分类器。调出模型后需提供输入特征列、检查点路径和有关模型参数（例如神经网络的隐含层结构）。 4.训练和评估模型：所有预创建模型都包含train和evaluate接口用于学习和评估。 ####KerasKeras是一个支持TensorFlow、Thenao和Microsoft-CNTK的第三方高阶神经网络API ####Datatf.data是TensorFlow中进行数据管理的高阶API tf.data可用于构建和优化大规机器学习的输入管道（input pipline），提升TensorFlow性能。一个典型的输入管道包含三个部分 1.提取（Extract）：从本地或云端的数据存储点读取原始数据 2.转化（Transform）：使用计算设备（通常为CPU）对数据进行解析和后处理，例如解压缩、洗牌（shuffling）、打包（batching）等 3.加载（Load）：在运行机器学习算法的高性能计算设备（GPU和TPU）加载经过后处理的数据 加速器CPU和GPU设备 ###加速器1.CPU和GPU设备 2.TPU设备 3.设备管理（tf.device） ###优化器 #####模型优化工具Tensorflow提供了模型优化工具（Model Optimization Toolkit）对模型的尺度、响应时间和计算开销进行优化 [51] 。模型优化工具可以减少模型参数的使用量（pruning）、对模型精度进行量化（quantization）和改进模型的拓扑结构，适用于将模型部署到终端设备，或在有硬件局限时运行模型，因此有很多优化方案是TensorFlow Lite项目的一部分 #####XLA线性代数加速器（Accelerated Linear Algebra, XLA）是一个特殊的编译器，用于优化TensorFlow中的线性代数计算，其目标是优化内存使用，提升TensorFlow的运行速度和跨平台，尤其是移动终端的可移植性 ###可视化工具TensorFlow拥有自带的可视化工具TensorBoard，TensorBoard具有展示数据流图、绘制分析图、显示附加数据等功能 ###调试程序TensorFlow团队开发了专用的调试模块TFDBG，该模块可以在学习和预测时查看会话中数据流图的内部结构和状态 ###部署TensorFlow支持在一个或多个系统下使用多个设备并部署分布式服务器（distributed server）和服务器集群（cluster）。tf.train.Server.create_local_server可在本地构建简单的分布式服务器]]></content>
      <categories>
        <category>TensorFlow教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用LaTex语法]]></title>
    <url>%2Freading%2F%E5%B8%B8%E7%94%A8LaTex%E8%AF%AD%E6%B3%95.html</url>
    <content type="text"><![CDATA[LaTex语法公式前后都加上两个$$符号123456$$LaTex$$$$x=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;,y=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;$$ $$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a},y=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$ 行符号为 “\”12345$$x=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;,\\ y=\frac&#123;-b\pm\sqrt&#123;b^2-4ac&#125;&#125;&#123;2a&#125;$$ $$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}\y=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$ 基本运算乘法：x \times y$$x \times y$$ 乘方：2^{3x}$$2^{3x}$$ 平方根：\sqrt {x + y}$$\sqrt {x + y}$$ 除法：x \div y$$x \div y$$ 分数：\frac {x}{y}$$\frac {x}{y}$$ 异或：\oplus$$\oplus$$ 小于或等于：x \leq y$$x \leq y$$ 大于或等于：x \geq y$$x \geq y$$ 不等于：x \neq y$$x \neq y$$ 向下取整：\lfloor x \rfloor$$\lfloor 32.443 \rfloor$$ 向上取整：\lceil x \rceil$$\lceil 20.444 \rceil$$ 求和：\sum\limits_{x = 1}^{n} 7x$$\sum\limits_{x = 1}^{n} 7x$$ 积分：\int_{0}^{\frac{\pi}{2}} \sin(x)$$\int_{0}^{\frac{\pi}{2}} \sin(x)$$ 集合符号：A \in B$$A \in B$$ 不属于：A \notin B$$A \notin B$$ 子集：\subet \sub$$A \sube B\A \sub B$$ 希腊字母\alpha$$\alpha$$ \beta$$\beta$$ theta$$\theta$$ \pi$$\pi$$ 向量矩阵向量：\vec{v}$$\vec{v}$$ 简单Matrix：$$ \begin{matrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{matrix} \tag{1} $$$$\begin{matrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9 \end{matrix} \tag{1}$$ 带括号的Matrix可以给矩阵加上括号，加括号的方式有很多，大致可分为两种：使用\left ... \right 或者把公式命令中的matrix 改成 pmatrix、bmatrix、Bmatrix、vmatrix、Vmatrix等。 使用\left … \right 123456789$$ \left\&#123; \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125; \right\&#125; \tag&#123;2&#125;$$ $$\left{ \begin{matrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9 \end{matrix} \right} \tag{2}$$ 123456789$$ \left[ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125; \right] \tag&#123;3&#125;$$ $$\left[ \begin{matrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9 \end{matrix} \right] \tag{3}$$ 1234567$$ \begin&#123;bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;bmatrix&#125; \tag&#123;4&#125;$$ $$\begin{bmatrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9 \end{bmatrix} \tag{4}$$ 1234567$$ \begin&#123;Bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;Bmatrix&#125; \tag&#123;5&#125;$$ $$\begin{Bmatrix} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9 \end{Bmatrix} \tag{5}$$ 如果矩阵元素太多，可以使用\cdots ⋯⋯ \ddots ⋱⋱ \vdots ⋮⋮ 等省略符号来定义矩阵。 12345678910$$\left[\begin&#123;matrix&#125; 1 &amp; 2 &amp; \cdots &amp; 4 \\ 7 &amp; 6 &amp; \cdots &amp; 5 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 8 &amp; 9 &amp; \cdots &amp; 0 \\\end&#123;matrix&#125;\right]$$ $$\left[\begin{matrix} 1 &amp; 2 &amp; \cdots &amp; 4 \ 7 &amp; 6 &amp; \cdots &amp; 5 \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ 8 &amp; 9 &amp; \cdots &amp; 0 \\end{matrix}\right]$$ 带参数的Matrix比如写增广矩阵，可能需要最右边一列单独考虑。可以用array命令来处理： 12345678$$ \left[ \begin&#123;array&#125;&#123;cc|c&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end&#123;array&#125;\right] \tag&#123;7&#125;$$ $$\left[ \begin{array}{cc|c} 1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \end{array}\right] \tag{7}$$ 其中\begin{array}{cc|c}中的c表示居中对齐元素,|用来作为分割列的符号。]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用Markdown语法]]></title>
    <url>%2Freading%2F%E5%B8%B8%E7%94%A8Markdown%E8%AF%AD%E6%B3%95.html</url>
    <content type="text"><![CDATA[MarkdownMarkdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档。Markdown 语言在 2004 由约翰·格鲁伯（英语：John Gruber）创建。Markdown 编写的文档可以导出 HTML 、Word、图像、PDF、Epub 等多种格式的文档。Markdown 编写的文档后缀为 .md, .markdown Markdown 标题123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 Markdown 段落Markdown 段落没有特殊的格式，直接编写文字就好，段落的换行是使用两个以上空格加上回车 字体Markdown 可以使用以下几种字体： 123456*斜体文本*_斜体文本_**粗体文本**__粗体文本__***粗斜体文本***___粗斜体文本___ 分隔线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线： 123456789**** * ******- - ----------- 删除线如果段落上的文字要添加删除线，只需要在文字的两端加上两个波浪线 ~~ 即可，实例如下： 1~~baidu~~ baidu ###下划线下划线可以通过 HTML 的 &lt; u &gt; 标签来实现： 1&lt;u&gt;带下划线文本&lt;/u&gt; 带下划线文本 ###脚注脚注是对文本的补充说明。Markdown 脚注的格式如下: 1[^要注明的文本] ##Markdown 列表Markdown 支持有序列表和无序列表。无序列表使用星号(*)、加号(+)或是减号(-)作为列表标记： 123456789101112* 第一项* 第二项* 第三项+ 第一项+ 第二项+ 第三项- 第一项- 第二项- 第三项 有序列表使用数字并加上 . 号来表示，如： 1231. 第一项2. 第二项3. 第三项 列表嵌套列表嵌套只需在子列表中的选项添加四个空格即可： 1234561. 第一项： - 第一项嵌套的第一个元素 - 第一项嵌套的第二个元素2. 第二项： - 第二项嵌套的第一个元素 - 第二项嵌套的第一个元素 Markdown 区块Markdown 区块引用是在段落开头使用 &gt; 符号 ，然后后面紧跟一个空格符号： 1&gt; 区块引用 引用 Markdown 代码###函数或片段代码用反引号把它包起来（`），例如： 1`printf()` 函数 print()函数 ###代码区块代码区块使用 4 个空格或者一个制表符（Tab 键） 12def hello(): print('hello') ##Markdown 链接链接使用方法如下： 12345[链接名称](链接地址)或者&lt;链接地址&gt; 这是百度链接：百度 ##Markdown 图片Markdown 图片语法格式如下： 123![alt 属性文本](图片地址)![alt 属性文本](图片地址 "可选标题") Markdown 还没有办法指定图片的高度与宽度，如果你需要的话，你可以使用普通的 标签 1&lt;img src="logo.png" width="50%"&gt; ##Markdown 表格Markdown 制作表格使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行。语法格式如下： 1234| 表头 | 表头 || ---- | ---- || 单元格 | 单元格 || 单元格 | 单元格 | 表头 表头 单元格 单元格 单元格 单元格 ###对齐方式 我们可以设置表格的对齐方式： -: 设置内容和标题栏居右对齐 :- 设置内容和标题栏居左对齐 :-: 设置内容和标题栏居中对齐 ##Markdown 高级技巧 ###支持的 HTML 元素不在 Markdown 涵盖范围之内的标签，都可以直接在文档里面用 HTML 撰写。 目前支持的 HTML 元素有：&lt;kbd&gt; &lt;b&gt; &lt;i&gt; &lt;em&gt; &lt;sup&gt; &lt;sub&gt; &lt;br&gt; 1使用 &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Alt&lt;/kbd&gt;+&lt;kbd&gt;Del&lt;/kbd&gt; 重启电脑 使用 Ctrl+Alt+Del 重启电脑 ###转义Markdown 使用了很多特殊符号来表示特定的意义，如果需要显示特定的符号则需要使用转义字符，Markdown 使用反斜杠转义特殊字符： 12**文本加粗** \*\* 正常显示星号 \*\* Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号： 123456789101112\ 反斜线` 反引号* 星号_ 下划线&#123;&#125; 花括号[] 方括号() 小括号# 井字号+ 加号- 减号. 英文句点! 感叹号 ###公式当你需要在编辑器中插入数学公式时，可以使用两个美元符 $$ 包裹 TeX 或 LaTeX 格式的数学公式来实现。提交后，问答和文章页会根据需要加载 Mathjax 对数学公式进行渲染。 123$$$$]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[20.Python的面对对象]]></title>
    <url>%2Fpython%2F20.Python%E7%9A%84%E9%9D%A2%E5%AF%B9%E5%AF%B9%E8%B1%A1.html</url>
    <content type="text"><![CDATA[书接上文Python 面向对象Python从设计之初就已经是一门面向对象的语言，正因为如此，在Python中创建一个类和对象是很容易的。本章节我们将详细介绍Python的面向对象编程。如果你以前没有接触过面向对象的编程语言，那你可能需要先了解一些面向对象语言的一些基本特征，在头脑里头形成一个基本的面向对象的概念，这样有助于你更容易的学习Python的面向对象编程。 接下来我们先来简单的了解下面向对象的一些基本特征。 面向对象技术简介 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员：类变量或者实例变量, 用于处理类及其实例对象的相关的数据。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 局部变量：定义在方法中的变量，只作用于当前实例的类。 实例变量：在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。 继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例图，Dog是一个Animal）。 实例化：创建一个类的实例，类的具体对象。 方法：类中定义的函数。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 创建类使用 class 语句来创建一个新类，class 之后为类的名称并以冒号结尾: 123class ClassName: '类的帮助信息' #类文档字符串 class_suite #类体 类的帮助信息可以通过ClassName.doc查看。 class_suite 由类成员，方法，数据属性组成。 1234567891011121314151617181920class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print("Total Employee %d" % Employee.empCount) def displayEmployee(self): print("Name:",self.name,", Salary:",self.salary)chen = Employee('chen',100)jian = Employee('jian',200)Employee.displayCount(chen)Employee.displayEmployee(chen) 12Total Employee 2Name: chen , Salary: 100 empCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。你可以在内部类或外部类使用 Employee.empCount 访问。 第一种方法init()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法 self 代表类的实例，self 在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。 self代表类的实例，而非类 类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是 self。 1234567class Test: def prt(self): print(self) print(self.__class__) t = Test()t.prt() 12&lt;__main__.Test object at 0x000002795B9841D0&gt;&lt;class '__main__.Test'&gt; 创建实例对象实例化类其他编程语言中一般用关键字 new，但是在 Python 中并没有这个关键字，类的实例化类似函数调用方式。 以下使用类的名称 Employee 来实例化，并通过 init 方法接收参数。 1234"创建 Employee 类的第一个对象"emp1 = Employee("Zara", 2000)"创建 Employee 类的第二个对象"emp2 = Employee("Manni", 5000) 访问属性您可以使用点号 . 来访问对象的属性。使用如下类的名称访问类变量: 123emp1.displayEmployee()emp2.displayEmployee()print("Total Employee %d" % Employee.empCount) 你也可以使用以下函数的方式来访问属性： getattr(obj, name[, default]) : 访问对象的属性。 hasattr(obj,name) : 检查是否存在一个属性。 setattr(obj,name,value) : 设置一个属性。如果属性不存在，会创建一个新属性。 delattr(obj, name) : 删除属性。 Python内置类属性 dict : 类的属性（包含一个字典，由类的数据属性组成） doc :类的文档字符串 name: 类名 module: 类定义所在的模块（类的全名是’main.className’，如果类位于一个导入模块mymod中，那么className.module 等于 mymod） bases : 类的所有父类构成元素（包含了一个由所有父类组成的元组） 1234567891011121314151617181920class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print("Total Employee %d" % Employee.empCount) def displayEmployee(self): print("Name : ", self.name, ", Salary: ", self.salary) print("Employee.__doc__:", Employee.__doc__)print("Employee.__name__:", Employee.__name__)print("Employee.__module__:", Employee.__module__)print("Employee.__bases__:", Employee.__bases__)print("Employee.__dict__:", Employee.__dict__) 12345Employee.__doc__: 所有员工的基类Employee.__name__: EmployeeEmployee.__module__: __main__Employee.__bases__: (&lt;class 'object'&gt;,)Employee.__dict__: &#123;'__module__': '__main__', '__doc__': '所有员工的基类', 'empCount': 0, '__init__': &lt;function Employee.__init__ at 0x000001AF85D0C2F0&gt;, 'displayCount': &lt;function Employee.displayCount at 0x000001AF867052F0&gt;, 'displayEmployee': &lt;function Employee.displayEmployee at 0x000001AF86705378&gt;, '__dict__': &lt;attribute '__dict__' of 'Employee' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Employee' objects&gt;&#125; python对象销毁(垃圾回收)Python 使用了引用计数这一简单技术来跟踪和回收垃圾。 在 Python 内部记录着所有使用中的对象各有多少引用。 一个内部跟踪变量，称为一个引用计数器。 当对象被创建时， 就创建了一个引用计数， 当这个对象不再需要时， 也就是说， 这个对象的引用计数变为0 时， 它被垃圾回收。但是回收不是”立即”的， 由解释器在适当的时机，将垃圾对象占用的内存空间回收。 垃圾回收机制不仅针对引用计数为0的对象，同样也可以处理循环引用的情况。循环引用指的是，两个对象相互引用，但是没有其他变量引用他们。这种情况下，仅使用引用计数是不够的。Python 的垃圾收集器实际上是一个引用计数器和一个循环垃圾收集器。作为引用计数的补充， 垃圾收集器也会留心被分配的总量很大（及未通过引用计数销毁的那些）的对象。 在这种情况下， 解释器会暂停下来， 试图清理所有未引用的循环。 析构函数 del ，del在对象销毁的时候被调用，当对象不再被使用时，del方法运行： 123456789101112131415class Point: def __init__( self, x=0, y=0): self.x = x self.y = y def __del__(self): class_name = self.__class__.__name__ print(class_name, "销毁") pt1 = Point()pt2 = pt1pt3 = pt1print(id(pt1), id(pt2), id(pt3)) # 打印对象的iddel pt1del pt2del pt3 122009501876744 2009501876744 2009501876744Point 销毁 类的继承面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。 通过继承创建的新类称为子类或派生类，被继承的类称为基类、父类或超类。 继承语法 12class 派生类名(基类名) ... 如果在继承元组中列了一个以上的类，那么它就被称作”多重继承” 。 语法： 派生类的声明，与他们的父类类似，继承的基类列表跟在类名之后，如下所示： 12class SubClassName (ParentClass1[, ParentClass2, ...]): ... 1234567891011121314151617181920212223242526class Parent: # 定义父类 parentAttr = 100 def __init__(self): print("调用父类构造函数") def parentMethod(self): print('调用父类方法') def setAttr(self, attr): Parent.parentAttr = attr def getAttr(self): print("父类属性 :", Parent.parentAttr) class Child(Parent): # 定义子类 def __init__(self): print("调用子类构造方法") def childMethod(self): print('调用子类方法') c = Child() # 实例化子类c.childMethod() # 调用子类的方法c.parentMethod() # 调用父类方法c.setAttr(200) # 再次调用父类的方法 - 设置属性值c.getAttr() # 再次调用父类的方法 - 获取属性值 1234调用子类构造方法调用子类方法调用父类方法父类属性 : 200 你可以使用issubclass()或者isinstance()方法来检测。 issubclass() - 布尔函数判断一个类是另一个类的子类或者子孙类，语法：issubclass(sub,sup) isinstance(obj, Class) 布尔函数如果obj是Class类的实例对象或者是一个Class子类的实例对象则返回true。 方法重写如果你的父类方法的功能不能满足你的需求，你可以在子类重写你父类的方法： 12345678910class Parent: # 定义父类 def myMethod(self): print('调用父类方法') class Child(Parent): # 定义子类 def myMethod(self): print('调用子类方法') c = Child() # 子类实例c.myMethod() # 子类调用重写方法 基础重载方法下表列出了一些通用的功能，你可以在自己的类重写： 方法, 描述 &amp; 简单的调用 序号 init ( self [,args…] ) 构造函数简单的调用方法: obj = className(args) 1 del( self ) 析构方法, 删除一个对象简单的调用方法 : del obj 2 repr( self ) 转化为供解释器读取的形式简单的调用方法 : repr(obj) 3 str( self )用于将值转化为适于人阅读的形式简单的调用方法 : str(obj) 4 运算符重载Python同样支持运算符重载，实例如下： 1234567891011121314class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return('Vector (%d, %d)' % (self.a, self.b)) def __add__(self,other): return(Vector(self.a + other.a, self.b + other.b)) v1 = Vector(2,10)v2 = Vector(5,-2)print(v1 + v2) 1Vector (7, 8) 类属性与方法类的私有属性 __private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。 类的方法在类的内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self,且为第一个参数 类的私有方法__private_method：两个下划线开头，声明该方法为私有方法，不能在类的外部调用。在类的内部调用 self.__private_methods 1234567891011121314class JustCounter: __secretCount = 0 # 私有变量 publicCount = 0 # 公开变量 def count(self): self.__secretCount += 1 self.publicCount += 1 print(self.__secretCount) counter = JustCounter()counter.count()counter.count()print(counter.publicCount)print(counter.__secretCount) # 报错，实例不能访问私有变量 1234567122Traceback (most recent call last): File "F:\python37_chen\1.py", line 14, in &lt;module&gt; print(counter.__secretCount) # 报错，实例不能访问私有变量AttributeError: 'JustCounter' object has no attribute '__secretCount' Python不允许实例化的类访问私有数据，但你可以使用 object._className__attrName（ 对象名._类名__私有属性名 ）访问属性，参考以下实例： 1print(counter._JustCounter__secretCount) 12 单下划线、双下划线、头尾双下划线说明： foo: 定义的是特殊方法，一般是系统定义名字 ，类似 init() 之类的。 _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import * __foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[19.Python的异常处理]]></title>
    <url>%2Fpython%2F19.Python%E7%9A%84%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86.html</url>
    <content type="text"><![CDATA[书接上文Python 异常处理python提供了两个非常重要的功能来处理python程序在运行中出现的异常和错误。你可以使用该功能来调试python程序。 异常处理 断言(Assertions) python标准异常 异常名称 描述 BaseException 所有异常的基类 SystemExit 解释器请求退出 KeyboardInterrupt 用户中断执行(通常是输入^C) Exception 常规错误的基类 StopIteration 迭代器没有更多的值 GeneratorExit 生成器(generator)发生异常来通知退出 StandardError 所有的内建标准异常的基类 ArithmeticError 所有数值计算错误的基类 FloatingPointError 浮点计算错误 OverflowError 数值运算超出最大限制 ZeroDivisionError 除(或取模)零 (所有数据类型) AssertionError 断言语句失败 AttributeError 对象没有这个属性 EOFError 没有内建输入,到达EOF 标记 EnvironmentError 操作系统错误的基类 IOError 输入/输出操作失败 OSError 操作系统错误 WindowsError 系统调用失败 ImportError 导入模块/对象失败 LookupError 无效数据查询的基类 IndexError 序列中没有此索引(index) KeyError 映射中没有这个键 MemoryError 内存溢出错误(对于Python 解释器不是致命的) NameError 未声明/初始化对象 (没有属性) UnboundLocalError 访问未初始化的本地变量 ReferenceError 弱引用(Weak reference)试图访问已经垃圾回收了的对象 RuntimeError 一般的运行时错误 NotImplementedError 尚未实现的方法 SyntaxError Python 语法错误 IndentationError 缩进错误 TabError Tab 和空格混用 SystemError 一般的解释器系统错误 TypeError 对类型无效的操作 ValueError 传入无效的参数 UnicodeError Unicode 相关的错误 UnicodeDecodeError Unicode 解码时的错误 UnicodeEncodeError Unicode 编码时错误 UnicodeTranslateError Unicode 转换时错误 Warning 警告的基类 DeprecationWarning 关于被弃用的特征的警告 FutureWarning 关于构造将来语义会有改变的警告 OverflowWarning 旧的关于自动提升为长整型(long)的警告 PendingDeprecationWarning 关于特性将会被废弃的警告 RuntimeWarning 可疑的运行时行为(runtime behavior)的警告 SyntaxWarning 可疑的语法的警告 UserWarning 用户代码生成的警告 什么是异常？异常即是一个事件，该事件会在程序执行过程中发生，影响了程序的正常执行。 一般情况下，在Python无法正常处理程序时就会发生一个异常。 异常是Python对象，表示一个错误。 当Python脚本发生异常时我们需要捕获处理它，否则程序会终止执行。 异常处理捕捉异常可以使用try/except语句。 try/except语句用来检测try语句块中的错误，从而让except语句捕获异常信息并处理。 如果你不想在异常发生时结束你的程序，只需在try里捕获它。 语法： 以下为简单的try….except…else的语法： 12345678try:&lt;语句&gt; #运行别的代码except &lt;名字&gt;：&lt;语句&gt; #如果在try部份引发了'name'异常except &lt;名字&gt;，&lt;数据&gt;:&lt;语句&gt; #如果引发了'name'异常，获得附加的数据else:&lt;语句&gt; #如果没有异常发生 try的工作原理是，当开始一个try语句后，python就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try子句先执行，接下来会发生什么依赖于执行时是否出现异常。 如果当try后的语句执行时发生异常，python就跳回到try并执行第一个匹配该异常的except子句，异常处理完毕，控制流就通过整个try语句（除非在处理异常时又引发新的异常）。 如果在try后的语句里发生了异常，却没有匹配的except子句，异常将被递交到上层的try，或者到程序的最上层（这样将结束程序，并打印默认的出错信息）。 如果在try子句执行时没有发生异常，python将执行else语句后的语句（如果有else的话），然后控制流通过整个try语句。 使用except而不带任何异常类型12345678try: 正常的操作 ......................except: 发生异常，执行这块代码 ......................else: 如果没有异常执行这块代码 使用except而带多种异常类型你也可以使用相同的except语句来处理多个异常信息 12345678try: 正常的操作 ......................except(Exception1[, Exception2[,...ExceptionN]]]): 发生以上多个异常中的一个，执行这块代码 ......................else: 如果没有异常执行这块代码 try-finally 语句try-finally 语句无论是否发生异常都将执行最后的代码。 12345try:&lt;语句&gt;finally:&lt;语句&gt; #退出try时总会执行raise 异常的参数一个异常可以带上参数，可作为输出的异常信息参数。 你可以通过except语句来捕获异常的参数，如下所示： 12345try: 正常的操作 ......................except ExceptionType, Argument: 你可以在这输出 Argument 的值... 变量接收的异常值通常包含在异常的语句中。在元组的表单中变量可以接收一个或者多个值。 元组通常包含错误字符串，错误数字，错误位置。 触发异常我们可以使用raise语句自己触发异常 raise语法格式如下： 1raise [Exception [, args [, traceback]]] 语句中 Exception 是异常的类型（例如，NameError）参数标准异常中任一种，args 是自已提供的异常参数。 最后一个参数是可选的（在实践中很少使用），如果存在，是跟踪异常对象。 用户自定义异常通过创建一个新的异常类，程序可以命名它们自己的异常。异常应该是典型的继承自Exception类，通过直接或间接的方式。 以下为与RuntimeError相关的实例,实例中创建了一个类，基类为RuntimeError，用于在异常触发时输出更多的信息。 在try语句块中，用户自定义的异常后执行except块语句，变量 e 是用于创建Networkerror类的实例。 其实我很少用这个异常处理，原因在于时候未到，还没有达到贡献自己代码的高度，将来进阶的时候再研究这个，初学者可以跳过这一部分，与其把时间花在处理异常，不如多学一点，思考的更全面一点。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[18.Python的文件]]></title>
    <url>%2Fpython%2F18.Python%E7%9A%84%E6%96%87%E4%BB%B6.html</url>
    <content type="text"><![CDATA[书接上文文件 File 文件是用于存储数据的基本单位 文件通常用来长期存储数据 文件中数据是以字节为单位进行顺序存储的文件的操作流程 打开文件 读/写文件 关闭文件 12注: 任何的操作系统,一个应用程序同时打开文件的数量 有最大数限制文件的打开函数 open(filename, mode=’rt’) 用于打开一个文件,返回 此用来操作此文件的文件流对象,如果打开失败, 则会触发OSError错误通知 文件流对象的关闭方法 F.close() 关闭文件.释放系统资源 1.创建写入文件1234f = open('C:/Users/CC/Desktop/a.txt','a')for i in range(10): f.write(str(i)+'\n')f.close() 123456789100123456789 2.读文件123456f = open('C:/Users/CC/Desktop/a.txt','r')s = f.read()print(s)f.close() 123456789100123456789 这是最简单的两个示例，具体的方法语法： 12&gt; open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)&gt; 参数说明: file: 必需，文件路径（相对或者绝对路径）。 mode: 可选，文件打开模式 buffering: 设置缓冲 encoding: 一般使用utf8 errors: 报错级别 newline: 区分换行符 closefd: 传入的file参数类型 opener mode 参数有： 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（不推荐）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 file 对象file 对象使用 open 函数来创建，下表列出了 file 对象常用的函数： 序号 方法及描述 1 file.close()关闭文件。关闭后文件不能再进行读写操作。 2 file.flush()刷新文件内部缓冲，直接把内部缓冲区的数据立刻写入文件, 而不是被动的等待输出缓冲区写入。 3 file.fileno()返回一个整型的文件描述符(file descriptor FD 整型), 可以用在如os模块的read方法等一些底层操作上。 4 file.isatty()如果文件连接到一个终端设备返回 True，否则返回 False。 5 file.next()返回文件下一行。 6 [file.read(size])从文件读取指定的字节数，如果未给定或为负则读取所有。 7 file.readline([size])读取整行，包括 “\n” 字符。 8 file.readlines([sizeint])读取所有行并返回列表，若给定sizeint&gt;0，则是设置一次读多少字节，这是为了减轻读取压力。 9 file.seek(offset[, whence])设置文件当前位置 10 file.tell()返回文件当前位置。 11 file.truncate([size])截取文件，截取的字节通过size指定，默认为当前文件位置。 12 file.write(str)将字符串写入文件，返回的是写入的字符长度。 13 file.writelines(sequence)向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符。 十个汉字占多少个字节?汉字编码(只有两种) 国标系列: GB18030(二字节或四字节编码, 27533个字) GBK(二字节编码,20013个字) GB2313(二字节编码,约7千多个字) (Windows常用) 国际标准: UNICODE(UNCODE16/UNICODE32) &lt;-&gt; UTF-8 (Linux/Mac OS X/ IOS/ Android 常用) UTF-8中: 英文ASCII (0x0 - 0x7F) 一字节 (0x80 - 0x3FF) 二字节 (0x400 - 0xFFFF) 三字节(中文在此区) python 编码字符串: ‘gb2312’ ‘gbk’ ‘gb18030’ ‘utf-8’ ‘ascii’ … 如: 1234s = "你好" print(s.encode('gbk')) print(s.encode('utf-8')) print(s.encode('ascii'))#出错,"你好"不在ascii内 编码注释: 在源文件中,第一行或第二行写入的如下内容是编码注释 1234567s = "百度"print(s.encode('gbk'))print(s.encode('utf-8'))b'\xb0\xd9\xb6\xc8'b'\xe7\x99\xbe\xe5\xba\xa6' 作业：1.写程序实现复制文件的功能 要求: 1. 要考虑超大文件问题 2. 要能复制二进制文件(如:图片等) 3. 要考虑关闭文件 12345678910111213141516171819202122232425262728293031323334# 1. 写程序实现复制文件的功能# 要求:# 1. 要考虑超大文件问题# 2. 要能复制二进制文件(如:图片等)# 3. 要考虑关闭文件def copyfile(src_file, dst_file): &apos;&apos;&apos;src_file 源文件名 dst_file 目标文件名&apos;&apos;&apos; try: fr = open(src_file, &apos;rb&apos;) # 读文件 try: try: fw = open(dst_file, &apos;wb&apos;) # 写文件 try: while True: # 每次搬4096个字节 data = fr.read(4096) if not data: # 已到达文件尾 break fw.write(data) print(&quot;复制文件有成功&quot;) finally: fw.close() except OSError: print(&quot;复制失败&quot;) finally: fr.close() except OSError: print(&quot;复制失败&quot;)src = input(&quot;请输入源文件: &quot;)dst = input(&quot;请输入目标文件: &quot;)copyfile(src, dst) —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[17.Python的模块]]></title>
    <url>%2Fpython%2F17.Python%E7%9A%84%E6%A8%A1%E5%9D%97.html</url>
    <content type="text"><![CDATA[书接上文Python 模块Python 模块(Module)，是一个 Python 文件，以 .py 结尾，包含了 Python 对象定义和Python语句。模块让你能够有逻辑地组织你的 Python 代码段。把相关的代码分配到一个模块里能让你的代码更好用，更易懂。模块能定义函数，类和变量，模块里也能包含可执行的代码。 import 语句我们按照上一章的函数方式可以定义自己的模块，例如：my_print.py 12def chen_print(s): return ('Hello:' + s) 在当前文件夹下新建model_test.py 123import my_print as mpprint(mp.chen_print('haha')) //Hello:haha 如果在当前目录下，直接 import 导入就可以了，此外还可以用as给他重命名，你要导入这个文件中哪些模块，直接用 . 就可以了 from…import 语句Python 的 from 语句让你从模块中导入一个指定的部分到当前命名空间中 12345from my_print import chen_printprint(chen_print('haha')) //Hello:haha from…import * 语句 //使用*将所有模块导入进来，你也可以定义哪些模块能被这样的语法导入，后面再聊 搜索路径当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 搜索路径怎么理解呢？ 你可以认为程序是怎么找到你写的模块，想一想，我们刚开始是不是说在当前目录下创建测试文件，其实就是这个原因，你如果不告诉程序去哪里找，它默认先找当前目录，然后看你指定的目录，也就是环境变量，也就是shell变量下的PYTHONPATH的所有目录，如果都找不到，那就找程序默认的路径。 我们可以借 sys这个模块来查看路径，如果你自己写的包或者安装的第三方模块没有在这些路径下，那么可能没法使用模块哦 123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path['', 'C:\\Program Files\\Python37\\Lib\\idlelib', 'C:\\Program Files\\Python37\\python37.zip', 'C:\\Program Files\\Python37\\DLLs', 'C:\\Program Files\\Python37\\lib', 'C:\\Program Files\\Python37', 'C:\\Users\\CC\\AppData\\Roaming\\Python\\Python37\\site-packages', 'C:\\Program Files\\Python37\\lib\\site-packages'] 假如说我们知道路径不在这里面，怎么添加使用呢？ windows: 此电脑 —&gt; 属性 —&gt; 高级系统设置 —&gt; 环境变量 —&gt; Path 进行添加即可，通常都是你安装地址的bin下的目录 Linux：123vim ~/.bashrcexport PYTHONPATH=$PYTHONPATH:/YOU_MODULE_DIRECTORYsource ~/.bashrc 如果说你想大量管理这些路径呢？ pth文件这种方式很简单便于管理，在需要大量添加外部路径的时候非常推荐，而且是永久生效。启动Python程序时候，解释器就会遍历目录，遇到pth就会读取这个文件中的内容并添加到sys.path中。这里以系统自带的Python2.7为例说明 1234cd /usr/lib/python2.7/site-packagescat Allinone.path//内容就是路径 不可忽视模块的路径学习，这是初学者最大问题，模块都用不了，搬砖都没法搬 dir()函数dir() 函数一个排好序的字符串列表，内容是一个模块里定义过的名字。 返回的列表容纳了在一个模块里定义的所有模块，变量和函数 开始我们学习math的时候使用过这个函数，不知道还有没有印象，假如你恍惚记得my_print这个文件名，里面具体的模块函数忘记了怎么办？ 123import my_printprint(dir(my_print)) 1['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'chen_print'] 看见这个列表了吗？里面有个 chen_print正好是我们要找的函数。那么其他这么多玩意儿是什么？可以不用管它，当它是系统自带吧。我们就简单操作一下，看看和我们的模块关系，也是为了后面理解面对对象铺垫一下。 12345''' 这是我自己的打印函数'''def chen_print(s): return ('Hello:' + s) 1234import my_printprint(my_print.__name__)print(my_print.__doc__) 123my_print 这是我自己的打印函数 大概了解了一点吧，原来系统默认了一些模块自身的东西，比如帮助文档和模块的名字 命名空间和作用域变量是拥有匹配对象的名字（标识符）。命名空间是一个包含了变量名称们（键）和它们各自相应的对象们（值）的字典。 一个 Python 表达式可以访问局部命名空间和全局命名空间里的变量。如果一个局部变量和一个全局变量重名，则局部变量会覆盖全局变量。 每个函数都有自己的命名空间。类的方法的作用域规则和通常函数的一样。 Python 会智能地猜测一个变量是局部的还是全局的，它假设任何在函数内赋值的变量都是局部的。 因此，如果要给函数内的全局变量赋值，必须使用 global 语句。 global VarName 的表达式会告诉 Python， VarName 是一个全局变量，这样 Python 就不会在局部命名空间里寻找这个变量了。 例如，我们在全局命名空间里定义一个变量 Money。我们再在函数内给变量 Money 赋值，然后 Python 会假定 Money 是一个局部变量。然而，我们并没有在访问前声明一个局部变量 Money，结果就是会出现一个 UnboundLocalError 的错误。取消 global 语句前的注释符就能解决这个问题。 123456789Money = 100def AddMoney(): # global Money Money = Money + 1 print(Money)AddMoney()print(Money) 因为函数也不知道这个Money哪来的，又没人给它传参，所以只好报错 1234567100Traceback (most recent call last): File "C:\Users\CC\Desktop\print\model_test.py", line 8, in &lt;module&gt; AddMoney() File "C:\Users\CC\Desktop\print\model_test.py", line 5, in AddMoney Money = Money + 1UnboundLocalError: local variable 'Money' referenced before assignment 我们再试试加上声明它是全局变量 12345678Money = 100def AddMoney(): global Money Money = Money + 1 print(Money)AddMoney()print(Money) 12100101 还记得我们讲过的可变不可变吗？我们用下面的几个例子帮助对比理解 12345678Money = 100def AddMoney(money): Money = Money + 1 print(Money) print(Money)AddMoney(Money)print(Money) 1234567100Traceback (most recent call last): File "C:\Users\CC\Desktop\print\model_test.py", line 7, in &lt;module&gt; AddMoney(Money) File "C:\Users\CC\Desktop\print\model_test.py", line 3, in AddMoney Money = Money + 1UnboundLocalError: local variable 'Money' referenced before assignment 12345678Money = 100def AddMoney(money): money = money + 1 print(Money) print(Money)AddMoney(Money)print(Money) 123100100100 12345678910Money = 100def AddMoney(money): money = Money + 2 print(Money) print(money) print(Money)AddMoney(Money)print(Money) 1234100100102100 123456789Money = 100def AddMoney(money): global Money money += 1 print(Money)AddMoney(Money)print(money) // 外面没法访问里面的变量，所以这里报错 请仔细体会各种情况下的输出，这些示例能够帮助你理解函数，不可忽视这些细节，因为今后的编程发生一些不知名的错误，往往都是命名空间不对，程序庞大的话很难排错，最好的办法就是熟悉函数，然后不要用 global ，神奇吧，为了不用它而学它。 globals() 和 locals() 函数根据调用地方的不同，globals() 和 locals() 函数可被用来返回全局和局部命名空间里的名字。 如果在函数内部调用 locals()，返回的是所有能在该函数里访问的命名。 如果在函数内部调用 globals()，返回的是所有在该函数里能访问的全局名字。 两个函数的返回类型都是字典。所以名字们能用 keys() 函数摘取。代码栗子： 123456789a = 1b = 2def c(): d = 4print(globals())print('*'*20)print(locals()) 123&#123;'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': &#123;&#125;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '__file__': 'C:\\Users\\CC\\Desktop\\print\\my_print.py', 'argv': ['C:\\Users\\CC\\Desktop\\print\\my_print.py'], 'a': 1, 'b': 2, 'c': &lt;function c at 0x00000295CA3ED5E8&gt;&#125;********************&#123;'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': &#123;&#125;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, '__file__': 'C:\\Users\\CC\\Desktop\\print\\my_print.py', 'argv': ['C:\\Users\\CC\\Desktop\\print\\my_print.py'], 'a': 1, 'b': 2, 'c': &lt;function c at 0x00000295CA3ED5E8&gt;&#125; 所以，你还可以使用这两个函数进行模块函数的调用和查看哦 reload() 函数当一个模块被导入到一个脚本，模块顶层部分的代码只会被执行一次。 因此，如果你想重新执行模块里顶层部分的代码，可以用 reload() 函数。该函数会重新导入之前导入过的模块。注意不要用字符串，直接输入模块名字就可以了 Python中的包包是一个分层次的文件目录结构，它定义了一个由模块及子包，和子包下的子包等组成的 Python 的应用环境。 简单来说，包就是文件夹，但该文件夹下必须存在 init.py 文件, 该文件的内容可以为空。init.py 用于标识当前文件夹是一个包。 其实见的包多了自然能够理解，随便下载包，注意都是有init这个文件的，我们举例说明： 12345test.pypackage|-- __init__.py|-- 1.py|-- 2.py 123456789101112131415161718#1.pydef one(): print('one')#2.pydef two(): print('two')#test.pyfrom package.1 import onefrom package.2 import twoone()two()#__init__.pyif __name__ == "__main__": print('作为主程序运行')else: print('模块初始化') 我们运行一下test看看效果： 123模块初始化onetwo 好像它还运行了init这是怎么回事呢，程序它找包，其实给它这样一个限制之后，它就能够很方便管理这个文件夹下的所有模块了，相当于一个配置文件。我们再单独运行它看看： 1作为主程序运行 还记得前面看全局变量吗？它有这样的一个属性，来判断是不是主程序。 这个玩意儿又有什么用？ 其实你写函数的时候，写模块的时候，自然要测试一下程序能否正常运行，但是测试过后吧又不想删测试代码，毕竟能够证明你这代码是正确的，而且很方便移植的时候进行调试更改，所以很多程序的单元代码测试都可以直接这样写，确保每个功能正确，拼装起来自然容易。 题外话：其实我们使用import 和 from 导入就是导包，凡是带点的左边都必须是包才可以，随着模块的增多，增加模块的管理方便后期的维护是很重要的。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[16.Python的函数]]></title>
    <url>%2Fpython%2F16.Python%E7%9A%84%E5%87%BD%E6%95%B0.html</url>
    <content type="text"><![CDATA[书接上文Python 函数函数是组织好的，可重复使用的，用来实现单一，或相关联功能的代码段。函数能提高应用的模块性，和代码的重复利用率。你已经知道Python提供了许多内建函数，比如print()。但你也可以自己创建函数，这被叫做用户自定义函数。注意：这一节属于重难点，务必吃透。这是进阶为高级程序员的必经之路，想让别人用你的代码？不想一段代码反复编写？那就一定要会写函数。之前我们也提到过你使用别人的函数，只关心功能，输入输出就可以了，那么你也要这样设计你的函数。 函数的思想就是化繁为简，每个函数只负责一个功能，通过功能的组合才能形成各种复杂的程序结构。编程语言已经为你设计了很多高效方便的基础函数，其实加法功能就是一个函数，你可以在任何需要加法的地方使用它。 定义一个函数你可以定义一个由自己想要功能的函数，以下是简单的规则： 函数代码块以 def 关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return [表达式] 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回 None。 语法1234def functionname( parameters ): "函数_文档字符串" function_suite return [expression] 文档字符串说明你函数的主要作用，输入输出的解释，通常可以用三引号表述 我们写一个自己的打印函数： 1234567891011121314def chen_print(s): ''' 我自己的打印函数，在系统打印的字符串前面加我的名字 输入：一个输入字符串 输出：我的字符串 ''' print('chenjian:' + s) return Nones = 'hello'a = chen_print(s)chenjian:hello 函数调用定义一个函数只给了函数一个名称，指定了函数里包含的参数，和代码块结构。仔细观察我们上面的代码会了解到我们是怎么调用自己的函数呢？ 你要注意我们定义了函数 def 程序运行到这里其实并没有进入函数内部，而是直接走过 s = ‘hello’ 下一句就关键了，它看到我们要将 chen_print(s) 绑定到 a 变量身上，可是系统没有这个函数啊，咦，它发现我们自己定义了这个函数，所以它抱着 s 去找我们的自定义函数，自定义函数一看 s 实打实的真s ，再看看自己手上的模型假 s ，表示愿意接受。就进入函数内部运行，完了之后 return 返回一个东西给系统好给 a 交差啊，但是这里的函数就是我什么都不给，默认也是这样。这整个过程就是函数的调用和参数传递 参数传递我们再聊五毛钱的参数传递 12345678910111213141516171819202122232425262728293031323334353637def chen_print(s): ''' 我自己的打印函数，在系统打印的字符串前面加我的名字 输入：一个输入字符串 输出：我的字符串 ''' print('chenjian:' + s) return Nones = 'hello'ss = '[1,2,3]'sss = 10a = chen_print(s)b = chen_print(ss)c = chen_print(sss)chenjian:hellochenjian:[1,2,3]Traceback (most recent call last): File "&lt;ipython-input-8-82b1c8879221&gt;", line 1, in &lt;module&gt; runfile('C:/Users/CC/Desktop/for.py', wdir='C:/Users/CC/Desktop') File "H:\Anaconda\lib\site-packages\spyder_kernels\customize\spydercustomize.py", line 786, in runfile execfile(filename, namespace) File "H:\Anaconda\lib\site-packages\spyder_kernels\customize\spydercustomize.py", line 110, in execfile exec(compile(f.read(), filename, 'exec'), namespace) File "C:/Users/CC/Desktop/for.py", line 16, in &lt;module&gt; c = chen_print(sss) File "C:/Users/CC/Desktop/for.py", line 7, in chen_print print('chenjian:' + s)TypeError: can only concatenate str (not "int") to str 看报错信息就知道是类型出了问题，这也正好暴露了python的一个问题，那就是动态类型，我们可能根本不知道到底输入什么类型，输出什么类型。解决办法也不是没有，只不过都太繁琐了违背了简约的思想，所以，花时间完善你的文档字符串吧，至少让你自己能够看懂你的函数想干嘛。 123456789101112def chen_print(s): ''' 我自己的打印函数，在系统打印的字符串前面加我的名字 输入：一个输入字符串 输出：我的字符串 ''' print('chenjian:' + s) return Nones = 'hello'a = chen_print(s)print(a) 12chenjian:helloNone 在 python 中，类型属于对象，变量是没有类型的： 怎么理解？ 12a = [1,2,3]a = 'sss' 这里看其实对象[1,2,3] 是列表类型，‘sss’就是字符串类型，a只是一个便利贴，开始它贴在列表上，你想访问列表，找便利贴就可以了，后来它又贴在字符串上，之前的列表怎么办？如果没有其他的标签，系统会认为这货是垃圾，干脆回收算了。 可更改(mutable)与不可更改(immutable)对象在 python 中，strings, tuples, 和 numbers 是不可更改的对象，而 list,dict 等则是可以修改的对象。 不可变类型：变量赋值 a=5 后再赋值 a=10，这里实际是新生成一个 int 值对象 10，再让 a 指向它，而 5 被丢弃，不是改变a的值，相当于新生成了a。 可变类型：变量赋值 la=[1,2,3,4] 后再赋值 la[2]=5 则是将 list la 的第三个元素值更改，本身la没有动，只是其内部的一部分值被修改了。 python 函数的参数传递： 不可变类型：类似 c++ 的值传递，如 整数、字符串、元组。如fun（a），传递的只是a的值，没有影响a对象本身。比如在 fun（a）内部修改 a 的值，只是修改另一个复制的对象，不会影响 a 本身。 可变类型：类似 c++ 的引用传递，如 列表，字典。如 fun（la），则是将 la 真正的传过去，修改后fun外部的la也会受影响 python 中一切都是对象，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对象和传可变对象。 上面的话如果听起来比较懵逼，尤其C++只是听说过。那就看看下面的代码，我们详细看看他们之间的区别 python 传不可变对象实例123456789def changeint(a): print('我进来了哦，看看我是谁:',a) a = 10 print('我改变了哦，看看我是谁:',a) b = 1print('我没使用函数前:',b)changeint(b)print('我使用了函数后:',b) 1234我没使用函数前: 1我进来了哦，看看我是谁: 1我改变了哦，看看我是谁: 10我使用了函数后: 1 先甭管它可变不可变，函数的调用和传参这个例子是不是表现的明明白白，安排！再来看，刚进去的时候是把a这个标签贴在了1上面，然后创建了 10 这个对象 ，把a这个标签又贴到了10上面。 python 传可变对象实例123456789def changelist(a): print('我进来了哦，看看我是谁:',a) a[0] = 'chen' print('我改变了哦，看看我是谁:',a) b = [1,2,3]print('我没使用函数前:',b)changelist(b)print('我使用了函数后:',b) 1234我没使用函数前: [1, 2, 3]我进来了哦，看看我是谁: [1, 2, 3]我改变了哦，看看我是谁: ['chen', 2, 3]我使用了函数后: ['chen', 2, 3] 看到区别了吗，b本身也发生的变化，初学很难理解的话，你就当作不可变的对象只是给你我的值，你的权限只能贴贴标签，反正我借给你的东西，别人再问我要你要还我当初的模样。而可变类型就不一样了，你借出去的就是篮球了，用一次脏一次，每次的图画印记都能看到，说不定还你的看样子成了足球也说不定，总之，你要是正好想要一个涂鸦篮球，或者签名篮球，倒也正合你意。 我们再看个例子终结这个话题。 123456789def changetuple(a): print('我进来了哦，看看我是谁:',a) a[0][0] = 'chen' print('我改变了哦，看看我是谁:',a) b = ([1],[2])print('我没使用函数前:',b)changetuple(b)print('我使用了函数后:',b) 1234我没使用函数前: ([1], [2])我进来了哦，看看我是谁: ([1], [2])我改变了哦，看看我是谁: (['chen'], [2])我使用了函数后: (['chen'], [2]) 元组不是不可变吗？仔细体会，注意里面的元素是列表属于可变类型。 参数以下是调用函数时可使用的正式参数类型： 必备参数 关键字参数 默认参数 不定长参数 参数是至关重要的，它是函数的输入口，口都没找到，怎么进行下去呢？ 而且，我们前面谈过了参数类型的问题，还有就是参数数目一定要能对应得上，否则就不是自己人，密码位数都输错，进门的资格都没有。 关键字参数关键字参数和函数调用关系紧密，函数调用使用关键字参数来确定传入的参数值。使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值。 就是说门上直接写明白只有谁能进门，而默认的方式是排队进对应的门，第一个进第一个门，关键字的话，基本就是各找各家，举个栗子： 1234def keyword(name,password): return 'please:&#123;&#125;.&#123;&#125;'.format(name,password)a = keyword(password='123'，name='chen') 12aOut[16]: 'please:chen.123' 默认参数调用函数时，默认参数的值如果没有传入，则被认为是默认值 举个栗子： 1234def keyword(name='chen',password='123'): return 'please:&#123;&#125;.&#123;&#125;'.format(name,password)a = keyword() 12aOut[18]: 'please:chen.123' 这种写法可以极大避免程序报错情况的发生，设置很多习惯上的默认值可以简化用户繁重的输入工作 不定长参数你可能需要一个函数能处理比当初声明时更多的参数。这些参数叫做不定长参数，和上述2种参数不同，声明时不会命名。基本语法如下： 1234def functionname([formal_args,] *var_args_tuple ): "函数_文档字符串" function_suite return [expression] 加了星号（*）的变量名会存放所有未命名的变量参数。星号的意思表达的就是通配符的意思，这样就可以对参数组进行解包，获取所有的参数了 举个栗子： 12345678910111213141516171819&gt;&gt;&gt; def args(arg1,*args): print(arg1) for i in args: print(i) &gt;&gt;&gt; args('1')1&gt;&gt;&gt; args('1','2','3')123&gt;&gt;&gt; args(('1','2'),'3')('1', '2')3&gt;&gt;&gt; args(1,2,3)123 首先匹配第一个参数，然后把剩下的参数都打包成元组给args 123456789def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)a = 1b = 2c = 3args = ('a', 'b')kw = &#123;'x': 99&#125;f1(a,b,c,args,kw) 1a = 1 b = 2 c = 3 args = ((&apos;a&apos;, &apos;b&apos;), &#123;&apos;x&apos;: 99&#125;) kw = &#123;&#125; 在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 对于任意函数，都可以通过类似func(*args, **kw)的形式调用它 递归函数在函数内部，可以调用其他函数。如果一个函数在内部调用自身本身，这个函数就是递归函数。递归是一种思想，它让一些循环处理的复杂问题变得简单。看下面的例子： 12345678# 实现一个从1加到100的递归函数def sum_n(n): if n == 1: return n return sum_n(n-1) + na = sum_n(100)print(a) 15050 大家都听说过高斯大神的故事吧，相信大家也熟知了头尾相加的高效算法，当然你也可以使用比较传统的等差数列求和公式： 12345def sum_n_fuc(n): return ((n+1)*n)/2a = sum_n_fuc(100)print(a) 我们还是先看一看第一个递归函数，其实求1加到100的和，不妨拆解成前99个数的和加100，所以我们调用sum_n(n-1),那么这个函数调用自身的时候发现不满足n==1的条件，那就自身再减1加上当前的n值，这样一直循环下去，终于到了满足条件然后返回结果。这样就可以将问题简化成前99，前98，直到前1个数和，当然了，递归的思想虽然让人简单，机器可不好做了，首先，它要记住每一次的调用吗，就好像什么情况： 123456789101112程序：sum_n(100)你等于多少啊？sum_n(100):我不知道，我给你100，你带着去找sum_n(99)问问,然后加起来就可以了程序：sum_n(99)你等于多少啊？我兜里有100sum_n(99):我不知道，我给你99，你带着去找sum_n(98)问问,然后加起来就可以了程序：sum_n(98)你等于多少啊？我兜里有100,99sum_n(98):我不知道，我给你98，你带着去找sum_n(97)问问,然后加起来就可以了...sum_n(1):我等于1啊程序：卧槽，我兜里有100，99，...1，我加起来看看，咦5050 所以说，递归通常是很占用内存空间的，很多程序也有递归层数限制。那么有没有什么办法来优化递归呢？我们顺序递归就可以了，这样我们加一个存一个，就不会存在递归层数太多内存爆掉了。 我们总结一下，函数式的编程比较简单，无非就是在我们想要实现的某代码块前加def就可以了，我们告诉它，要给它什么东西，通过函数的功能进行加工，最终return出我们想要的东西进行接收。那么有哪些必须要注意的呢？输入参数我们要注意，必选参数、默认参数、可变参数、命名关键字参数和关键字参数位置不能变，必选参数用的比较多，就是按位置排队往里放嘛，默认参数是门上有名字，指定的才能进，还要注意默认参数必须指向不变对象可变就是可以传几个参数，函数对它进行解包操作，关键字参数就是调用的时候我指定给哪个参数进行传，命名关键字参数就是可以向函数传递很多带名字的参数，函数内部可以对其进行解包。初学者，看上面的内容可能比较难以理解，虽然上面的示例已经讲过一遍，不妨我们再看一点示例： 我们想要自定义一个pow函数实现和内置函数一样的功能，我们看看pow是怎么使用的： 12345678&gt;&gt;&gt; help(pow)Help on built-in function pow in module builtins:pow(x, y, z=None, /) Equivalent to x**y (with two arguments) or x**y % z (with three arguments) Some types, such as ints, are able to use a more efficient algorithm when invoked using the three argument form. 我们利用位置参数，也就是必选参数来实现第一个功能： 12345def my_pow(a,b): return a**bc = my_pow(2,5)print(c) 很多文章中将a,b叫做形式参数，就是装样子，占位置的，2，5是实际参数，实实在在的参数，有实际意义的。它们必须位置对应才能没有歧义嘛。这就是位置参数。那么我们同样很容易实现第二个功能求对z的余数 12345def my_pow(a,b,z): return a**b%zc = my_pow(2,5,3)print(c) 可是内置函数中是不需要传入z这个参数的，不传入就是求幂，传入就是求幂余。我们不得不使用默认参数的选项了。 位置参数，默认参数实现两个功能 1234567def my_pow(a,b,z=None): if z == None: return a**b return a**b%zc = my_pow(2,5)print(c) 这样你就完全实现了pow的功能呢，那么要是有更刁难的用户说，他们不习惯 a的b次方这种读法，更喜欢 b次方的a呢？如果调换顺序，那就违背了位置参数的约束，不如这样吧，你干脆告诉我 a 等于多少，b等于多少吧。 那么，我们用关键字参数实现了这个需求 1234567def my_pow(a,b,z=None): if z == None: return a**b return a**b%zc = my_pow(b=5,a=2) //这里就是关键字，直接告诉它，a,b的值，那就不用在乎位置了print(c) 我们还想实现一个不限制用户输入多少个参数，我们都一直求和下去，这里就用到了不定长参数 12345678910def my_pow(*var): print(var) _sum = 0 _len = len(var) for i in var: _sum += i return _sumc = my_pow(1,2,3)print(c) 12(1, 2, 3)6 注意这里是元组，我们得到了这个未知个数的参数，通过解包循环得到所有的值 那么我们还想把这些值的具体含义也传给函数呢？对吧，万一给他了，他不知道含义瞎解包岂不麻烦 12345678910def my_pow(**var): print(var) if var['height'] &lt; 200: print('矮') if var['weight'] &gt; 50: print('挫') if var['money'] &lt; 1000000000: print('穷') my_pow(height=170, weight=62, money = 30) 1234&#123;'height': 170, 'weight': 62, 'money': 30&#125;矮挫穷 泪！！~，以身教学最为致命。函数的辛酸暂时结束。我们下一节介绍文件处理，就算没学编程，文件大家也都是见过的，如何使用编程语言对文件进行编辑操作才是真正进入工作实用的阶段。再谈递归，如果你是学生，要面试递归就必须掌握的很透，很多面试题用递归都是可以达到简化的效果，工作中倒使用的少，主要就是搬砖嘛。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[15.Python的循环]]></title>
    <url>%2Fpython%2F15.Python%E7%9A%84%E5%BE%AA%E7%8E%AF.html</url>
    <content type="text"><![CDATA[书接上文Python 循环语句循环是什么？循环就是重复地去做同一件事情，社会的进步一定是解放了生产力。这个知识点你可以在政治课本或者近代工业革命的历史书中了解到。机械代替人类去做一些繁重的工作由来已久，比如用板车代替肩扛运输货物，那么那种循环重复的事情怎么利用机器替代人力呢？像步枪上膛一样，传统机械通过精心的设计可以完成简单的循环，机械上的确实现了往复运动构件，那么每一个任务都要费时费力进行设计。我们不但想让机器代替人类来做体力活，简单的智力活也想让它来做，甚至更复杂的智力工作，这就是人工智能，后面我会用其他篇章来教学。那么我们有了这个思路，我们可以设计让它能够做加法运算，然后在硬件上设置它可以做一个暂时存储，并可以重复这段流程，那么它就可以做所有的加法工作了。 本章节就向大家介绍Python的循环语句，程序在一般情况下是按顺序执行的。 编程语言提供了各种控制结构，允许更复杂的执行路径。循环语句允许我们执行一个语句或语句组多次，有了条件判断和循环，你就可以实现任意负责的执行流程了。听起来是不是很简单，这是因为人的判断本来就很简单，是或者否，然后重复的事情让机器做，设定好你的判断，就不用担心出问题了。下面是在大多数编程语言中的循环语句的一般形式： Python提供了for循环和while循环，python中的循环简化了很多，这对我们学习而言是很有帮助的，假如你去学C++现在恐怕还在看数据类型，，，，严格的语言让机器利用率更高，轻松的语言让人更轻松。 循环类型 描述 while 循环 在给定的判断条件为 true 时执行循环体，否则退出循环体。 for 循环 重复执行语句 嵌套循环 你可以在while循环体中嵌套for循环 循环控制语句循环控制语句可以更改语句执行的顺序。Python支持以下循环控制语句： 控制语句 描述 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 continue 语句 在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 pass 语句 pass是空语句，是为了保持程序结构的完整性。 接下来，让我们进行循环的代码学习 Python while 循环语句while 简单循环123456789101112131415&gt;&gt;&gt; n = 0&gt;&gt;&gt; while n &lt; 10: print(n) n += 10123456789 while 无限循环123456789&gt;&gt;&gt; while 1: a = input('请输入：') if a == 'q': break 请输入：1请输入：2请输入：3请输入：q 到这里好像忘了讲 input 了 ，不过它很简单，这里的代码就是让你输入的的意思，不妨试试，假如去掉判断，那么它就会一直让你输入，成了一个死循环。无限循环一定要加判断，工作中很多糟糕的事情都是循环的锅，比如你想循环剪切一个文件夹到目标文件夹，后来发现没加判断导致所有的内容都剪切过去了，更可怕的是所有的剪切过去没有增量命名，导致所有文件归一，变成一个文件。这些都是我亲身遭遇的。 刚刚好，这里遇到的一个 break ,一个循环控制语句。 还有两个也顺带 过了吧，continue 和 pass 其实这三个语义上都很好理解，我们来试试代码 12345678910111213141516171819&gt;&gt;&gt; while 1: a = input('请输入：') if a == 'c': continue print(a) if a == 'p': pass print(a) if a == 'b': break print(a) 请输入：1请输入：2请输入：3请输入：c请输入：pp请输入：b 看到三者的区别了吧，我们只聊 pass 和 continue , pass就是过啊，过了之后该怎么运行就怎么运行，continue 就是滚回去排队，这次不让你往下走，直接就运行下一个循环了。 Python for 循环语句Python for循环可以遍历任何序列的项目，如一个列表或者一个字符串。 我没记错的话，前面我们谈过一点 for循环，那么我们开始学习代码的编写吧 for 简单循环12345678910111213141516&gt;&gt;&gt; a = [chr(i) for i in range(97,107)]&gt;&gt;&gt; a['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']&gt;&gt;&gt; for i in a: print(i) abcdefghij 还有一种简单常用的的语法： 12345678910111213&gt;&gt;&gt; for i in range(10): print(i) 0123456789 你会发现循环执行了10次，这要比 while 判断循环次数简单许多，这里的 range 其实并不属于序列结构，更准确来说是个生成器，这个概念以后再说。这里你只需要简单知道，生成器就是每次给你一个，这就避免了你想循环100万次，要弄一个长度为100万的列表可不好。就是有了生成器才让 for 几乎可以完全代替while 通常学完循环以后，老师就会用几道 ACM 来虐虐你了，让你严重怀疑你的智商，然后放弃。我觉得这是不人道的，我曾经也是这样然后放弃学编程，直到几年后自学入手才发现妙不可言。如果后面的题让你觉得超过讲解的难度过多，请不要悲伤，没办法，网上都是出这些题，我已经尽力挑简单的了。 1.python 打印等腰直角三角形（记得一本小说，一会儿等腰直角三角形）12345678910s = int(input('输入列数：'))for i in range(s,0,-1): print('*' * i) 输入列数：4********** 2.打印斐波那契数列1234567891011121314斐波那契数列（Fibonacci sequence），又称黄金分割数列、因数学家列昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：1、1、2、3、5、8、13、21、34、……在数学上，斐波纳契数列以如下被以递推的方法定义：F(1)=1，F(2)=1, F(n)=F(n-1)+F(n-2)（n&gt;=3，n∈N*）Fibonacci = [1,1]s = int(input('循环多少次：'))i = 0while i &lt; s: Fibonacci.append( Fibonacci[-1] + Fibonacci[-2]) i += 1print(Fibonacci)循环多少次：10[1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144] 列表的可变性好用就在这里，你都不需要考虑其他的，直接后面两个加起来就可以了，简单快捷 打印九九乘法表 1234567891011121314for i in range(1,10): for j in range(1,i+1): print("%d*%d=%d\t"%(i,j,i*j),end=" ") print("") 1*1=1 2*1=2 2*2=4 3*1=3 3*2=6 3*3=9 4*1=4 4*2=8 4*3=12 4*4=16 5*1=5 5*2=10 5*3=15 5*4=20 5*5=25 6*1=6 6*2=12 6*3=18 6*4=24 6*5=30 6*6=36 7*1=7 7*2=14 7*3=21 7*4=28 7*5=35 7*6=42 7*7=49 8*1=8 8*2=16 8*3=24 8*4=32 8*5=40 8*6=48 8*7=56 8*8=64 9*1=9 9*2=18 9*3=27 9*4=36 9*5=45 9*6=54 9*7=63 9*8=72 9*9=81 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[13.Python运算符]]></title>
    <url>%2Fpython%2F13.Python%E8%BF%90%E7%AE%97%E7%AC%A6.html</url>
    <content type="text"><![CDATA[书接上文Python 运算符什么是运算符？本章节主要说明Python的运算符。举个简单的例子 4 +5 = 9 。 例子中，4 和 5 被称为操作数，”+” 称为运算符。Python语言支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 接下来让我们一个个来学习Python的运算符。 Python算术运算符以下假设变量： a=10，b=20： 运算符 描述 实例 + 加 - 两个对象相加 a + b 输出结果 30 - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -10 * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 200 / 除 - x除以y b / a 输出结果 2 % 取模 - 返回除法的余数 b % a 输出结果 0 ** 幂 - 返回x的y次幂 a**b 为10的20次方， 输出结果 100000000000000000000 // 取整除 - 返回商的整数部分（向下取整） &gt;&gt;&gt; 9//2 4 &gt;&gt;&gt; -9//2 -5 注意：python2 和 python 3 有一些小改变，比如整数除整数，python2只能得到整数，python3是可以得到浮点数的 Python比较运算符以下假设变量a为10，变量b为20： 运算符 描述 实例 == 等于 - 比较对象是否相等 (a == b) 返回 False。 != 不等于 - 比较两个对象是否不相等 (a != b) 返回 true. &lt;&gt; 不等于 - 比较两个对象是否不相等 (a &lt;&gt; b) 返回 true。这个运算符类似 != 。 &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。 &lt; 小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。 (a &lt; b) 返回 true。 &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。 &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 true。 Python赋值运算符以下假设变量a为10，变量b为20： 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c *= a 等效于 c = c * a /= 除法赋值运算符 c /= a 等效于 c = c / a %= 取模赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c **= a 等效于 c = c ** a //= 取整除赋值运算符 c //= a 等效于 c = c // a Python位运算符按位运算符是把数字看作二进制来进行计算的。Python中的按位运算法则如下： 下表中变量 a 为 60，b 为 13，二进制格式如下： 12345678910111213a = 0011 1100b = 0000 1101-----------------a&amp;b = 0000 1100a|b = 0011 1101a^b = 0011 0001~a = 1100 0011 运算符 描述 实例 &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 | 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101 ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1 。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由 &lt;&lt; 右边的数字指定了移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 &gt;&gt; 右移动运算符：把”&gt;&gt;”左边的运算数的各二进位全部右移若干位，&gt;&gt; 右边的数字指定了移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 位运算符可能不是经常用到，但是需要了解到一个点，那就是如果你想让你的程序更加高效，例如判断奇偶性，利用位运算的特性可以快速得出。也可以用于某些内存限制比较严格的领域，比如嵌入式开发当中，状态判别等等。 Python逻辑运算符Python语言支持逻辑运算符，以下假设变量 a 为 10, b为 20: 运算符 逻辑表达式 描述 实例 and x and y 布尔”与” - 如果 x 为 False，x and y 返回 False，否则它返回 y 的计算值。 (a and b) 返回 20。 or x or y 布尔”或” - 如果 x 是非 0，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。 not not x 布尔”非” - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False Python成员运算符除了以上的一些运算符之外，Python还支持成员运算符，测试实例中包含了一系列的成员，包括字符串，列表或元组。 运算符 描述 实例 in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。 not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 Python身份运算符身份运算符用于比较两个对象的存储单元 运算符 描述 实例 is is 是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 is 与 == 区别： is 用于判断两个变量引用对象是否为同一个(同一块内存空间)， == 用于判断引用变量的值是否相等。 = 与 == 区别： =号是赋值 倒没有难点，主要是很多面试陷阱，容易脱口而出搞混 Python运算符优先级以下表格列出了从最高到最低优先级的所有运算符： 运算符 描述 ** 指数 (最高优先级) ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@) * / % // 乘，除，取模和取整除 + - 加法减法 &gt;&gt; &lt;&lt; 右移，左移运算符 &amp; 位 ‘AND’ ^ | 位运算符 &lt;= &lt; &gt; &gt;= 比较运算符 &lt;&gt; == != 等于运算符 = %= /= //= -= += = *= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not and or 逻辑运算符 我自己使用的时候没有记得那么多，可以直接用括号来帮助自己理解逻辑 运算符的学习还是比较简单，主要是大家应该都有数学基础，理解起来并不难。要注意的是，这些运算符并不是所有的对象都可以使用。这里提到这一点是为了后面理解面对对象做铺垫。后面我们自己设计数据类型，可能会发现无法使用这些运算了，这时候就要我们自己写运算告诉计算机我们想要的操作，叫做运算符重载，其实就是向系统借一下这些运算符。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[14.Python条件判断]]></title>
    <url>%2Fpython%2F14.Python%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD.html</url>
    <content type="text"><![CDATA[书接上文Python 条件语句Python条件语句是通过一条或多条语句的执行结果（True或者False）来决定执行的代码块。什么意思？就是假如你今天出门，下雨就带伞，不下雨就不带伞。就是一个判断问题，如何让程序实现这种判断呢？1234if 判断条件： 执行语句……else： 执行语句…… 多个判断就用 elif 条件复杂就用上一节提到的逻辑运算符或者位运算符进行辅助。 123456789101112if 判断条件1: 执行语句1……elif 判断条件2: 执行语句2……elif 判断条件3: 执行语句3……else: 执行语句4…… num = 2if num &gt;= 0 and num &lt;= 10: print('hello') 看下面的例子： 12345if 1 &amp; 2: print('a')print('b')&gt;&gt;&gt; b 条件判断 if 后的关系表达式一定是一个可判断的表达式，也就是只能回答真或者假两种情况。只要是满足这个条件都可以当作if 判断的表达式。所以你会看到很多死循环是：while 1 ，而不是 while True. 有什么区别呢？ 数字非0都是True，而True的意思就是非空。在一些条件下，非空比非0可要省事的多，所以很多人喜欢用1 来代替 True 下面用一则简单的程序结束条件判断的学习 123456789101112131415s = '下雨'b = '周末'if b == '周末': if s == '下雨': print('带伞出去玩') else: print('不带伞出去玩')else: if s == '下雨': print('带伞去工作') else: print('不带伞也不工作') &gt;&gt;&gt; 带伞出去玩 程序中错误的逻辑是哪一条？试着分析一下并阐述条件逻辑 程序中最有魅力的就是循环了，可以说循环真正意义上是程序的灵魂，没有循环，如何自动化？如何让机器日复一日的被我们所剥削？下一节好好讲一下循环。学完循环才算是入门了吧。判断在循环中的作用是非常重要的，复杂的循环都要借助判断。就像日复一日的火车运行，有了判断的分叉，才能将人送到幸福的终点。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[11.Python的字典]]></title>
    <url>%2Fpython%2F11.Python%E7%9A%84%E5%AD%97%E5%85%B8.html</url>
    <content type="text"><![CDATA[书接上文接下来讲解第三常用数据结构，字典你心目中前二应该怎么排序？在我的理解里，第二当属列表，它灵活多变，非常多的任务都要通过它来完成。第一毫无疑问就是字符串了，严格意义上来讲，字符串不属于数据结构，因为它的组成还是比较单一的，也没有各种花哨的操作，它组成的天生有序，使得它运用广泛。你所见到的一切不都是它所构成的吗？字符串的重要性不言而喻，对于字符串的操作一定要烂熟于心，这样会省去很多麻烦。 Python 字典(Dictionary)字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=&gt;value 对用冒号 : 分割，每个键值对之间用逗号 , 分割，整个字典包括在花括号 {} 中 ,格式如下所示： 1d = &#123;key1 : value1, key2 : value2 &#125; 键一般是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一。 12345&gt;&gt;&gt;dict = &#123;'a': 1, 'b': 2, 'b': '3'&#125;&gt;&gt;&gt; dict['b']'3'&gt;&gt;&gt; dict&#123;'a': 1, 'b': '3'&#125; 值可以取任何数据类型，但键必须是不可变的，如字符串，数字或元组。 又谈到了可变不可变，我们再来理解一下。可变的数据类型其实就是手里多了一根牵牛绳，假如你手里有10根绳子，那就是10头牛，可变就是，你可以随便扔一根，或者再加一根，或者把牛换成羊。你记录的只不过是绳子上的标签，如果牛变成羊了，你把标签换了就可以了，简直太方便管理统计了。 那么字典比较显然了，键就是绳子，所以最后栓的才是显示的值。 列表又如何理解呢？它的键就是它的索引，也就是那个下标啊，这下你知道怎么取值了吧。 访问字典里的值把相应的键放入熟悉的方括弧，是不是很像列表啊 123&gt;&gt;&gt; dict = &#123;'Name': 'Zara', 'Age': 7, 'Class': 'First'&#125;&gt;&gt;&gt; dict['Name']'Zara' 修改字典向字典添加新内容的方法是增加新的键/值对，修改或删除已有键/值对如下实例: 1234567&gt;&gt;&gt; del dict['Name']&gt;&gt;&gt; dict&#123;'Age': 7, 'Class': 'First'&#125;&gt;&gt;&gt; dict.clear()&gt;&gt;&gt; dict&#123;&#125;&gt;&gt;&gt; del dict 这个例子看完，大概精通 del 了吧 警报警报估计很少人仔细看上面的文字内容，为了不踩坑，请仔细理解下面的代码，再回头看看概念 12345678&gt;&gt;&gt; a = &#123;1:1,2:2,3:3,1:4&#125;&gt;&gt;&gt; a&#123;1: 4, 2: 2, 3: 3&#125;&gt;&gt;&gt; b = &#123;[1]:1&#125;Traceback (most recent call last): File "&lt;pyshell#4&gt;", line 1, in &lt;module&gt; b = &#123;[1]:1&#125;TypeError: unhashable type: 'list' 字典内置函数&amp;方法Python字典包含了以下内置函数： 序号 函数及描述 1 len(dict) 计算字典元素个数，即键的总数。 2 str(dict) 输出字典可打印的字符串表示。 3 type(variable) 返回输入的变量类型，如果变量是字典就返回字典类型。 123456789101112&gt;&gt;&gt; dict&#123;'Name': 'Zara', 'Age': 7, 'Class': 'First'&#125;&gt;&gt;&gt; len(dict)3&gt;&gt;&gt; min(dict)'Age'&gt;&gt;&gt; max(dict)'Name'&gt;&gt;&gt; type(dict)&lt;class 'dict'&gt;&gt;&gt;&gt; str(dict)"&#123;'Name': 'Zara', 'Age': 7, 'Class': 'First'&#125;" Python字典包含了以下内置方法： 序号 函数及描述 1 dict.clear() 删除字典内所有元素 2 dict.copy() 返回一个字典的浅复制 3 [dict.fromkeys(seq, val]) 创建一个新字典，以序列 seq 中元素做字典的键，val 为字典所有键对应的初始值 4 dict.get(key, default=None) 返回指定键的值，如果值不在字典中返回default值 5 dict.has_key(key) 如果键在字典dict里返回true，否则返回false。Python3以后删除了has_key()方法 6 dict.items() 以列表返回可遍历的(键, 值) 元组数组 7 dict.keys() 以列表返回一个字典所有的键 8 dict.setdefault(key, default=None) 和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default 9 dict.update(dict2) 把字典dict2的键/值对更新到dict里 10 dict.values() 以列表返回字典中的所有值 11 [pop(key,default]) 删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值。 12 popitem() 随机返回并删除字典中的一对键和值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt;&gt;&gt; dict&#123;'Name': 'Zara', 'Age': 7, 'Class': 'First'&#125;&gt;&gt;&gt; dict.clear()&gt;&gt;&gt; dict&#123;&#125;&gt;&gt;&gt; a = dict.fromkeys(range(97,100),[chr(i) for i in range(97,100)])&gt;&gt;&gt; a&#123;97: ['a', 'b', 'c'], 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; b = a&gt;&gt;&gt; b[97] = 0&gt;&gt;&gt; b&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; c = a.copy()&gt;&gt;&gt; c[97] = 100&gt;&gt;&gt; c&#123;97: 100, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a.get(97)0&gt;&gt;&gt; a.get(90)&gt;&gt;&gt; b = a.get(90)&gt;&gt;&gt; b&gt;&gt;&gt; a.get(90,default='hehe')Traceback (most recent call last): File "&lt;pyshell#14&gt;", line 1, in &lt;module&gt; a.get(90,default='hehe')TypeError: get() takes no keyword arguments &gt;&gt;&gt; 97 in aTrue&gt;&gt;&gt; a&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; b = a.items()&gt;&gt;&gt; bdict_items([(97, 0), (98, ['a', 'b', 'c']), (99, ['a', 'b', 'c'])])&gt;&gt;&gt; type(b)&lt;class 'dict_items'&gt;&gt;&gt;&gt; c = a.keys()&gt;&gt;&gt; cdict_keys([97, 98, 99])&gt;&gt;&gt; d = a.values()&gt;&gt;&gt; ddict_values([0, ['a', 'b', 'c'], ['a', 'b', 'c']])&gt;&gt;&gt; a&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a.setdefault(100,100)100&gt;&gt;&gt; a&#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c'], 100: 100&#125;&gt;&gt;&gt; a = &#123;97: 0, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; b = &#123;97:100&#125;&gt;&gt;&gt; a.update(b)&gt;&gt;&gt; a&#123;97: 100, 98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a.pop(97,66)100&gt;&gt;&gt; a&#123;98: ['a', 'b', 'c'], 99: ['a', 'b', 'c']&#125;&gt;&gt;&gt; a.pop(97,66)66&gt;&gt;&gt; a.popitem()(99, ['a', 'b', 'c'])&gt;&gt;&gt; a&#123;98: ['a', 'b', 'c']&#125; 字典相对而言比较复杂，具有一定的难度，主要的难点在于密集恐惧。看见这么多括号，逗号，冒号，的确是有点冒火。难倒不难，还是比较有层次结构的一种数据类型。重点理解 fromkeys , get , setdefault , pop , popitem 的易错点，还要理解为什么部分方法会有返回值呢？ 其实系统自带的库函数，输入输出还是挺讲究的，不会无缘无故返回一个你用不到的值。除了哪些大家都烂熟的功能，大部分函数，包括你未来要写的代码，一定要有返回值，也就是输出。否则像列表这种原地操作，可是很容易出现问题，程序还不报错，灭BUG就幸苦了。 重点理解字典的可变和函数方法，我们下一节，先休息一下，做点复习。然后正式向程序编程出发 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[10.Python的元组]]></title>
    <url>%2Fpython%2F10.Python%E7%9A%84%E5%85%83%E7%BB%84.html</url>
    <content type="text"><![CDATA[书接上文上一节谈到了列表，这一节聊一下元组，初学的时候，元组和列表类似，所以我们快速过一遍。然后结合前面的内容来一些习题进行巩固。Python 元组Python的元组与列表类似，不同之处在于元组的元素不能修改。元组使用小括号，列表使用方括号。元组创建很简单，只需要在括号中添加元素，并使用逗号隔开即可。 123tup1 = ('physics', 'chemistry', 1997, 2000)tup2 = (1, 2, 3, 4, 5 )tup3 = "a", "b", "c", "d" 注意到tup3没有加括号，这里是省略掉了，后面我们学习函数传参的时候就能够理解这种简写的方式是非常简洁的 创建空元组 1tup1 = () 元组中只包含一个元素时，需要在元素后面添加逗号 1tup1 = (50,) 这里一定要注意，逗号是必不可少的。否则它就是一个数学括号了，为了避免混淆，所以语法定义必须加上。如果你读过大量的代码，你会发现，很多程序员都喜欢在结束的地方加上逗号，这是个不错的习惯，正好可以避免元组生成的问题 访问元组1234567&gt;&gt;&gt; a = (1 , 2 , 3 , 4, )&gt;&gt;&gt; a(1, 2, 3, 4)&gt;&gt;&gt; a[0]1&gt;&gt;&gt; a[::-1](4, 3, 2, 1) 和列表一样 修改元组元组中的元素值是不允许修改的，但我们可以对元组进行连接组合 123456789101112&gt;&gt;&gt; a(1, 2, 3, 4)&gt;&gt;&gt; b[0, 1, 2, 3]&gt;&gt;&gt; b[0] = 4&gt;&gt;&gt; b[4, 1, 2, 3]&gt;&gt;&gt; a[0] = 4Traceback (most recent call last): File "&lt;pyshell#11&gt;", line 1, in &lt;module&gt; a[0] = 4TypeError: 'tuple' object does not support item assignment 看见列表和元组的区别了吗？这是它们的第一个不同点，你也可以元组生产式对列表进行转换再拼接 123&gt;&gt;&gt; a += tuple(b)&gt;&gt;&gt; a(1, 2, 3, 4, 4, 1, 2, 3) 删除元组元组中的元素值是不允许删除的，但我们可以使用del语句来删除整个元组，如下实例: 123456789&gt;&gt;&gt; a = tuple(range(10))&gt;&gt;&gt; a(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)&gt;&gt;&gt; del a&gt;&gt;&gt; aTraceback (most recent call last): File "&lt;pyshell#3&gt;", line 1, in &lt;module&gt; aNameError: name 'a' is not defined 元组运算符与字符串一样，元组之间可以使用 + 号和 * 号进行运算。这就意味着他们可以组合和复制，运算后会生成一个新的元组 Python 表达式 结果 描述 len((1, 2, 3)) 3 计算元素个数 (1, 2, 3) + (4, 5, 6) (1, 2, 3, 4, 5, 6) 连接 (‘Hi!’,) * 4 (‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’) 复制 3 in (1, 2, 3) True 元素是否存在 for x in (1, 2, 3): print x, 1 2 3 迭代 无关闭分隔符任意无符号的对象，以逗号隔开，默认为元组 12345678910&gt;&gt;&gt; a , b = 1 , 2&gt;&gt;&gt; a1&gt;&gt;&gt; b2&gt;&gt;&gt; a , b = b , a&gt;&gt;&gt; a2&gt;&gt;&gt; b1 元组的解包操作非常方便互换两元素的值。通常来讲，你要互换两杯水，你得另找一个杯子做过度。这里深处的原理我们后面再讲 元组内置函数 序号 方法及描述 1 len(tuple) 计算元组元素个数。 2 max(tuple) 返回元组中元素最大值。 3 min(tuple) 返回元组中元素最小值。 4 tuple(seq) 将列表转换为元组。 还记得之前谈过的列表和字符串方法和函数吗，你姑且认为函数就是别人家的高压锅。所以，你可以用这些函数得到元组食材的某些属性，这些属性在很多食材上面都有所体现。所以我们特制这个函数锅。 123456789&gt;&gt;&gt; a = tuple(range(10))&gt;&gt;&gt; a(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)&gt;&gt;&gt; min(a)0&gt;&gt;&gt; max(a)9&gt;&gt;&gt; len(a)10 作业：查询元组函数，并进行实践，注意观察和列表的不同。想一想，为什么要定义这些差别。并复习列表，深刻体会列表的可变特性。 1234567891011121314151617&gt;&gt;&gt; a = 'hello'&gt;&gt;&gt; len(a,)5&gt;&gt;&gt; b = a,&gt;&gt;&gt; len(b)1&gt;&gt;&gt; A,B,C,D,E = tuple(a)&gt;&gt;&gt; A'h'&gt;&gt;&gt; B'e'&gt;&gt;&gt; C'l'&gt;&gt;&gt; D'l'&gt;&gt;&gt; E'o' 注意小细节 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[12.复习]]></title>
    <url>%2Fpython%2F12.%E5%A4%8D%E4%B9%A0.html</url>
    <content type="text"><![CDATA[前面的内容都比较简单，多敲敲就会了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[9.Python 的列表]]></title>
    <url>%2Fpython%2F9.Python%20%E7%9A%84%E5%88%97%E8%A1%A8.html</url>
    <content type="text"><![CDATA[书接上文Python 列表(List)序列是Python中最基本的数据结构。序列中的每个元素都分配一个数字 - 它的位置，或索引，第一个索引是0，第二个索引是1，依此类推。Python有6个序列的内置类型，但最常见的是列表和元组。序列都可以进行的操作包括索引，切片，加，乘，检查成员。此外，Python已经内置确定序列的长度以及确定最大和最小的元素的方法。 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 创建一个列表，只要把逗号分隔的不同的数据项使用方括号括起来即可。如下所示： 123list1 = ['physics', 'chemistry', 1997, 2000]list2 = [1, 2, 3, 4, 5 ]list3 = ["a", "b", "c", "d"] 与字符串的索引一样，列表索引从0开始。列表可以进行截取、组合等。 先来介绍一下常用的列表操作 访问列表中的值用索引和切片进行访问就可以了，和字符串一样 12345&gt;&gt;&gt; a = [1 , 2 , 3 , 4 , 5]&gt;&gt;&gt; a[0]1&gt;&gt;&gt; a[-1]5 12&gt;&gt;&gt; a[::-1][5, 4, 3, 2, 1] 更新列表你可以对列表的数据项进行修改或更新，你也可以使用append()方法来添加列表项，如下所示： 12345&gt;&gt;&gt; list = []&gt;&gt;&gt; list.append('a')&gt;&gt;&gt; list.append('b')&gt;&gt;&gt; list['a', 'b'] 这里可以看到 list 不是保留字，所以可以拿来当变量名，不过不建议你这样做，除非有别的原因，更推荐你用它当前缀和后缀，方便代码阅读 删除列表元素可以使用 del 语句来删除列表的元素，如下实例： 12345&gt;&gt;&gt; a[1, 2, 3, 4, 5]&gt;&gt;&gt; del a[0]&gt;&gt;&gt; a[2, 3, 4, 5] 虽然我们还没有讲到循环，和列表生成，不过讲到这里，有一个知识点提一下，方便大家对于列表的本质进行理解 123&gt;&gt;&gt; a = list(range(20))&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 有这样一个列表 a 如果你想删除前面 10 个 怎么做呢？ 123456&gt;&gt;&gt; a = list(range(20))&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]&gt;&gt;&gt; for i in range(10): del a[i]这个循环的意思是，i的取值从0到10之间，每循环一次，删除一个元素 这样做正确吗？ 我们来看看答案 12&gt;&gt;&gt; a[1, 3, 5, 7, 9, 11, 13, 15, 17, 19] 其实这里就好像删除一个，后面就补上了，这样，del 每次跑过来说我要删第下一个了，却忘了列表已经不是以前的列表了。讲到这里你大概理解一下列表是可变序列这个定义 列表脚本操作符列表对 + 和 * 的操作符与字符串相似。+ 号用于组合列表，* 号用于重复列表。 如下所示： Python 表达式 结果 描述 len([1, 2, 3]) 3 长度 [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] 组合 [‘Hi!’] * 4 [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’] 重复 3 in [1, 2, 3] True 元素是否存在于列表中 for x in [1, 2, 3]: print x, 1 2 3 迭代 Python列表函数&amp;方法Python包含以下函数: 序号 函数 1 len(list) 列表元素个数 2 max(list) 返回列表元素最大值 3 min(list) 返回列表元素最小值 4 list(seq) 将元组转换为列表 1234567891011&gt;&gt;&gt; a[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]&gt;&gt;&gt; len(a)10&gt;&gt;&gt; max(a)19&gt;&gt;&gt; min(a)1&gt;&gt;&gt; s = 'hello world'&gt;&gt;&gt; list(s)['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'] Python包含以下方法: 序号 方法 1 list.append(obj) 在列表末尾添加新的对象 2 list.count(obj) 统计某个元素在列表中出现的次数 3 list.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表） 4 list.index(obj) 从列表中找出某个值第一个匹配项的索引位置 5 list.insert(index, obj) 将对象插入列表 6 [list.pop(index=-1]) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值 7 list.remove(obj) 移除列表中某个值的第一个匹配项 8 list.reverse() 反向列表中元素 9 list.sort(cmp=None, key=None, reverse=False) 对原列表进行排序 123456789101112131415161718192021222324252627282930313233.&gt;&gt;&gt; a = [i for i in range(10)]&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; a.append(10)&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&gt;&gt;&gt; a.count(5)1&gt;&gt;&gt; b = [11 , 12]&gt;&gt;&gt; a.extend(b)&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]&gt;&gt;&gt; a.index(10)10&gt;&gt;&gt; a.insert(20,2)&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2]&gt;&gt;&gt; a.insert(2,20)&gt;&gt;&gt; a[0, 1, 20, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2]&gt;&gt;&gt; a.remove(20)&gt;&gt;&gt; a[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2]&gt;&gt;&gt; a.reverse()&gt;&gt;&gt; a[2, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[0, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]&gt;&gt;&gt; a.sort(reverse = True)&gt;&gt;&gt; a[12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 2, 1, 0] 请注意，列表的 a 的值的变化，我们始终是在 a 的自身上进行操作，并不是复制一份，操作完 a 不变，而是可变的，再说一遍，列表是可变序列。还要注意哪些操作是没有返回结果的，哪些是有返回结果的。 其实，在学习工作的过程中，对于一个函数，最重要的就是，输入？ 输出？ 注释？，有了这三点的函数，才是一个合格的函数 方法？函数？有区别吗？ 暂时你就当作函数是别人的高压锅，方法是你自己家的调料和食材。你看函数通常就是把你的食材放进去，方法就是每次放一点调味料。 12345678910&gt;&gt;&gt; a[12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 2, 1, 0]&gt;&gt;&gt; a.pop()0&gt;&gt;&gt; a[12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 2, 1]&gt;&gt;&gt; a.pop(0)12&gt;&gt;&gt; a[11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 2, 1] 为什么pop要单独放出来？ 确实是忘了。。。 既然拿出来了，顺便抛砖引玉说一下，入栈出栈。就是说你去买香辣酱香饼，烙的一层一层堆那么高，你也不能指定只要最下面的那层吧，会被打死的。最妥的办法就是你买最上面一层，老板烙好饼继续堆上去。这里就是 pop 和 append ，所以不建议你 pop(0)。 这就是数据结构的魅力 还是这个烙饼大叔，因为你的光顾，大叔决定开店让顾客坐下来吃饼。体会大饼的斯过咦。店不大，只能容纳百来号人，大家有序点饼，吃完就走。爆满的时候，后面就等着，吃完一个走一个，走一个进一个，这就是队列。 下一节讲元组，和列表差不太多，我觉得是要比列表学起来简单，敬请期待。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[几种经典的卷积神经网络模型]]></title>
    <url>%2Freading%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html</url>
    <content type="text"><![CDATA[几种经典的卷积神经网络模型1.卷积神经网络介绍卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一1.1输入层 输入层可以处理多维数据 1.2隐含层卷积神经网络的隐含层包含卷积层、池化层和全连接层3类常见构筑，在一些更为现代的算法中可能有Inception模块、残差块（residual block）等复杂构筑。 1.2.1 卷积层（convolutional layer）卷积核（convolutional kernel）:卷积层的功能是对输入数据进行特征提取，其内部包含多个卷积核，组成卷积核的每个元素都对应一个权重系数和一个偏差量（bias vector），类似于一个前馈神经网络的神经元（neuron）卷积层内每个神经元都与前一层中位置接近的区域的多个神经元相连，区域的大小取决于卷积核的大小，在文献中被称为“感受野（receptive field） 卷积层参数:卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数 激励函数（activation function）:ReLU出现以前，Sigmoid函数和双曲正切函数，在一些早期的卷积神经网络研究，例如LeNet-5中，激励函数在池化层之后 1.2.2池化层（pooling layer）：在卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤 Inception模块（Inception module）：是对多个卷积层和池化层进行堆叠所得的隐含层构筑，具体而言，一个Inception模块会同时包含多个不同类型的卷积和池化操作，并使用相同填充使上述操作得到相同尺寸的特征图，随后在数组中将这些特征图的通道进行叠加并通过激励函数 1.2.3全连接层（fully-connected layer）：全连接层位于卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去空间拓扑结构，被展开为向量并通过激励函数，全连接层本身不被期望具有特征提取能力，而是试图利用现有的高阶特征完成学习目标。 1.3输出层： 输出层使用逻辑函数或归一化指数函数（softmax function）输出分类标签 在物体识别（object detection）问题中，输出层可设计为输出物体的中心坐标、大小和分类 在图像语义分割中，输出层直接输出每个像素的分类结果 2.卷积神经网络要解决什么图像数据本身具有特殊性，像素点之间是有关联的，如果直接展开进行学习分类，不但失去了形状空间的信息，像素点过多造成要学习的参数过多，几乎不可能完成训练。 卷积操作尝试去解决这个问题： 卷积层保留输⼊形状，使图像的像素在⾼和宽两个方向上的相关性均可能被有效识别 卷积层通过滑动窗口将同⼀卷积核与不同位置的输⼊重复计算，从而避免参数尺⼨过大。 卷积操作的优势： 稀疏连接 权值共享 怎么去理解，稀疏连接就像是军营等级制度，每一层只负责管理下面几个人，并只向上几个汇报，权值共享就是一个工厂生产，图纸大家一起使用，大大减少了每个人都要画一张的繁重 3.经典的卷积神经网络3.1 LeNet 卷积层块中，每个卷积层都使⽤5 × 5的窗口，并在输出上使⽤sigmoid激活函数。第⼀个卷积层输出通道数为6，第⼆个卷积层输出通道数则增加到16 3.2 AlexNetAlexNet使⽤了8层卷积神经⽹络，并以很⼤的优势赢得了ImageNet 2012图像识别挑战赛 AlexNet与LeNet的设计理念⾮常相似，但也有显著的区别。 第⼀，与相对较小的LeNet相⽐，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。下⾯我们来详细描述这些层的设计。AlexNet第⼀层中的卷积窗口形状是11 × 11。因为ImageNet中绝⼤多数图像的⾼和宽均⽐MNIST图像的⾼和宽⼤10倍以上，ImageNet图像的物体占⽤更多的像素，所以需要更⼤的卷积窗口来捕获物体。第⼆层中的卷积窗口形状减小到5 × 5，之后全采⽤3 × 3。此外，第⼀、第⼆和第五个卷积层之后都使⽤了窗口形状为3 × 3、步幅为2的最⼤池化层。而且，AlexNet使⽤的卷积通道数也⼤于LeNet中的卷积通道数数⼗倍。紧接着最后⼀个卷积层的是两个输出个数为4096的全连接层。这两个巨⼤的全连接层带来将近1GB的模型参数。由于早期显存的限制，最早的AlexNet使⽤双数据流的设计使⼀个GPU只需要处理⼀半模型。幸运的是，显存在过去⼏年得到了长足的发展，因此通常我们不再需要这样的特别设计了。 第⼆，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。⼀⽅⾯，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另⼀⽅⾯，ReLU激活函数在不同的参数初始化⽅法下使模型更容易训练。这是由于当sigmoid激活函数输出极接近0或1时，这些区域的梯度⼏乎为0，从而造成反向传播⽆法继续更新部分模型参数；而ReLU激活函数在正区间的梯度恒为1。因此，若模型参数初始化不当，sigmoid函数可能在正区间得到⼏乎为0的梯度，从而令模型⽆法得到有效训练。 第三，AlexNet通过丢弃法来控制全连接层的模型复杂度。而LeNet并没有使⽤丢弃法。 第四，AlexNet引⼊了⼤量的图像增⼴，如翻转、裁剪和颜⾊变化，从而进⼀步扩⼤数据集来缓解过拟合。 3.3 VGG3.3.1 VGG块VGG块的组成规律是：连续使⽤数个相同的填充为1、窗口形状为3 × 3的卷积层后接上⼀个步幅为2、窗口形状为2 × 2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。 3.3.2 VGG网络 与AlexNet和LeNet⼀样，VGG⽹络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块⾥卷积层个数和输出通道数。全连接模块则跟AlexNet中的⼀样。现在我们构造⼀个VGG⽹络。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个⽹络使⽤了8个卷积层和3个全连接层，所以经常被称为VGG-11。 其实是从VGG开始，大家发现，与其使用更多卷积核，更大的网络，不如使用小卷积核更深的网络来替代。也就是说这时候解决梯度消失和梯度爆炸成为一个必须面对的问题 3.4 NiN3.4.1 Nin块我们知道， 卷积层的输⼊和输出通常是四维数组 （样本， 通道， ⾼， 宽） ， 而全连接层的输⼊和输出则通常是⼆维数组 （样本， 特征） 。 如果想在全连接层后再接上卷积层， 则需要将全连接层的输出变换为四维。 回忆在 “多输⼊通道和多输出通道” ⼀节⾥介绍的1×1卷积层。 它可以看成全连接层， 其中空间维度 （⾼和宽） 上的每个元素相当于样本， 通道相当于特征。 因此， NiN使⽤1×1卷积层来替代全连接层， 从而使空间信息能够⾃然传递到后⾯的层中去。 下图对⽐了NiN同AlexNet和VGG等⽹络在结构上的主要区别。 3.4.2 Nin网络 除使⽤NiN块以外，NiN还有⼀个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。 这⾥的全局平均池化层即窗口形状等于输⼊空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺⼨，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。 3.5 GooLeNet名字就能看出来，是向LeNet致敬 3.5.1 Inception块 Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、3 ×3和5 × 5的卷积层来抽取不同空间尺⼨下的信息，其中中间2个线路会对输⼊先做1 × 1卷积来减少输⼊通道数，以降低模型复杂度。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变通道数。4条线路都使⽤了合适的填充来使输⼊与输出的⾼和宽⼀致。最后我们将每条线路的输出在通道维上连结，并输⼊接下来的层中去。 3.5.2 GoogLeNet网络GoogLeNet跟VGG⼀样， 在主体卷积部分中使⽤5个模块 （block） ， 每个模块之间使⽤步幅为2的3×3最⼤池化层来减小输出⾼宽。 第⼀模块使⽤⼀个64通道的7 × 7卷积层。 第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层， 然后是将通道增⼤3倍的3 × 3卷积层。它对应Inception块中的第⼆条线路。 第三模块串联2个完整的Inception块。 第⼀个Inception块的输出通道数为64+128+32+32 = 256，其中4条线路的输出通道数⽐例为64 : 128 : 32 : 32 = 2 : 4 : 1 : 1。 其中第⼆、 第三条线路先分别将输⼊通道数减小⾄96/192 = 1/2和16/192 = 1/12后，再接上第⼆层卷积层。第⼆个Inception块输出通道数增⾄128 + 192 + 96 + 64 = 480，每条线路的输出通道数之⽐为128 : 192 : 96 : 64 =4 : 6 : 3 : 2。其中第⼆、第三条线路先分别将输⼊通道数减小⾄128/256 = 1/2和32/256 = 1/8。 第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + 208 + 48 + 64 = 512、160+224+64+64 = 512、128+256+64+64 = 512、 112+288+64+64 = 528和256+320+128+128 =832。这些线路的通道数分配和第三模块中的类似，⾸先含3 × 3卷积层的第⼆条线路输出最多通道， 其次是仅含1 × 1卷积层的第⼀条线路， 之后是含5 × 5卷积层的第三条线路和含3 × 3最⼤池化层的第四条线路。 其中第⼆、 第三条线路都会先按⽐例减小通道数。 这些⽐例在各个Inception块中都略有不同。 第五模块有输出通道数为256 + 320 + 128 + 128 = 832和384 + 384 + 128 + 128 = 1024的两个Inception块。 其中每条线路的通道数的分配思路和第三、 第四模块中的⼀致， 只是在具体数值上有所不同。需要注意的是，第五模块的后⾯紧跟输出层，该模块同NiN⼀样使⽤全局平均池化层来将每个通道的⾼和宽变成1。最后我们将输出变成⼆维数组后接上⼀个输出个数为标签类别数的全连接层。GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。 3.6 ResNet理论上，原模型解的空间只是新模型解的空间的⼦空间。也就是说，如果我们能将新添加的层训练成恒等映射f(x) = x，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利⽤批量归⼀化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。 针对这⼀问题， 何恺明等⼈提出了残差⽹络（ResNet）。 它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经⽹络的设计。 3.6.1 残差块让我们聚焦于神经⽹络局部。如图所⽰，设输⼊为x。假设我们希望学出的理想映射为f(x)，从而作为图上⽅激活函数的输⼊。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射f(x) − x。残差映射在实际中往往更容易优化。以本节开头提到的恒等映射作为我们希望学出的理想映射f(x)。我们只需将图中右图虚线框内上⽅的加权运算（如仿射）的权重和偏差参数学成0，那么f(x)即为恒等映射。实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。右图也是ResNet的基础块，即残差块（residual block） 。在残差块中，输⼊可通过跨层的数据线路更快地向前传播。 3.6.2 残差网络 ResNet沿⽤了VGG全3 × 3卷积层的设计。残差块⾥⾸先有2个有相同输出通道数的3 × 3卷积层。每个卷积层后接⼀个批量归⼀化层和ReLU激活函数。然后我们将输⼊跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输⼊形状⼀样，从而可以相加。如果想改变通道数， 就需要引⼊⼀个额外的1 × 1卷积层来将输⼊变换成需要的形状后再做相加运算。 3.7 DenseNetResNet中的跨层连接设计引申出了数个后续⼯作。本节我们介绍其中的⼀个：稠密连接⽹络（DenseNet）。它与ResNet的主要区别如图所⽰。 图中将部分前后相邻的运算抽象为模块A和模块B。与ResNet的主要区别在于，DenseNet⾥模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传⼊模块B后⾯的层。在这个设计⾥，模块A直接跟模块B后⾯的所有层连接在了⼀起。这也是它被称为“稠密连接”的原因。 DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer） 。前者定义了输⼊和输出是如何连结的，后者则⽤来控制通道数，使之不过⼤。 参考博客：https://blog.csdn.net/yaoxunji/article/details/88351396 文中简单介绍了主流卷积网络，接下来我会分两部分对作者的这篇博客进行补充，一是把论文进行翻译解读，二是用不同的框架对论文网络进行复现]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[直接可以用的Python和OpenCV检测及分割图像的目标区域例子]]></title>
    <url>%2Freading%2FOpenCV%E6%A3%80%E6%B5%8B%E5%88%86%E5%89%B2%E5%9B%BE%E5%83%8F.html</url>
    <content type="text"><![CDATA[直接可以用的Python和OpenCV检测及分割图像的目标区域例子第一天老师：你知道么，今天有人问了我一个问题。~.我：什么？老师：他说很难。~.我：关于什么的？老师：图像处理。~.我：喔，你说说看，我确实做了不少图像处理的东西（心里默念，你不知知道你给过我多少图像吗？）老师：好嘞！在用深度学习的时候，比如说面对一张图像，对某个区域感兴趣怎么办？~.我：他傻啊，切割出来啊，只需要训练感兴趣的部分就好啦。 老师：哎，那你给我一个教程，我正好顺手把他的问题解决了。~.我：好的（黑人脸.gif）老师：我回头把图片数据发给你。~.我：好的好的，老师，by the way, 有多少数据啊？老师：也不多，一个U盘够了，这样吧，明天你过来拷一下吧。~.我：好的（hello？一个U盘？） 第二天 有这么一个文件看了里面。。。我要爆炸了。。。 &gt; 598M * 15 = 8970M = 8.97G 我的个妈呀。 打开一看 全是密密麻麻的——虫子！！！为了视觉体验，自动屏蔽，请大家自行去谷歌：虫子、worm、bug、insects。。。 三天后~.我： 老师， 我就给一个方法啊， 不同的虫子他们可以自己调阈值和方法，我已经有写说明文件。老师： 好的，我看看。 考虑到视觉忍受能力，我用一个可爱的虫子做为一个示例，其他的都差不多，大家自行尝试。 目标是把虫子区域抠出来 环境：例图：谷歌，可爱的虫子–image软件：Anaconda 4.20，Opencv3.2OpenCv的安装：1.1安装Python3.601.2下载安装opencv3.2 具体思路如下：1.获取图片，这个简单哈123img_path = r'C:\Users\aixin\Desktop\chongzi.png'img = cv2.imread(img_path)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) 看，这不就是你处理初始的样子？ 2.转换灰度并去噪声12gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)blurred = cv2.GaussianBlur(gray, (9, 9),0) 我们可以得到这两张图，第一张是灰度图，第二张是去噪之后的，另外说一下，去噪咱们有很多种方法，均值滤波器、高斯滤波器、中值滤波器、双边滤波器等。 这里取高斯是因为高斯去噪效果是最好的。 3.提取图像的梯度12345gradX = cv2.Sobel(gray, ddepth=cv2.CV_32F, dx=1, dy=0)gradY = cv2.Sobel(gray, ddepth=cv2.CV_32F, dx=0, dy=1)gradient = cv2.subtract(gradX, gradY)gradient = cv2.convertScaleAbs(gradient) 以Sobel算子计算x，y方向上的梯度，之后在x方向上减去y方向上的梯度，通过这个减法，我们留下具有高水平梯度和低垂直梯度的图像区域。 此时，我们会得到 4.我们继续去噪声 考虑到图像的孔隙 首先使用低通滤泼器平滑图像, 这将有助于平滑图像中的高频噪声。 低通滤波器的目标是降低图像的变化率。如将每个像素替换为该像素周围像素的均值， 这样就可以平滑并替代那些强度变化明显的区域。 对模糊图像二值化，顾名思义，就是把图像数值以某一边界分成两种数值，细节我会附在文章底部，如果还是不懂，去cao文档吧。 12blurred = cv2.GaussianBlur(gradient, (9, 9),0)(_, thresh) = cv2.threshold(blurred, 90, 255, cv2.THRESH_BINARY) 此时，我们会得到 其实就算手动分割我们也是需要找到一个边界吧，可以看到轮廓出来了，但是我们最终要的是整个轮廓，所以内部小区域就不要了 5.图像形态学（牛逼吧、唬人的）在这里我们选取ELLIPSE核，采用CLOSE操作，具体细节你依旧可以参考我的附录文档，及拓展。 12kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (25, 25))closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel) 此时，我们会得到 6.细节刻画从上图我们可以发现和原图对比，发现有细节丢失，这会干扰之后的昆虫轮廓的检测，要把它们扩充，分别执行4次形态学腐蚀与膨胀（附录文档） 12closed = cv2.erode(closed, None, iterations=4)closed = cv2.dilate(closed, None, iterations=4) 此时，我们会得到 7.找出昆虫区域的轮廓此时用cv2.findContours()函数第一个参数是要检索的图片，必须是为二值图，即黑白的（不是灰度图） 1234567891011121314(_, cnts, _) = cv2.findContours( 参数一： 二值化图像 closed.copy(), 参数二：轮廓类型 # cv2.RETR_EXTERNAL, #表示只检测外轮廓 # cv2.RETR_CCOMP, #建立两个等级的轮廓,上一层是边界 # cv2.RETR_LIST, #检测的轮廓不建立等级关系 # cv2.RETR_TREE, #建立一个等级树结构的轮廓 # cv2.CHAIN_APPROX_NONE, #存储所有的轮廓点，相邻的两个点的像素位置差不超过1 参数三：处理近似方法 # cv2.CHAIN_APPROX_SIMPLE, #例如一个矩形轮廓只需4个点来保存轮廓信息 # cv2.CHAIN_APPROX_TC89_L1, # cv2.CHAIN_APPROX_TC89_KCOS ) 8.画出轮廓找到轮廓了，接下来，要画出来的，即用cv2.drawContours()函数。 123456789c = sorted(cnts, key=cv2.contourArea, reverse=True)[0]# compute the rotated bounding box of the largest contourrect = cv2.minAreaRect(c)box = np.int0(cv2.boxPoints(rect))# draw a bounding box arounded the detected barcode and display the imagedraw_img = cv2.drawContours(img.copy(), [box], -1, (0, 0, 255), 3)cv2.imshow("draw_img", draw_img) 此时，我们会得到 9.裁剪出来就完成啦方法嘛，这不就是么，找到这四个点切出来就好啦我们放大一点看一下细节 12345678910Xs = [i[0] for i in box]Ys = [i[1] for i in box]x1 = min(Xs)x2 = max(Xs)y1 = min(Ys)y2 = max(Ys)hight = y2 - y1width = x2 - x1crop_img= img[y1:y1+hight, x1:x1+width]cv2.imshow('crop_img', crop_img) 其实，box里保存的是绿色矩形区域四个顶点的坐标。 我将按下图红色矩形所示裁剪昆虫图像。找出四个顶点的x，y坐标的最大最小值。新图像的高=maxY-minY，宽=maxX-minX 终于我们得到了可爱的小虫子。得到了目标区域，那么你想拿它干什么就干什么！我不管你哈。 考虑到现在的python教程一般都是一上来就是list、tuple什么的，而不是文件的读写和保存，包括批量读取等等，我特地加入了python版的文件批量读写和保存等附录文件。 快快快、夸我！ 附录1.实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#-*- coding: UTF-8 -*- '''Author: Steve WangTime: 2017/12/8 10:00Environment: Python 3.6.2 |Anaconda 4.3.30 custom (64-bit) Opencv 3.3'''import cv2import numpy as npdef get_image(path): #获取图片 img=cv2.imread(path) gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) return img, gray def Gaussian_Blur(gray): # 高斯去噪 blurred = cv2.GaussianBlur(gray, (9, 9),0) return blurred def Sobel_gradient(blurred): # 索比尔算子来计算x、y方向梯度 gradX = cv2.Sobel(blurred, ddepth=cv2.CV_32F, dx=1, dy=0) gradY = cv2.Sobel(blurred, ddepth=cv2.CV_32F, dx=0, dy=1) gradient = cv2.subtract(gradX, gradY) gradient = cv2.convertScaleAbs(gradient) return gradX, gradY, gradientdef Thresh_and_blur(gradient): blurred = cv2.GaussianBlur(gradient, (9, 9),0) (_, thresh) = cv2.threshold(blurred, 90, 255, cv2.THRESH_BINARY) return thresh def image_morphology(thresh): # 建立一个椭圆核函数 kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (25, 25)) # 执行图像形态学, 细节直接查文档，很简单 closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel) closed = cv2.erode(closed, None, iterations=4) closed = cv2.dilate(closed, None, iterations=4) return closed def findcnts_and_box_point(closed): # 这里opencv3返回的是三个参数 (_, cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) c = sorted(cnts, key=cv2.contourArea, reverse=True)[0] # compute the rotated bounding box of the largest contour rect = cv2.minAreaRect(c) box = np.int0(cv2.boxPoints(rect)) return boxdef drawcnts_and_cut(original_img, box): # 因为这个函数有极强的破坏性，所有需要在img.copy()上画 # draw a bounding box arounded the detected barcode and display the image draw_img = cv2.drawContours(original_img.copy(), [box], -1, (0, 0, 255), 3) Xs = [i[0] for i in box] Ys = [i[1] for i in box] x1 = min(Xs) x2 = max(Xs) y1 = min(Ys) y2 = max(Ys) hight = y2 - y1 width = x2 - x1 crop_img = original_img[y1:y1+hight, x1:x1+width] return draw_img, crop_img def walk(): img_path = r'C:\Users\aixin\Desktop\chongzi.png' save_path = r'C:\Users\aixin\Desktop\chongzi_save.png' original_img, gray = get_image(img_path) blurred = Gaussian_Blur(gray) gradX, gradY, gradient = Sobel_gradient(blurred) thresh = Thresh_and_blur(gradient) closed = image_morphology(thresh) box = findcnts_and_box_point(closed) draw_img, crop_img = drawcnts_and_cut(original_img,box) # 暴力一点，把它们都显示出来看看 cv2.imshow('original_img', original_img) cv2.imshow('blurred', blurred) cv2.imshow('gradX', gradX) cv2.imshow('gradY', gradY) cv2.imshow('final', gradient) cv2.imshow('thresh', thresh) cv2.imshow('closed', closed) cv2.imshow('draw_img', draw_img) cv2.imshow('crop_img', crop_img) cv2.waitKey(20171219) cv2.imwrite(save_path, crop_img)walk() 附录2.本篇文章精华函数说明1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 用来转化图像格式的img = cv2.cvtColor(src, COLOR_BGR2HSV # BGR----&gt;HSV COLOR_HSV2BGR # HSV----&gt;BGR ...)# For HSV, Hue range is [0,179], Saturation range is [0,255] and Value range is [0,255]# 返回一个阈值，和二值化图像，第一个阈值是用来otsu方法时候用的# 不过现在不用了，因为可以通过mahotas直接实现T = ret = mahotas.threshold(blurred)ret, thresh_img = cv2.threshold(src, # 一般是灰度图像 num1, # 图像阈值 num2, # 如果大于或者num1, 像素值将会变成 num2# 最后一个二值化参数 cv2.THRESH_BINARY # 将大于阈值的灰度值设为最大灰度值，小于阈值的值设为0 cv2.THRESH_BINARY_INV # 将大于阈值的灰度值设为0，大于阈值的值设为最大灰度值 cv2.THRESH_TRUNC # 将大于阈值的灰度值设为阈值，小于阈值的值保持不变 cv2.THRESH_TOZERO # 将小于阈值的灰度值设为0，大于阈值的值保持不变 cv2.THRESH_TOZERO_INV # 将大于阈值的灰度值设为0，小于阈值的值保持不变)thresh = cv2.AdaptiveThreshold(src, dst, maxValue, # adaptive_method ADAPTIVE_THRESH_MEAN_C, ADAPTIVE_THRESH_GAUSSIAN_C, # thresholdType THRESH_BINARY, THRESH_BINARY_INV, blockSize=3, param1=5)# 一般是在黑色背景中找白色物体，所以原始图像背景最好是黑色# 在执行找边缘的时候，一般是threshold 或者是canny 边缘检测后进行的。# warning:此函数会修改原始图像、# 返回：坐标位置（x,y）, (_, cnts, _) = cv2.findContours(mask.copy(), # cv2.RETR_EXTERNAL, #表示只检测外轮廓 # cv2.RETR_CCOMP, #建立两个等级的轮廓,上一层是边界 cv2.RETR_LIST, #检测的轮廓不建立等级关系 # cv2.RETR_TREE, #建立一个等级树结构的轮廓 # cv2.CHAIN_APPROX_NONE, #存储所有的轮廓点，相邻的两个点的像素位置差不超过1 cv2.CHAIN_APPROX_SIMPLE, #例如一个矩形轮廓只需4个点来保存轮廓信息 # cv2.CHAIN_APPROX_TC89_L1, # cv2.CHAIN_APPROX_TC89_KCOS )img = cv2.drawContours(src, cnts, whichToDraw(-1), color, line)img = cv2.imwrite(filename, dst, # 文件路径，和目标图像文件矩阵 # 对于JPEG，其表示的是图像的质量，用0-100的整数表示，默认为95 # 注意，cv2.IMWRITE_JPEG_QUALITY类型为Long，必须转换成int [int(cv2.IMWRITE_JPEG_QUALITY), 5] [int(cv2.IMWRITE_JPEG_QUALITY), 95] # 从0到9,压缩级别越高，图像尺寸越小。默认级别为3 [int(cv2.IMWRITE_PNG_COMPRESSION), 5]) [int(cv2.IMWRITE_PNG_COMPRESSION), 9])# 如果你不知道用哪个flags，毕竟太多了哪能全记住，直接找找。寻找某个函数或者变量events = [i for i in dir(cv2) if 'PNG' in i]print( events )寻找某个变量开头的flagsflags = [i for i in dir(cv2) if i.startswith('COLOR_')]print flags批量读取文件名字import osfilename_rgb = r'C:\Users\aixin\Desktop\all_my_learning\colony\20170629'for filename in os.listdir(filename_rgb): #listdir的参数是文件夹的路径 print (filename) ————————————————版权声明：本文为CSDN博主「BRUCE_WUANG」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/sinat_36458870/article/details/78825571 感谢博主分享，由于我的OpenCV版本不一样，所以有部分修改，附上我的代码如下： 1234567891011121314151617import cv2cv2.__version__Out[]: '4.1.0' def findcnts_and_box_point(closed): ''' 注意：这里opencv4.1.0 返回的是两个个参数 ''' (cnts, _) = cv2.findContours(closed.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) c = sorted(cnts, key=cv2.contourArea, reverse=True)[0] # compute the rotated bounding box of the largest contour rect = cv2.minAreaRect(c) box = np.int0(cv2.boxPoints(rect)) return box 源文档说明： 12345678§ findContours() [1/2]void cv::findContours ( InputArray image,OutputArrayOfArrays contours,OutputArray hierarchy,int mode,int method,Point offset = Point() ) 1contours, hierarchy = cv.findContours(image, mode, method[, contours[, hierarchy[, offset]]])]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[7.Python的随机数模块]]></title>
    <url>%2Fpython%2F7.Python%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%95%B0%E6%A8%A1%E5%9D%97.html</url>
    <content type="text"><![CDATA[书接上文随机数可以用于数学，游戏，安全等领域中，还经常被嵌入到算法中，用以提高算法效率，并提高程序的安全性。Python随机数函数 函数 描述 choice(seq) 从序列的元素中随机挑选一个元素，比如random.choice(range(10))，从0到9中随机挑选一个整数。 randrange ([start,] stop [,step]) 从指定范围内，按指定基数递增的集合中获取一个随机数，基数默认值为 1 random() 随机生成下一个实数，它在[0,1)范围内。 [seed(x)] 改变随机数生成器的种子seed。如果你不了解其原理，你不必特别去设定seed，Python会帮你选择seed。 shuffle(lst) 将序列的所有元素随机排序 uniform(x, y) 随机生成下一个实数，它在[x,y]范围内。 sample 序列中取出指定个数 123456789101112131415&gt;&gt;&gt; import random&gt;&gt;&gt; random.choice(range(10))0&gt;&gt;&gt; random.randrange(1,10,3)1&gt;&gt;&gt; random.random()0.023248875506237332&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; random.shuffle(a)&gt;&gt;&gt; a[2, 1, 3]&gt;&gt;&gt; random.uniform(1,10)3.86057585107958&gt;&gt;&gt; random.sample(range(10),k=3)[4, 9, 3] 利用随机数模块还可以进行概率分布的模拟，可以用在其他数值分析领域，例如金融，包括机器学习的初始化 本来想深入的，但是实际上用到这个模块也就上面的这些，我个人用的较多的是：choice , random , shuffle —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[8.Python的字符串]]></title>
    <url>%2Fpython%2F8.Python%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2.html</url>
    <content type="text"><![CDATA[书接上文在学习新的内容之前，我们做一个小校验，看看自己前面的数值类型是否掌握。 123456789&gt; 1.把你的名字拼音用编码数字进行输出&gt; 2.把上一题的编码解析出来，你或许以后可以用这个当成你的某网站密码&gt; 3.数字3.1向上取整&gt; 4.数字4.9向下取整&gt; 5.数字5.555保留两位小数&gt; 6.把你名字的数字编码转为二进制，八进制，十六进制&gt; 7.将数字8.88转为整型&gt; 8.将字符串的python语句转为正常语句，再赋给一个字符串&gt; Python 字符串字符串是 Python 中最常用的数据类型。我们可以使用引号(‘或”)来创建字符串。 创建字符串很简单，只要为变量分配一个值即可。例如： 12s1 = 'Hello's2 = "world" Python访问字符串中的值Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 Python访问子字符串，可以使用方括号来截取字符串，如下实例： 12345678910111213&gt;&gt;&gt; a = 'Hello world'&gt;&gt;&gt; a[0]'H'&gt;&gt;&gt; a[-1]'d'&gt;&gt;&gt; a[0:-1]'Hello worl'&gt;&gt;&gt; a[:]'Hello world'&gt;&gt;&gt; a[::-1]'dlrow olleH'&gt;&gt;&gt; a[::2]'Hlowrd' 注意是怎么将字符串反转的语法。 Python字符串更新123456789&gt;&gt;&gt; a = 'Hello world'&gt;&gt;&gt; a[0] = 'C'Traceback (most recent call last): File "&lt;pyshell#2&gt;", line 1, in &lt;module&gt; a[0] = 'C'TypeError: 'str' object does not support item assignment&gt;&gt;&gt; b = a[0] + 'C'&gt;&gt;&gt; b'HC' 字符串是不能直接修改的，原因和总结，学习完基本类型后会进行说明。但是你可以访问得到它的值然后赋给其他变量 Python转义字符 转义字符 描述 \(在行尾时) 续行符 \\ 反斜杠符号 \‘ 单引号 \“ 双引号 \a 响铃 \b 退格(Backspace) \e 转义 \000 空 \n 换行 \v 纵向制表符 \t 横向制表符 \r 回车 \f 换页 \oyy 八进制数，yy代表的字符，例如：\o12代表换行 \xyy 十六进制数，yy代表的字符，例如：\x0a代表换行 \other 其它的字符以普通格式输出 用的较多的是 \n 和 \t Python字符串运算符下表实例变量 a 值为字符串 “Hello”，b 变量值为 “Python”： 操作符 描述 实例 + 字符串连接 &gt;&gt;&gt;a + b ‘HelloPython’ * 重复输出字符串 &gt;&gt;&gt;a * 2 ‘HelloHello’ [] 通过索引获取字符串中字符 &gt;&gt;&gt;a[1] ‘e’ [ : ] 截取字符串中的一部分 &gt;&gt;&gt;a[1:4] ‘ell’ in 成员运算符 - 如果字符串中包含给定的字符返回 True &gt;&gt;&gt;”H”in a True not in 成员运算符 - 如果字符串中不包含给定的字符返回 True &gt;&gt;&gt;”M” not in a True r/R 原始字符串 - 原始字符串：所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字符。 &gt;&gt;&gt;print(r’\n’ \n)&gt;&gt;&gt; print(R’\n’ ) % 格式字符串 请看下一章节 Python 字符串格式化如果我们想让字符串可以随时变化，就像很多网站当你登录的时候都会说，欢迎您，[你的用户名]，那就要用到字符串格式化，这样表达有点抽象，也就是说你想让字符串哪里变化，不妨在那个位置上留点东西，像图书馆占位置一样。也不能随便留，你要留点象征性的东西嘛，这样它看到你留的记号就知道这里是给它占的位置。 python 字符串格式化符号: 符 号 描述 %c 格式化字符及其ASCII码 %s 格式化字符串 %d 格式化整数 %u 格式化无符号整型 %o 格式化无符号八进制数 %x 格式化无符号十六进制数 %X 格式化无符号十六进制数（大写） %f 格式化浮点数字，可指定小数点后的精度 %e 用科学计数法格式化浮点数 %E 作用同%e，用科学计数法格式化浮点数 %g %f和%e的简写 %G %F 和 %E 的简写 %p 用十六进制数格式化变量的地址 格式化操作符辅助指令: 符号 功能 * 定义宽度或者小数点精度 - 用做左对齐 + 在正数前面显示加号( + ) 在正数前面显示空格 # 在八进制数前面显示零(‘0’)，在十六进制前面显示’0x’或者’0X’(取决于用的是’x’还是’X’) 0 显示的数字前面填充’0’而不是默认的空格 % ‘%%’输出一个单一的’%’ (var) 映射变量(字典参数) m.n. m 是显示的最小总宽度,n 是小数点后的位数(如果可用的话) Python三引号python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 三引号的语法是一对连续的单引号或者双引号（通常都是成对的用） 那些地方使用较多呢？通常在大段大段的说明使用较多，根本不像注释了，几乎就是帮助文档。还有就是很复杂的长字符串，比如下面这样的html页面，和SQL执行语句，常规的单双引简直没法处理 1234567891011121314151617errHTML = '''&lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;Friends CGI Demo&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;&lt;H3&gt;ERROR&lt;/H3&gt;&lt;B&gt;%s&lt;/B&gt;&lt;P&gt;&lt;FORM&gt;&lt;INPUT TYPE=button VALUE=BackONCLICK="window.history.back()"&gt;&lt;/FORM&gt;&lt;/BODY&gt;&lt;/HTML&gt;'''cursor.execute('''CREATE TABLE users ( login VARCHAR(8), uid INTEGER,prid INTEGER)''') Unicode 字符串12&gt;&gt;&gt; u'Hello World !'u'Hello World !' python的字符串内建函数这部分内容较多，先附上总表，然后我们一起在交互环境中多敲敲，逐步攻破 方法 描述 string.capitalize() 把字符串的第一个字符大写 string.center(width) 返回一个原字符串居中,并使用空格填充至长度 width 的新字符串 string.count(str, beg=0, end=len(string)) 返回 str 在 string 里面出现的次数，如果 beg 或者 end 指定则返回指定范围内 str 出现的次数 string.decode(encoding=’UTF-8’, errors=’strict’) 以 encoding 指定的编码格式解码 string，如果出错默认报一个 ValueError 的 异 常 ， 除非 errors 指 定 的 是 ‘ignore’ 或 者’replace’ string.encode(encoding=’UTF-8’, errors=’strict’) 以 encoding 指定的编码格式编码 string，如果出错默认报一个ValueError 的异常，除非 errors 指定的是’ignore’或者’replace’ string.endswith(obj, beg=0, end=len(string)) 检查字符串是否以 obj 结束，如果beg 或者 end 指定则检查指定的范围内是否以 obj 结束，如果是，返回 True,否则返回 False. string.expandtabs(tabsize=8) 把字符串 string 中的 tab 符号转为空格，tab 符号默认的空格数是 8。 string.find(str, beg=0, end=len(string)) 检测 str 是否包含在 string 中，如果 beg 和 end 指定范围，则检查是否包含在指定范围内，如果是返回开始的索引值，否则返回-1 string.format() 格式化字符串 string.index(str, beg=0, end=len(string)) 跟find()方法一样，只不过如果str不在 string中会报一个异常. string.isalnum() 如果 string 至少有一个字符并且所有字符都是字母或数字则返回 True,否则返回 False string.isalpha() 如果 string 至少有一个字符并且所有字符都是字母则返回 True,否则返回 False string.isdecimal() 如果 string 只包含十进制数字则返回 True 否则返回 False. string.isdigit() 如果 string 只包含数字则返回 True 否则返回 False. string.islower() 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False string.isnumeric() 如果 string 中只包含数字字符，则返回 True，否则返回 False string.isspace() 如果 string 中只包含空格，则返回 True，否则返回 False. string.istitle() 如果 string 是标题化的(见 title())则返回 True，否则返回 False string.isupper() 如果 string 中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True，否则返回 False string.join(seq) 以 string 作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串 string.ljust(width) 返回一个原字符串左对齐,并使用空格填充至长度 width 的新字符串 string.lower() 转换 string 中所有大写字符为小写. string.lstrip() 截掉 string 左边的空格 string.maketrans(intab, outtab]) maketrans() 方法用于创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。 max(str) 返回字符串 str 中最大的字母。 min(str) 返回字符串 str 中最小的字母。 string.partition(str) 有点像 find()和 split()的结合体,从 str 出现的第一个位置起,把 字 符 串 string 分 成 一 个 3 元 素 的 元 组 (string_pre_str,str,string_post_str),如果 string 中不包含str 则 string_pre_str == string. string.replace(str1, str2, num=string.count(str1)) 把 string 中的 str1 替换成 str2,如果 num 指定，则替换不超过 num 次. string.rfind(str, beg=0,end=len(string) ) 类似于 find()函数，不过是从右边开始查找. string.rindex( str, beg=0,end=len(string)) 类似于 index()，不过是从右边开始. string.rjust(width) 返回一个原字符串右对齐,并使用空格填充至长度 width 的新字符串 string.rpartition(str) 类似于 partition()函数,不过是从右边开始查找 string.rstrip() 删除 string 字符串末尾的空格. string.split(str=””, num=string.count(str)) 以 str 为分隔符切片 string，如果 num 有指定值，则仅分隔 num+ 个子字符串 string.splitlines([keepends]) 按照行(‘\r’, ‘\r\n’, \n’)分隔，返回一个包含各行作为元素的列表，如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。 string.startswith(obj, beg=0,end=len(string)) 检查字符串是否是以 obj 开头，是则返回 True，否则返回 False。如果beg 和 end 指定值，则在指定范围内检查. string.strip([obj]) 在 string 上执行 lstrip()和 rstrip() string.swapcase() 翻转 string 中的大小写 string.title() 返回”标题化”的 string,就是说所有单词都是以大写开始，其余字母均为小写(见 istitle()) string.translate(str, del=””) 根据 str 给出的表(包含 256 个字符)转换 string 的字符,要过滤掉的字符放到 del 参数中 string.upper() 转换 string 中的小写字母为大写 string.zfill(width) 返回长度为 width 的字符串，原字符串 string 右对齐，前面填充0 下面是我的示例代码，均是用jupyter编写： 1a = 'the answer to life the universe and everything' 1a.capitalize() 1&apos;The answer to life the universe and everything&apos; 1a.count('a',0,20) 11 1b = a.encode(encoding='UTF-8') 1b 1b&apos;the answer to life the universe and everything&apos; 1c = b.decode(encoding='UTF-8') 1c 1&apos;the answer to life the universe and everything&apos; 1a.endswith('g') 1True 1a.startswith('g') 1False 1a.expandtabs(tabsize=8) 1&apos;the answer to life the universe and everything&apos; 1a.find('the',0,42) # 如果是返回开始的索引 10 1fs = '&#123;&#125;'.format('hello') 1fs 1&apos;hello&apos; 1a.index('an') 14 1a.isalnum() 1False 1a.isalpha() 1False 1a.isdecimal() 1False 1a.isdigit() 1False 1d = '0b110' 1d.isalnum() 1True 1d.isdecimal() 1False 1d.isdigit() 1False 1a.islower() 1True 1a.isnumeric() 1False 1a.isspace() 1False 1a.istitle() 1False 1a.isupper() 1False 1' '.join(['a','b']) 1&apos;a b&apos; 1a.ljust(60) 1&apos;the answer to life the universe and everything &apos; 1a.lower() 1&apos;the answer to life the universe and everything&apos; 1a.lstrip() 1&apos;the answer to life the universe and everything&apos; 1a.maketrans('the','CCC') # 两个字符串必须相等长度 1&#123;116: 67, 104: 67, 101: 67&#125; 1ord('t') 1116 1max(a) 1&apos;y&apos; 1min(a) 1&apos; &apos; 1a.partition('and') 1(&apos;the answer to life the universe &apos;, &apos;and&apos;, &apos; everything&apos;) 1a.replace('the','The',a.count('the')) 1&apos;The answer to life The universe and everything&apos; 1a.rfind('a') 132 1a.rindex('a') 132 1a.rjust(60) 1&apos; the answer to life the universe and everything&apos; 1a.rpartition('the') 1(&apos;the answer to life &apos;, &apos;the&apos;, &apos; universe and everything&apos;) 1a.rstrip() 1&apos;the answer to life the universe and everything&apos; 1a.split('a',1) 1[&apos;the &apos;, &apos;nswer to life the universe and everything&apos;] 12345s = '''i \nam a \nstudent''' 1sp1 = s.splitlines() 1sp1 1[&apos;&apos;, &apos;i &apos;, &apos;&apos;, &apos;am a &apos;, &apos;&apos;, &apos;student&apos;] 1sp2 = s.splitlines(True) 1sp2 1[&apos;\n&apos;, &apos;i \n&apos;, &apos;\n&apos;, &apos;am a \n&apos;, &apos;\n&apos;, &apos;student\n&apos;] 1a.strip() 1&apos;the answer to life the universe and everything&apos; 1a.title() 1&apos;The Answer To Life The Universe And Everything&apos; 1a.upper() 1&apos;THE ANSWER TO LIFE THE UNIVERSE AND EVERYTHING&apos; 1a.zfill(60) 1&apos;00000000000000the answer to life the universe and everything&apos; 123strs = "this is string example....wow!!!"trantab = str.maketrans('aeiou','12345')strs.translate(trantab) 1&apos;th3s 3s str3ng 2x1mpl2....w4w!!!&apos; 大概也就这么多了，多加练习就可以了。 —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何使用SVM训练自己的数据]]></title>
    <url>%2Freading%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8SVM%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE.html</url>
    <content type="text"><![CDATA[如何使用SVM训练自己的数据1.分析数据示例数据集使用的是 CSV 文件（以逗号为分隔符的文本文件）数据集：标签文件2.导入相关模块12345678from sklearn.externals import joblib # 保存加载模型时使用from sklearn.model_selection import train_test_split # 划分数据集from sklearn.model_selection import GridSearchCV # 网格搜索最优参数from sklearn.svm import SVC # 导入 SVM 算法模块from sklearn.metrics import classification_report # 生成训练结果报告import pandas as pdimport numpy as np # 这个可以不用，我手滑习惯性都导入，新版的pandas改版了许多兼容numpy的类型，以前是很依赖numpy的，所以普遍教程都是先教你numpy再教你pandas。现在基本上numpy能做的pandas都能做，既独立，又兼容。 3.载入数据12X = pd.read_csv('histocsv.csv').astype('float32')Y = pd.read_csv('histocsv_labels.csv').astype('float32') 因为我的数据里面同字段的数值类型不一样，所以导入的时候统一转换成了 float32 如果你不确定自己的字段是否一致，可以用类似下面的语句进行校验 12345678&gt;&gt;&gt; X.dtypesOut[]: 145 float3235 float3240 float3253 float3274 float3274.1 float32 产看数据维度是否对应 1234&gt;&gt;&gt; X.shapeOut[]: (35153, 257)&gt;&gt;&gt; Y.shapeOut[]: (35153, 1) 取出你感兴趣的字段，我这里因为之前做 CSV 文件的时候，自己每一行结束写了 ‘\n’ ,所以我少取一列 4.取出想要训练的数据12x = X.values[:,:256]y = Y.values.ravel() 这里有一个坑，虽然你的标签数据只是一列，显示的形状也是（None,1),但是实际训练需要的维度是（None,)，尽管表现是一样的，实际上却会报错。所以在取到 Value 的同时还需要 ravel 一下。这里还有一个坑，我们取值的时候，使用了 values ，不使用会报错的，因为这里的默认类型是 Dataframe , 需要转成 dnarray 类型。使用values可以直接转 5.划分数据集12345train_x,test_x,train_y,test_y = train_test_split( x,y, test_size=0.2, shuffle=True, random_state=42) 将你的数据进行划分，默认情况 shuffle 是会打乱的，random_state建议设为42，因为这是宇宙一切问题的终极答案，划分比例随意，实践大多数都是 0.2 ，0.3左右 6.设置网格搜索的超参数1234turn_parameters = [ &#123;'kernel' : ['rbf'], 'gamma':[1e-3, 1e-4], 'C':[1,10,100,1000]&#125;, &#123;'kernel':['linear'], 'C':[1,10,100,1000]&#125;] 7.调用网格搜索模块将上面设置的超参数放入网格搜索，让训练按照你设置的参数进行 1clf = GridSearchCV(SVC(C=1), turn_parameters, cv=5) cv就是交叉验证的意思，5就是5折，意思数据分5块，这样你的训练就有80%的量 8.进行训练1clf.fit(train_x,train_y) fit 虽然是一个简单的函数，使用非常方便，但是有兴趣可以看看内部的源码，结构设计非常独特，整个fit家族貌合神离。 9.输出最佳参数1print(clf.best_params_) 10.预测结果和分类报告12y_true, y_pred = test_y, clf.predict(test_x)print(classification_report(y_true, y_pred)) 11.完整代码12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-"""Created on Wed Aug 21 16:02:53 2019@author: CC"""from sklearn.externals import joblibfrom sklearn import preprocessingimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import classification_reportfrom sklearn.svm import SVCimport numpy as npX = pd.read_csv('histocsv.csv').astype('float32')Y = pd.read_csv('histocsv_labels.csv').astype('float32')x = X.values[:,:256]y = Y.values.ravel()train_x,test_x,train_y,test_y = train_test_split( x,y,test_size=0.25,shuffle=True,random_state=42)turn_parameters = [ &#123;'kernel' : ['rbf'], 'gamma':[1e-3, 1e-4], 'C':[1,10,100,1000]&#125;, &#123;'kernel':['linear'], 'C':[1,10,100,1000]&#125;]clf = GridSearchCV(SVC(C=1), turn_parameters, cv=5)clf.fit(train_x,train_y)print(clf.best_params_)#分类报告y_true, y_pred = test_y, clf.predict(test_x)print(classification_report(y_true, y_pred)) 12.保存和加载模型因为整个训练过程也不是每次都要保存，通常是有了重大突破的时候才需要保存模型，所以简单介绍一下，模型的保存和加载。为方便理解，使用了自带的 iris 花数据集 第一种方法是利用上面导入的 joblib 模块12345678910111213141516171819202122232425from sklearn.externals import joblibfrom sklearn.svm import SVCfrom sklearn import datasets # 定义分类器svm = SVC() # 加载iris数据集iris = datasets.load_iris()# 读取特征X = iris.data# 读取分类标签Y = iris.target # 训练模型svm.fit(X, Y) # 保存成sklearn自带的文件格式joblib.dump(svm, 'svm.pkl') # 加载svm.pklnew_svm = joblib.load('svm.pkl')print(new_svm.predict(X[0:1])) 第二种方法是使用pickle模块这个泡菜模块经常用来做数据打包 123456789101112131415161718192021222324252627import picklefrom sklearn.svm import SVCfrom sklearn import datasets # 定义分类器svm = SVC() # 加载iris数据集iris = datasets.load_iris()# 读取特征X = iris.data# 读取分类标签Y = iris.target # 训练模型svm.fit(X, Y) # 保存成python支持的文件格式picklewith open('svm.pickle', 'wb') as fw: pickle.dump(svm, fw) # 加载svm.picklewith open('svm.pickle', 'rb') as fr: new_svm = pickle.load(fr) print(new_svm.predict(X[0:1])) 13.知识扩展SVM是机器学习中分类和回归的算法之一，中文译为支持向量机。 SVM用于分类，名称为SVC，用于回归，名称为SCR SVM的基本概念 分隔超平面：将数据集分割开来的直线叫做分隔超平面。 超平面：如果数据集是N维的，那么就需要N-1维的某对象来对数据进行分割。该对象叫做超平面，也就是分类的决策边界。 间隔：一个点到分割面的距离，称为点相对于分割面的距离。 数据集中所有的点到分割面的最小间隔的2倍，称为分类器或数据集的间隔。 最大间隔：SVM分类器是要找最大的数据集间隔。 支持向量：坐落在数据边际的两边超平面上的点被称为支持向量 为什么要求最大间隔： 其实一个机器学习问题可能存在多解，我们想要找到的是最优的那个解，于是我们权衡，首先这个分类的平面要能够正确把两堆数据进行划分，其次我们希望两团数据难以区分的那些数据也就是支持向量距离这个平面的距离要足够远。这样得到的超平面就是最佳平面了。 软间隔和硬间隔：不可避免你的数据可能存在错误数据和采样误差，可能在事实上本来就区分不了，这样再去强行去分就达不到最佳的效果，所以我们允许部分数据分布在间隔的河道里，这就是软间隔，硬要分的话就是硬间隔 sklearn.svc 参数因为我们主要是使用分类模型，就只阐述SVC吧 sklearn中的SVC函数是基于libsvm实现的，所以在参数设置上有很多相似的地方。（PS: libsvm中的二次规划问题的解决算法是SMO）。LIBSVM是台湾大学林智仁(Lin Chih-Jen)教授等开发设计的一个简单、易于使用和快速有效的SVM模式识别与回归的软件包。有关SMO最后介绍 C: float参数 默认值为1.0 错误项的惩罚系数。C越大，即对分错样本的惩罚程度越大，因此在训练样本中准确率越高，但是泛化能力会降低。 kernel: 默认为‘rbf’ 核函数可选参数有： ‘linear’:线性核函数 ‘poly’：多项式核函数 ‘rbf’：径像核函数/高斯核 ‘sigmod’:sigmod核函数 ‘precomputed’:核矩阵 你也可以自定义核函数 degree: int型参数 默认为3 这个参数只对多项式核函数有用，是指多项式核函数的阶数n 如果给的核函数参数是其他核函数，则会自动忽略该参数。 gamma: float参数 默认为auto 核函数系数，只对‘rbf’,‘poly’,‘sigmod’有效。 如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features. coef0: float参数 默认为0.0 核函数中的独立项，只有对‘poly’和‘sigmod’核函数有用，是指其中的参数c probability： bool参数 默认为False 是否启用概率估计。 这必须在调用fit()之前启用，并且会fit()方法速度变慢。 shrinking： bool参数 默认为True 是否采用启发式收缩方式 tol: float参数 默认为1e^-3 svm停止训练的误差精度 cache_size： float参数 默认为200 指定训练所需要的内存，以MB为单位，默认为200MB。 class_weight： 字典类型或者‘balance’字符串。默认为None 给每个类别分别设置不同的惩罚参数C，如果没有给，则会给所有类别都给C=1，即前面参数指出的参数C. 如果给定参数‘balance’，则使用y的值自动调整与输入数据中的类频率成反比的权重。 verbose ： bool参数 默认为False 是否启用详细输出。 此设置利用libsvm中的每个进程运行时设置，如果启用，可能无法在多线程上下文中正常工作。一般情况都设为False，不用管它。 max_iter ： int参数 默认为-1 最大迭代次数，如果为-1，表示不限制 random_state： int型参数 默认为None 属性有哪些：svc.n_support_：各类各有多少个支持向量 svc.support_：各类的支持向量在训练样本中的索引 svc.support_vectors_：各类所有的支持向量 为什么要用核函数？这里用到了一网图，获取链接：https://blog.csdn.net/SummerStoneS/article/details/78551757 如果你的数据在低维空间无法用低维手段分来，那么你可以将数据升维到高维空间用低维手段进行区分，这个核函数就是这个作用。那么为什么不用高维手段来分呢？因为高维表述复杂，我们习惯于用简单的算法来描述复杂现象。比如你说大多数鸟儿都会飞，远比你说除了生病，死亡，残疾，正在休息……的鸟儿都会飞来得轻松，低维表述虽然会损失部分精度，但是我觉得也不是不能接受 SMO的介绍要介绍SMO的话，还是得从SVM说起。 分类器的共同目标都是学一个平面正确分开正反例，就是： SVM引入了支持向量，就当作哪些难以区分的点来看待吧，那么点x到平面的距离就是： 假设超平面可以完全正确地将所有样本分类，则对于任意一个样本（xi，yi）来说都有如下性质： 两个类支持向量到超平面距离之和为： 最大化这个距离就是最小化它的倒数： 这是一个优化问题，可以使用拉格朗日乘子法求解其对偶问题，SVM的对偶问题表达： 由于采用了拉格朗日乘子法，因此该对偶问题还有一个KKT条件约束，即要求： SMO算法思想观察SVM的优化目标我们可以发现其最终的目的是要计算出一组最优的alpha和常数项b的值。SMO算法的中心思想就是每次选出两个alpha进行优化（之所以是两个是因为alpha的约束条件决定了其与标签乘积的累加等于0，因此必须一次同时优化两个，否则就会破坏约束条件），然后固定其他的alpha值。重复此过程，直到达到某个终止条件程序退出并得到我们需要的优化结果。 https://www.jianshu.com/p/eef51f939ace 算法数学推导由于SVM中有核函数的概念，因此我们用Kij来表示在核函数K下向量i和向量j的计算值。现在假定我们已经选出alpha1和alpha2两个待优化项，然后将原优化目标函数展开为与alpha1和alpha2有关的部分和无关的部分： 其中c是与alpha1和alpha2无关的部分，在本次优化中当做常数项处理。由SVM优化目标函数的约束条件： 可以得到： 将优化目标中所有的alpha1都替换为用alpha2表示的形式，得到如下式子： 此时，优化目标中仅含有alpha2一个待优化变量了，我们现在将待优化函数对alpha2求偏导得到如下结果： 已知： 将以上三个条件带入偏导式子中，得到如下结果： 化简后得： 记： 若n&lt;=0则退出本次优化，若n&gt;0则可得到alpha2的更新公式： 此时，我们已经得到了alpha2的更新公式。不过我们此时还需要考虑alpha2的取值范围问题。因为alpha2的取值范围应该是在0到C之间，但是在这里并不能简单地把取值范围限定在0至C之间，因为alpha2的取值不仅仅与其本身的范围有关，也与alpha1，y1和y2有关。设alpha1y1+alpha2y2=k，画出其约束，在这里要分两种情况，即y1是否等于y2。我们在这里先来考虑y1!=y2的情况：在这种情况下alpha1-alpha2=k： 可以看出此时alpha2的取值范围为： 当y1=y2时，alpha1+alpha2=k： 可以看出此时alpha2的取值范围为： 以上，可以总结出alpha2的取值上下界的规律： 故可得到alpha2的取值范围： 由alpha_old1y1+alpha_old2y2=alpha_new1y1+alpha_new2y2可得alpha1的更新公式： 接下来，需要确定常数b的更新公式，在这里首先需要根据“软间隔”下SVM优化目标函数的KKT条件推导出新的KKT条件，得到结果如下： 由于现在alpha的取值范围已经限定在0至C之间，也就是上面KKT条件的第三种情况。接下来我们将第三种KKT条件推广到任意核函数的情境下： 由此我们可以得到常数b的更新公式： 其中Ei是SVM的预测误差，计算式为： 接下来，我们可以根据这些推导对SMO算法进行实现，并且用我们的算法训练一个SVM分类器 抓住其中两个特别重要的点就行了： 如何选取每次迭代的alpha对 如何确定SVM优化程序的退出条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165'''Created on 2018年3月28日@author: IL MARE代码源作者github:https://github.com/yhswjtuILMARE/Machine-Learning-Study-Notes/blob/master/SVM/Lib/SVMLib.py'''import numpy as npimport Lib.RFLib as RFLibdef kernalTransfrom(dataMatrix, vector, kTup): if kTup[0] == "lin": return vector * dataMatrix.transpose() elif kTup[0] == "rbf": delta = dataMatrix - vector K = np.matrix(np.diag(delta * delta.transpose()), dtype=np.float) K = np.exp(K / (-2 * kTup[1] ** 2)) return K else: raise NameError("Kernal Name Error")class osStruct: def __init__(self, dataMatIn, classlabels, C , toler, kTup): self.dataMatrix = np.matrix(dataMatIn, dtype=np.float) self.labelMatrix = np.matrix(classlabels, dtype=np.float).transpose() self.C = C self.toler = toler self.m = self.dataMatrix.shape[0] self.b = 0 self.alphas = np.matrix(np.zeros((self.m, 1)), dtype=np.float) self.eCache = np.matrix(np.zeros((self.m, 2)), dtype=np.float) self.K = np.matrix(np.zeros((self.m, self.m)), dtype=np.float) for i in range(self.m): self.K[i] = kernalTransfrom(self.dataMatrix, self.dataMatrix[i, :], kTup)def selectJRand(i, m): j = i while j == i: j = np.random.randint(0, m, 1)[0] return jdef clipAlpha(alpha, L, H): if alpha &gt;= H: return H elif alpha &lt;= L: return L else: return alphadef calEi(obj, i): fxi = float(np.multiply(obj.alphas, obj.labelMatrix).transpose() * \ obj.K[:, i]) + obj.b Ek = fxi - obj.labelMatrix[i, 0] return float(Ek)def updateEi(obj, i): Ei = calEi(obj, i) obj.eCache[i] = [1, Ei]def selectJIndex(obj, i, Ei): ''' 核心代码 ''' maxJ = -1 maxdelta = -1 Ek = -1 obj.eCache[i] = [1, Ei] vaildEiList = np.nonzero(obj.eCache[:, 0].A)[0] if len(vaildEiList) &gt; 1: for j in vaildEiList: if j == i: continue Ej = calEi(obj, j) delta = np.abs(Ei - Ej) if delta &gt; maxdelta: maxdelta = delta maxJ = j Ek = Ej else: maxJ = selectJRand(i, obj.m) Ek = calEi(obj, maxJ) return Ek, maxJdef innerLoop(obj, i): Ei = calEi(obj, i) if (obj.labelMatrix[i, 0] * Ei &lt; -obj.toler and obj.alphas[i, 0] &lt; obj.C) or \ (obj.labelMatrix[i, 0] * Ei &gt; obj.toler and obj.alphas[i, 0] &gt; 0): Ej, j = selectJIndex(obj, i, Ei) alphaIold = obj.alphas[i, 0].copy() alphaJold = obj.alphas[j, 0].copy() if obj.labelMatrix[i, 0] == obj.labelMatrix[j, 0]: L = max(0, obj.alphas[i, 0] + obj.alphas[j, 0] - obj.C) H = min(obj.C , obj.alphas[i, 0] + obj.alphas[j, 0]) else: L = max(0, obj.alphas[j, 0] - obj.alphas[i, 0]) H = min(obj.C, obj.C - obj.alphas[i, 0] + obj.alphas[j, 0]) if L == H: return 0 eta = obj.K[i, i] + obj.K[j, j] - 2 * obj.K[i, j] if eta &lt;= 0: return 0 obj.alphas[j, 0] += obj.labelMatrix[j, 0] * (Ei - Ej) / eta obj.alphas[j, 0] = clipAlpha(obj.alphas[j, 0], L, H) updateEi(obj, j) if np.abs(obj.alphas[j, 0] - alphaJold) &lt; 0.00001: return 0 obj.alphas[i, 0] += obj.labelMatrix[i, 0] * obj.labelMatrix[j, 0] * (alphaJold - obj.alphas[j, 0]) updateEi(obj, i) b1 = -Ei - obj.labelMatrix[i, 0] * obj.K[i, i] * (obj.alphas[i, 0] - alphaIold) \ - obj.labelMatrix[j, 0] * obj.K[i, j] * (obj.alphas[j, 0] - alphaJold) + obj.b b2 = -Ej - obj.labelMatrix[i, 0] * obj.K[i, j] * (obj.alphas[i, 0] - alphaIold) \ - obj.labelMatrix[j, 0] * obj.K[j, j] * (obj.alphas[j, 0] - alphaJold) + obj.b if obj.alphas[i, 0] &gt; 0 and obj.alphas[i, 0] &lt; obj.C: obj.b = b1 elif obj.alphas[j, 0] &gt; 0 and obj.alphas[j, 0] &lt; obj.C: obj.b = b2 else: obj.b = (b1 + b2) / 2.0 return 1 else: return 0def realSMO(trainSet, trainLabels, C, toler, kTup=('lin', 1.3), maxIter=40): ''' 核心代码 ''' obj = osStruct(trainSet, trainLabels, C, toler, kTup) entrySet = True iterNum = 0 alphapairschanged = 0 while (iterNum &lt; maxIter) and (alphapairschanged &gt; 0 or entrySet): print(iterNum) alphapairschanged = 0 if entrySet: for i in range(obj.m): alphapairschanged += innerLoop(obj, i) if i % 100 == 0: print("full set loop, iter: %d, alphapairschanged: %d, iterNum: %d" % (i, alphapairschanged, iterNum)) iterNum += 1 else: vaildalphsaList = np.nonzero((obj.alphas.A &gt; 0) * (obj.alphas.A &lt; C))[0] for i in vaildalphsaList: alphapairschanged += innerLoop(obj, i) if i % 100 == 0: print("non-bound set loop, iter: %d, alphapairschanged: %d, iterNum: %d" % (i, alphapairschanged, iterNum)) iterNum += 1 if entrySet: entrySet = False elif alphapairschanged == 0: entrySet = True print("iter num: %d" % (iterNum)) return obj.alphas, obj.bdef getSupportVectorandSupportLabel(trainSet, trainLabel, alphas): vaildalphaList = np.nonzero(alphas.A)[0] dataMatrix = np.matrix(trainSet, dtype=np.float) labelMatrix = np.matrix(trainLabel, dtype=np.float).transpose() sv = dataMatrix[vaildalphaList]#得到支持向量 svl = labelMatrix[vaildalphaList] return sv, svldef predictLabel(data, sv, svl, alphas, b, kTup): kernal = kernalTransfrom(sv, np.matrix(data, dtype=np.float), kTup).transpose() fxi = np.multiply(svl.T, alphas[alphas != 0]) * kernal + b return np.sign(float(fxi)) 最后，附上一张本人照片，以缓解学习头昏脑胀]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数值类型作业]]></title>
    <url>%2Fhomework%2Fnumber.html</url>
    <content type="text"><![CDATA[1.把你的名字拼音用编码数字进行输出12345678&gt;&gt;&gt; name = 'chen'&gt;&gt;&gt; a = ord(name[0])&gt;&gt;&gt; b = ord(name[1])&gt;&gt;&gt; c = ord(name[2])&gt;&gt;&gt; d = ord(name[3])&gt;&gt;&gt; s = str(a) + str(b) + str(c) + str(d)&gt;&gt;&gt; s'99104101110' 2.把上一题的编码解析出来，你或许以后可以用这个当成你的某网站密码 1234567&gt;&gt;&gt; a1 = chr(a)&gt;&gt;&gt; b1 = chr(b)&gt;&gt;&gt; c1 = chr(c)&gt;&gt;&gt; d1 = chr(d)&gt;&gt;&gt; yourname = a1 + b1 + c1 + d1&gt;&gt;&gt; yourname'chen' 3.数字3.1向上取整 123&gt;&gt;&gt; import math&gt;&gt;&gt; math.ceil(3.1)4 4.数字4.9向下取整 12&gt;&gt;&gt; math.floor(4.9)4 5.数字5.555保留两位小数 12&gt;&gt;&gt; round(5.555,2)5.55 6.把你名字的数字编码转为二进制，八进制，十六进制 123456789&gt;&gt;&gt; binname = bin(a) + bin(b) + bin(c) + bin(d)&gt;&gt;&gt; octname = oct(a) + oct(b) + oct(c) + oct(d)&gt;&gt;&gt; hexname = hex(a) + hex(b) + hex(c) + hex(d)&gt;&gt;&gt; binname'0b11000110b11010000b11001010b1101110'&gt;&gt;&gt; octname'0o1430o1500o1450o156'&gt;&gt;&gt; hexname'0x630x680x650x6e' 7.将数字8.88转为整型 12&gt;&gt;&gt; int(8.88)8 8.将字符串的python语句转为正常语句，再赋给一个字符串 12345&gt;&gt;&gt; eval(s)414&gt;&gt;&gt; s1 = repr(a + b + c + d)&gt;&gt;&gt; s1'414']]></content>
      <categories>
        <category>作业</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[5.Python的数字类型]]></title>
    <url>%2Fpython%2F5.Python%E7%9A%84%E6%95%B0%E5%AD%97%E7%B1%BB%E5%9E%8B.html</url>
    <content type="text"><![CDATA[书接上文上一篇博客中简单介绍了一下Python的各种变量类型，这一篇我们重点介绍Python的数字类型。数字类型算得上是程序语言的灵魂了，大家都知道计算机通过高低电平的状态，规定高电平为1，低电平为0，用0和1来表示了如此庞大的计算体系。用0和1推演出字符，数字等的表达方式，再用数字，字符组成了如此复杂的程序结构。古老的程序员都是用打孔纸带进行编程，哪有现在这么舒服。作为语言中最重要的一环，数字，不得不首先介绍它，因为没有数字就真的什么都没有了。包括没有在数学的表达上也是0，所以这么世界就是数学，没有数学表达不了的东西。 下面，我们就正式进入数值类型的学习，银行卡余额的数字大家应该还是很关心的。 Python Number(数字)Python Number 数据类型用于存储数值。 数据类型是不允许改变的,这就意味着如果改变 Number 数据类型的值，将重新分配内存空间。 Python 支持三种不同的数值类型： 整型(Int) - 通常被称为是整型或整数，是正或负整数，不带小数点。 浮点型(floating point real values) - 浮点型由整数部分与小数部分组成，浮点型也可以使用科学计数法表示（2.5e2 = 2.5 x 10^2 = 250） 复数(complex numbers) - 复数由实数部分和虚数部分构成，可以用a + bj,或者complex(a,b)表示， 复数的实部a和虚部b都是浮点型。 Python Number 类型转换 语法 解释 int(x [,base ]) 将x转换为一个整数 float(x) 将x转换到一个浮点数 complex(real [,imag ]) 创建一个复数 str(x) 将对象 x 转换为字符串 repr(x) 将对象 x 转换为表达式字符串 eval(str) 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s) 将序列 s 转换为一个元组 list(s) 将序列 s 转换为一个列表 chr(x) 将一个整数转换为一个字符 unichr(x) 将一个整数转换为Unicode字符 ord(x) 将一个字符转换为它的整数值 hex(x) 将一个整数转换为一个十六进制字符串 oct(x) 将一个整数转换为一个八进制字符串 bin(x) 将一个整数转换为一个二进制字符串 我们选讲 repr , eval , chr , ord , hex , oct , bin123a = b = 1c = repr(a+b)d = str(a + b) 12c , type(c)d , type(d) 12(&apos;2&apos;, str) (&apos;2&apos;, str) a + b 是一个表达式，使用 repr 将其转化为了字符串格式，当然也可以使用 str 进行转换。两者有什么区别呢？ repr和str这两个方法都是用于显示的，str是面向用户的，而repr面向程序员。 打印操作会首先尝试str和str内置函数(print运行的内部等价形式)，它通常应该返回一个友好的显示。 repr用于所有其他的环境中：用于交互模式下提示回应以及repr函数，如果没有使用str，会使用print和str。它通常应该返回一个编码字符串，可以用来重新创建对象，或者给开发者详细的显示。 当我们想所有环境下都统一显示的话，可以重构repr方法；当我们想在不同环境下支持不同的显示，例如终端用户显示使用str，而程序员在开发期间则使用底层的repr来显示，实际上str只是覆盖了repr以得到更友好的用户显示。 大概了解一下，不懂没关系。等学完整个课程再来理解就好。 下面的示例为未来王者归来的你所写，提前面个眼缘就行。 12345678910# -*- coding: utf-8 -*-class Test(object): def __init__(self,value='hello world'): self.data = valuet = Test()print(t)&lt;__main__.Test object at 0x00000249BD7180F0&gt; 上面打印类对象并不是很友好，显示的是对象的内存地址 下面我们重构下该类的repr以及str，看看它们俩有啥区别 12345678910111213141516171819202122# -*- coding: utf-8 -*-class Test(object): def __init__(self,value='hello world'): self.data = valuet = Test()print(t)class Test_Repr(Test): def __repr__(self): return 'Repr:&#123;&#125;'.format(self.data) class Test_Str(Test): def __str__(self): return 'Str:&#123;&#125;'.format(self.data)tr = Test_Repr()print(tr)ts = Test_Str()print(ts) 1234567891011121314&lt;__main__.Test object at 0x00000249BD718BA8&gt;Repr:hello worldStr:hello world#似乎后面两个还是一样的？#这里就要用交互环境了In[]:tOut[]: &lt;__main__.Test at 0x249bd718ba8&gt;In[]:trOut[]: Repr:hello worldIn[]:tsOut[10]: &lt;__main__.Test_Str at 0x249bd718860&gt; 相信你一定很好奇后面那一长串的16进制数是什么玩意，它就是变量的存储在内存中的内存地址，查看任何变量的内存地址都可以使用 id 来看 123456id(t)Out[11]: 2515734203304ps：好像是十进制和上面显示的不一样呃，还记得用什么转16进制吗？ 既然这里用到了，那就不说了hex(id(t))Out[12]: '0x249bd718ba8' eval 将字符串转为Python表达式并返回结果假如你要根据你每月的消费情况统计每个月的平均花费，你已经统计好了每个月的消费金额，刚好你计算机上的计算器无法使用，让室友帮你，结果他只给了你一个计算的字符串。编不下去了，直接开始 1234567891011a = 100b = 200s = '(a + b) / 2 ' c = eval(s)print(c)print(type(c))150.0&lt;class 'float'&gt; chr 和 ord12345678910111213141516&gt;&gt;&gt; ord('a')97&gt;&gt;&gt; ord('b')98&gt;&gt;&gt; ord('A')65&gt;&gt;&gt; ord('B')66&gt;&gt;&gt; chr(97)'a'&gt;&gt;&gt; chr(98)'b'&gt;&gt;&gt; chr(65)'A'&gt;&gt;&gt; chr(66)'B' 发现规律了吗，这两个操作可以互相转换。 以后你可能会遇到程序提示：按 q 键退出，或者按 s 键保存，可以试试看这两个字母是什么数字编码。 hex ，oct 和 bin1234567&gt;&gt;&gt; a = 100&gt;&gt;&gt; bin(a)'0b1100100'&gt;&gt;&gt; oct(a)'0o144'&gt;&gt;&gt; hex(a)'0x64' 注意操作后返回的是字符串格式，不再是数字哦 123456&gt;&gt;&gt; int(eval('0b1100100'))100&gt;&gt;&gt; eval('0b100')4&gt;&gt;&gt; type(eval('0b100'))&lt;class 'int'&gt; 这玩意儿写累了，拆开更吧，水一篇。 — 欲知后事是如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[6.Python的math模块]]></title>
    <url>%2Fpython%2F6.Python%E7%9A%84math%E6%A8%A1%E5%9D%97.html</url>
    <content type="text"><![CDATA[书接上文这年头，自己造轮子的还是少啊，当你觉得别人的轮子不好用的时候，试试自己造一个，就会反省道：”真香” 我就是饿死，从这里跳下去，也绝不会用 math 模块的 123&gt;&gt;&gt; import math&gt;&gt;&gt; dir(math)['__doc__', '__loader__', '__name__', '__package__', '__spec__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'gcd', 'hypot', 'inf', 'isclose', 'isfinite', 'isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'log2', 'modf', 'nan', 'pi', 'pow', 'radians', 'remainder', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'tau', 'trunc'] dir 查看 math 查看包中的内容,每遇到一个新的模块，都可以用来看看，不讲了，懒得讲 Python常用数学函数 函数 返回值 abs(x) 返回数字的绝对值，如abs(-10) 返回 10 ceil(x) 返回数字的上入整数，如math.ceil(4.1) 返回 5 cmp(x,y) 如果 x &lt; y 返回 -1, 如果 x == y 返回 0, 如果 x &gt; y 返回 1。已被删除 exp(x) 返回e的x次幂(ex),如math.exp(1) 返回2.718281828459045 fabs(x) 返回数字的绝对值，如math.fabs(-10) 返回10.0 floor(x) 返回数字的下舍整数，如math.floor(4.9)返回 4 log(x) 如math.log(math.e)返回1.0,math.log(100,10)返回2.0 max(x1,x2,…) 返回给定参数的最大值，参数可以为序列。 min(x1,x2,…) 返回给定参数的最小值，参数可以为序列。 modf(x) 返回x的整数部分与小数部分，两部分的数值符号与x相同，整数部分以浮点型表示。 pow(x,y,z) x**y 运算后的值。 round(x,[n]) 浮点数x的四舍五入值，如给出n值，则代表舍入到小数点后的位数。 sqrt(x) 返回数字x的平方根 下面给出部分函数的示例，如果是没有数学基础，第一次看见这些函数，或者想要更多详细的参考，可以参考官方文档 或者，使用帮助命名 help _ :救救我 1234567891011121314151617181920212223242526272829303132333435363738&gt;&gt;&gt; import math&gt;&gt;&gt; help(math)Help on built-in module math:NAME mathDESCRIPTION This module provides access to the mathematical functions defined by the C standard.FUNCTIONS acos(x, /) Return the arc cosine (measured in radians) of x. acosh(x, /) Return the inverse hyperbolic cosine of x. asin(x, /) Return the arc sine (measured in radians) of x. asinh(x, /) Return the inverse hyperbolic sine of x. atan(x, /) Return the arc tangent (measured in radians) of x. atan2(y, x, /) Return the arc tangent (measured in radians) of y/x. Unlike atan(y/x), the signs of both x and y are considered. atanh(x, /) Return the inverse hyperbolic tangent of x. ceil(x, /) Return the ceiling of x as an Integral.# 还有很多 12345678910import mathprint(abs(-10),'\t',math.ceil(1.1),'\t',math.exp(1),'\n', max(1,2),'\t',math.floor(4.5),'\t',math.log(2,4),'\n', pow(2,5,2),'\t',math.sin(math.pi),'\t',math.modf(2.3),'\n', round(4.3334,2),'\t',math.cmp(1,2),'\t',math.cmp(1,1),'\n', math.cmp(2,1),'\t',math.sqrt(4),'\t', )#因为已经移除，会报错AttributeError: module 'math' has no attribute 'cmp' 123456789101112import mathprint(abs(-10),'\t\t',math.ceil(1.1),'\t'*5,math.exp(1),'\n', max(1,2),'\t\t',math.floor(4.5),'\t'*5,math.log(2,4),'\n', pow(2,5,2),'\t\t',math.sin(math.pi),'\t\t',math.modf(2.3),'\n', round(4.3334,2),'\t\t',math.sqrt(4),'\t\t', )10 2 2.718281828459045 2 4 0.5 0 1.2246467991473532e-16 (0.2999999999999998, 2.0) 4.33 2.0 那我们想要比较两个值的时候怎么办呢？ 12345&gt;&gt;&gt; import operator&gt;&gt;&gt; operator.eq('a','b')False&gt;&gt;&gt; operator.eq('a','a')True 需要导入 operator 模块 并且提供了以下内置函数： 123456789101112operator.lt(a, b) operator.le(a, b) operator.eq(a, b) operator.ne(a, b) operator.ge(a, b) operator.gt(a, b) operator.__lt__(a, b) operator.__le__(a, b) operator.__eq__(a, b) operator.__ne__(a, b) operator.__ge__(a, b) operator.__gt__(a, b) math 模块暂时结束探讨，其实我自己使用感觉不是太多，通常用的还是自带的函数，尤其很多数据分析包都有自己的数学模块，不过看 math 包的源码倒是对编程思维的锻炼大有裨益。有时间一起看看咯。 — 欲知后事是如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何调用 OpenCV 读取摄像头图片保存在本地]]></title>
    <url>%2Freading%2FOpenCV%E8%B0%83%E7%94%A8%E6%91%84%E5%83%8F%E5%A4%B4%E6%8B%8D%E7%85%A7.html</url>
    <content type="text"><![CDATA[如何调用 OpenCV 读取摄像头图片保存在本地1.导入OenCV 库1import cv212print(cv2.__version__) #版本的重要性不言而喻4.1.02.创建一个 VideoCapture 对象1cap = cv2.VideoCapture(0) #如果你有其他摄像头，注意换参数 3.递增，用来保存文件名1num = 0 4.主程序12345678910111213141516171819while(cap.isOpened()):#循环读取每一帧 ret_flag, Vshow = cap.read() #返回两个参数，第一个是bool是否正常打开，第二个是照片数组， #如果只设置一个则变成一个tumple包含bool和图片 cv2.imshow("Capture_Test",Vshow) #窗口显示，显示名为 Capture_Test k = cv2.waitKey(1) &amp; 0xFF #每帧数据延时 1ms，延时不能为 0，否则读取的结果会是静态帧 if k == ord('s'): #若检测到按键 ‘s’，打印字符串 ''' 下面的存储路径改为你自己的文件路径，尽量不要有中文，num是上面的递增，保证新图片不会覆盖原图 ''' cv2.imwrite("images/"+ str(num) + ".jpg", Vshow) print(cap.get(3)); #得到长宽 print(cap.get(4)); print("success to save"+str(num)+".jpg") print("-------------------------") num += 1 elif k == ord('q'): #若检测到按键 ‘q’，退出 break 5.完整代码1234567891011121314151617181920import cv2cap = cv2.VideoCapture(0)num = 0while(cap.isOpened()):#循环读取每一帧 ret_flag, Vshow = cap.read() cv2.imshow("Capture_Test",Vshow) k = cv2.waitKey(1) &amp; 0xFF if k == ord('s'): ''' 下面的存储路径改为你自己的文件路径，尽量不要有中文，num是上面的递增，保证新图片不会覆盖原图 ''' cv2.imwrite("images/"+ str(num) + ".jpg", Vshow) print(cap.get(3)); print(cap.get(4)); print("success to save"+str(num)+".jpg") print("-------------------------") num += 1 elif k == ord('q'): break 6.效果展示]]></content>
      <categories>
        <category>一些笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[3.Python基础语法]]></title>
    <url>%2Fpython%2F3.Python%20%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95.html</url>
    <content type="text"><![CDATA[书接上文如果能够顺利地正确打印佛祖的的话，说明你是非常具有编程天赋的人。接下来，我们开始学习 Python 的基础语法，程序为了认识哪些单词是解释器自己的，哪些是程序员写的，规定了一些条条框框来限制大家不要瞎写。哪些是合法的命名规范呢？Python标识符在 Python 里，标识符由字母、数字、下划线组成。 在 Python 中，所有标识符可以包括英文、数字以及下划线(_)，但不能以数字开头。 Python 中的标识符是区分大小写的。 以下划线开头的标识符是有特殊意义的。以单下划线开头 _foo 的代表不能直接访问的类属性，需通过类提供的接口进行访问，不能用 from xxx import * 而导入。 以双下划线开头的 __foo 代表类的私有成员，以双下划线开头和结尾的 foo 代表 Python 里特殊方法专用的标识，如 init() 代表类的构造函数。 Python 可以同一行显示多条语句，方法是用分号 ; 分开，如： 123&gt;&gt;&gt; print('hello') ; print('world')helloworld 如果你学过其他语言，这个分号你一定很熟悉，很多语言都是用它作为语句的结束符，但是Python更加推荐用标准的代码缩进的方式来写程序。要注意空格和TAB键可是不能混用的，否则光凭肉眼可是很难发现问题所在。 行和缩进学习 Python 与其他语言最大的区别就是，Python 的代码块不使用大括号 {} 来控制类，函数以及其他逻辑判断。python 最具特色的就是用缩进来写模块。 缩进的空白数量是可变的，但是所有代码块语句必须包含相同的缩进空白数量，这个必须严格执行。如下所示： 1234if True: print("True")else: print("False") # 缩进不正确，同样等级的 print 你凭什么比上面的要高，是吧 12345$ python test.py File "test.py", line 10 print("False") ^IndentationError: unindent does not match any outer indentation level 错误提示显而易见呢，不但指出了行数，还指出了错误类型，不要忽略这些错误信息，不会的单词就去查，为了以后写BUG 改 BUG 快一点，不能偷懒 当然，光是正确书写标识符还不够，还要记得，程序都会带一群保留字符，这些都是程序的主料，总有一天你会挨个尝个遍，我们已经学了一个 print了，后续的学习之中，会逐步攻破的，不要看他有这么多 Python保留字符下面的列表显示了在Python中的保留字。这些保留字不能用作常数或变数，或任何其他标识符名称。 所有 Python 的关键字只包含小写字母。 and exec not assert finally or break for pass class from print continue global raise def if return del import try elif in while else is with except lambda yield 那么，一定要记住这些关键字吗？答案是：不用的，你只要使用比较明显表达含义的单词来写程序，一般是很难遇到使用关键字命名的情况，何况我们用自动补全的编辑器，要么高亮要么会有提示，这不是个问题，只是学到一定程度可以来看看，有哪些关键字你没有用到，高级程序员的话，基本上花里胡哨地用这些。加油吧。 多行语句Python语句中一般以新行作为语句的结束符。 但是我们可以使用斜杠（ \）将一行的语句分为多行显示，如下所示： 123total = item_one + \ item_two + \ item_three 后续的学习中，但凡是带括号的，[],{},(),直接换行也是可以的 Python引号Python 可以使用引号( ‘ )、双引号( “ )、三引号( ‘’’ 或 “”” ) 来表示字符串，引号的开始与结束必须的相同类型的。 其中三引号可以由多行组成，编写多行文本的快捷语法，常用于文档字符串，在文件的特定地点，被当做注释。 如果你的输出语句中带有多种引号，就挨个找成对的吧，对对碰一样，如果有单身的，那就有问题，或者他们就近组队了，后面乱了套，也会出问题 Python注释python中单行注释采用 # 开头。 12345678910111213# 第一个注释print("Hello, Python!") # 第二个注释'''第三个多行注释'''"""第四个多行注释""" 多个语句构成代码组123456if expression : suite elif expression : suite else : suite 命令行参数很多程序可以执行一些操作来查看一些基本信息，Python 可以使用 -h 参数查看各参数帮助信息： 123456789$ python -h usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ... Options and arguments (and corresponding environment variables): -c cmd : program passed in as string (terminates option list) -d : debug output from parser (also PYTHONDEBUG=x) -E : ignore environment variables (such as PYTHONPATH) -h : print this help message and exit [ etc. ] — 欲知后事是如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[4.Python 的变量类型]]></title>
    <url>%2Fpython%2F4.Python%20%E7%9A%84%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B.html</url>
    <content type="text"><![CDATA[书接上文了解了基本的编程规范，让我们来看看 Python 中的变量类型。变量变量也就是可以变化的值，可以把它比作是一个篮子，里面可以装苹果也可以装梨子。通常来讲，你要清楚你的目的是干什么，买苹果就拿符合苹果大小的篮子，如果你拿着只能装苹果的篮子去买西瓜，一定会跪在摊前认错。反之，拿着本来装西瓜的篮子去买一颗车厘子，实在是浪费了点。这也是为什么几乎所有的程序语言都要求你提前告知你要用什么篮子。 变量赋值Python 中的变量赋值不需要类型声明。 每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。 每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 等号（=）用来给变量赋值。 等号（=）运算符左边是一个变量名,等号（=）运算符右边是存储在变量中的值。例如： 123456#!/usr/bin/python# -*- coding: UTF-8 -*- counter = 100 # 赋值整型变量miles = 1000.0 # 浮点型name = "John" # 字符串 多个变量赋值1a = b = c = 1 # 这个操作在你需要创建几个空列表或者计数器时候非常好用 1a, b, c = 1, 2, "john" 上面这个例子其实是属于元组的解包操作，省略了圆括号，通常你如果看到类似这样的语句可以认为它是下面这样的 1a, b, c = (1, 2, "john") # 元组我们后面再讲 标准数据类型Python有五个标准的数据类型： Numbers（数字） String（字符串） List（列表） Tuple（元组） Dictionary（字典） 这5种基本的数据类型贯穿了你的整个Python编程生涯，所以务必要掌握它，基础操作还是要扎实才行 Python支持三种不同的数字类型： int（有符号整型） float（浮点型） complex（复数） 常用的 int 和 float ，也就是常说的整数和小数，现在的小朋友学习的时候教材已经将小数改为了浮点数 Python字符串字符串或串(String)是由数字、字母、下划线组成的一串字符。例如： 12345678910111213141516171819s = 'a_1_c''''如果你只想要字符串中的某一个，就要使用索引，索引就像班里的学号一样，只不过这里从0开始数所以：a_1_c正向索引a --- 0_ --- 11 --- 2_ --- 3c --- 4反向索引a --- -5_ --- -41 --- -3_ --- -2c --- -1''' 如果你想要某一部分，可以使用切片操作： 1s[0:2] # 'a_' 后面会详细介绍，这里只需要记住，取多少个字符，用切片尾数减去首位数就可以了，切片操作是取不到2的，这个区间在数学上算是左开右闭，[…) Python列表列表可以完成大多数集合类的数据结构实现。它支持字符，数字，字符串甚至可以包含列表（即嵌套）。 列表用 [ ] 标识，是 python 最通用的复合数据类型。 列表中值的切割也可以用到变量 [头下标:尾下标] ，就可以截取相应的列表，从左到右索引默认 0 开始，从右到左索引默认 -1 开始，下标可以为空表示取到头或尾。 Python 元组元组是另一个数据类型，类似于 List（列表）。 元组用 () 标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表。 元组的精髓现在还无法体会，初学觉得它很废，它能做到的列表都能，直到天荒地老才知道元组最可贵 Python 字典字典(dictionary)是除列表以外python之中最灵活的内置数据结构类型。列表是有序的对象集合，字典是无序的对象集合。 两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。 字典用”{ }”标识。字典由索引(key)和它对应的值value组成。 1234567891011121314#!/usr/bin/python# -*- coding: UTF-8 -*- dict = &#123;&#125;dict['one'] = "This is one"dict[2] = "This is two" tinydict = &#123;'name': 'john','code':6734, 'dept': 'sales'&#125; print dict['one'] # 输出键为'one' 的值print dict[2] # 输出键为 2 的值print tinydict # 输出完整的字典print tinydict.keys() # 输出所有键print tinydict.values() # 输出所有值 字典提前说一下，因为它是无序的，当然不能够以之前的学号索引来取值，键你可以认为是你的名字，值就是你这个真真正正的人，别以为没有学号就找不到你了，凭你的名字也是可以找到你的，说到这里，你能感受到字典和列表的各种优缺点吗？比方说，一个考场，大家都按照顺序坐，阅卷的时候按照顺序打分真的方便，但是要是中间缺个两个，就麻烦了。字典好比老师手里攥着带你名字无形的线，随时可以顺着线找到你，海贼王有一个七武海就是这样 Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。 [int(x ,base]) 将x转换为一个整数 long(x [,base] ) 将x转换为一个长整数 float(x) 将x转换到一个浮点数 complex(real [,imag]) 创建一个复数 str(x) 将对象 x 转换为字符串 repr(x) 将对象 x 转换为表达式字符串 eval(str) 用来计算在字符串中的有效Python表达式,并返回一个对象 tuple(s) 将序列 s 转换为一个元组 list(s) 将序列 s 转换为一个列表 set(s) 转换为可变集合 dict(d) 创建一个字典。d 必须是一个序列 (key,value)元组。 frozenset(s) 转换为不可变集合 chr(x) 将一个整数转换为一个字符 unichr(x) 将一个整数转换为Unicode字符 ord(x) 将一个字符转换为它的整数值 hex(x) 将一个整数转换为一个十六进制字符串 oct(x) 将一个整数转换为一个八进制字符串 这个是真的挺方便的，不像其他语言，比如：str2int 这种骚操作，又难记又不好看，以后看到2要当成to ，O2O，P2P知道不，谐音命名法也是很骚的操作，以后可以用 guluguluwater 命名温泉 到这里为止都是简单介绍，从下一节开始可要动真格的了。相比于其他教程，接下来的内容重在深挖本质，这也是我为什么写这篇教程的原因，当初随便学学，基础不扎实，导致现在还要回炉。都是泪。 — 欲知后事是如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2.安装 Anaconda]]></title>
    <url>%2Fpython%2F2.%E5%AE%89%E8%A3%85%20Anaconda.html</url>
    <content type="text"><![CDATA[书接上文上回书说到，Python的应用领域广泛，大家伙也有兴趣和我一起从头开始学习 Python ，那么怎么编写第一个 Python程序呢？不用说，我们需要一个可以编写 Python 代码的程序来帮助我们更快地入门。我推荐 Anaconda 这个集成环境来学习，就像你安装游戏一样双击就能安装，并且还会为我们下载很多常用的第三方库，这样我们就不用自己下载安装了。 PS:不要以为安装这个事情很简单，迟早会有自己安装第三方库因为版本问题捶胸顿足的时候 下载链接：https://www.anaconda.com/distribution/#download-section 选择合适自己的下载安装吧 在“开始”菜单中“Anaconda3”文件下找到“Jupyter Notebook”，点击进入: 选择 Python3，新建一个 Python 代码笔记本 修改文件名称，点击 untitled2 进行修改 点击进入代码编辑块，编辑：print(‘Hello World’)，Ctrl + Enter 进行运行 如果能够正常输出，那么说明安装成功了，并且你已经学会了 Python 的第一个程序，这将是你迈入编程世界最重要的一步 程序界有一个诅咒：如果你学习一门新的编程语言，不打印 Hello World ，那么你将学不会这门语言。 作业： 使用Jupyter Notebook打印： 世界，您好！ 您好，世界！ 打印佛祖： 123456789101112131415161718192021222324252627282930 1 * 2 * _ooOoo_ 3 * o8888888o 4 * 88&quot; . &quot;88 5 * (| -_- |) 6 * O\ = /O 7 * ____/`---&apos;\____ 8 * .&apos; \\| |// `. 9 * / \\||| : |||// \10 * / _||||| -:- |||||- \11 * | | \\\ - /// | |12 * | \_| &apos;&apos;\---/&apos;&apos; | |13 * \ .-\__ `-` ___/-. /14 * ___`. .&apos; /--.--\ `. . __15 * .&quot;&quot; &apos;&lt; `.___\_&lt;|&gt;_/___.&apos; &gt;&apos;&quot;&quot;.16 * | | : `- \`.;`\ _ /`;.`/ - ` : | |17 * \ \ `-. \_ __\ /__ _/ .-` / /18 * ======`-.____`-.___\_____/___.-`____.-&apos;======19 * `=---=&apos;20 * ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^21 * 佛祖保佑 永无BUG22 * 佛曰:23 * 写字楼里写字间，写字间里程序员；24 * 程序人员写程序，又拿程序换酒钱。25 * 酒醒只在网上坐，酒醉还来网下眠；26 * 酒醉酒醒日复日，网上网下年复年。27 * 但愿老死电脑间，不愿鞠躬老板前；28 * 奔驰宝马贵者趣，公交自行程序员。29 * 别人笑我忒疯癫，我笑自己命太贱；30 * 不见满街漂亮妹，哪个归得程序员？ 当我们虔诚地问候了这个世界之后，就可以开始我们的编程之旅了，佛祖保佑，永无BUG — 欲知后事是如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[1.Python 介绍]]></title>
    <url>%2Fpython%2F1.Python%20%E4%BB%8B%E7%BB%8D.html</url>
    <content type="text"><![CDATA[Python 来了Python（英国发音：/ˈpaɪθən/ 美国发音：/ˈpaɪθɑːn/）是一种广泛使用的解释型、高级编程、通用型编程语言，由吉多·范罗苏姆创造，第一版发布于1991年。背景Python的创始人为吉多·范罗苏姆。1989年的圣诞节期间，吉多·范罗苏姆为了在阿姆斯特丹打发时间，决心开发一个新的脚本解释程序，作为ABC语言的一种继承。之所以选中Python作为程序的名字，是因为他是BBC电视剧——蒙提·派森的飞行马戏团的爱好者。 特性与设计哲学Python是完全面向对象的语言。函数、模块、数字、字符串都是对象。并且完全支持继承、重载、派生、多重继承，有益于增强源代码的复用性。 标准库为了代码的重用性，Python 在设计之初就包含了许多好用的库函数，避免重复造轮子。所以对于标准库的掌握是Python开发者必备技能。 我们可以看看标准库包含哪些功能呢？ Python标准库的主要功能有： 文本处理，包含文本格式化、正则表达式匹配、文本差异计算与合并、Unicode支持，二进制数据处理等功能 文件处理，包含文件操作、创建临时文件、文件压缩与归档、操作配置文件等功能 操作系统功能，包含线程与进程支持、IO复用、日期与时间处理、调用系统函数、日志（logging）等功能 网络通信，包含网络套接字，SSL加密通信、异步网络通信等功能 网络协议，支持HTTP，FTP，SMTP，POP，IMAP，NNTP，XMLRPC等多种网络协议，并提供了编写网络服务器的框架 W3C格式支持，包含HTML，SGML，XML的处理。 其它功能，包括国际化支持、数学运算、HASH、Tkinter等 想要详细了解 Python 的标准库？ 请访问官方标准库文档：https://docs.python.org/3/library/index.html 如果你是初学者，上面的库功能可能难以理解，是的，标准库就是安装 Python 自带的内置电池，也算是比较底层的东西。接下来，让我们不妨再看看 Python 的第三方库吧： 著名的第三方库：Web框架[编辑] Django 开源Web开发框架，它鼓励快速开发,并遵循MVC设计，开发周期短。 Flask 轻量级的Web框架。 Pyramid 轻量，同时有可以规模化的Web框架，Pylon projects 的一部分。 ActiveGrid 企业级的Web2.0解决方案。 Karrigell 简单的Web框架，自身包含了Web服务，py脚本引擎和纯python的数据库PyDBLite。 Tornado 一个轻量级的Web框架，内置非阻塞式服务器，而且速度相当快 webpy 一个小巧灵活的Web框架，虽然简单但是功能强大。 CherryPy 基于Python的Web应用程序开发框架。 Pylons 基于Python的一个极其高效和可靠的Web开发框架。 Zope 开源的Web应用服务器。 TurboGears 基于Python的MVC风格的Web应用程序框架。 Twisted 流行的网络编程库，大型Web框架。 Quixote Web开发框架。 aiohttp 轻量级的Web框架，采用的是Python3的asyncio异步特性。 科学计算[编辑] Matplotlib 用Python实现的类matlab的第三方库，用以绘制一些高质量的数学二维图形。 Pandas 用于数据分析、数据建模、数据可视化的第三方库。 SciPy 基于Python的matlab实现，旨在实现matlab的所有功能。 NumPy 基于Python的科学计算第三方库，提供了矩阵，线性代数，傅立叶变换等等的解决方案。 GUI[编辑] PyGtk 基于Python的GUI程序开发GTK+库。 PyQt 用于Python的QT开发库。 WxPython Python下的GUI编程框架，与MFC的架构相似。 其它[编辑] BeautifulSoup 基于Python的HTML/XML解析器，简单易用。 gevent python的一个高性能并发框架,使用了epoll事件监听、协程等机制将异步调用封装为同步调用。 PIL 基于Python的图像处理库，功能强大，对图形文件的格式支持广泛。当前已无维护，另一个第三方库Pillow实现了对PIL库的支持和维护。 PyGame 基于Python的多媒体开发和游戏软件开发模块。 Py2exe 将python脚本转换为windows上可以独立运行的可执行程序。 Requests 适合于人类使用的HTTP库，封装了许多繁琐的HTTP功能，极大地简化了HTTP请求所需要的代码量。 scikit-learn 机器学习第三方库，实现许多知名的机器学习算法。 TensorFlow Google开发维护的开源机器学习库。 Keras 基于TensorFlow，Theano与CNTK的高端神经网络API。 SQLAlchemy 关系型数据库的对象关系映射(ORM)工具 利用上面这些著名的第三方库，可以完成下面的方向。有你感兴趣的领域吗？ 1. WEB开发 2. 网络编程 3. 爬虫开发 4. 云计算开发 5. 人工智能 6. 自动化运维 7. 金融分析 8. 科学运算 9. 游戏开发 10. 桌面软件 如果你对下面的某一个或几个方向感兴趣，不妨试试 Python 吧，我将会和你一起从零开始学习这些，并且在每一个细分领域从实战出发进行学习，每一篇博客布置相应内容的习题帮助巩固，最后完结的时候，让我们共同攻克一个较为完整的项目。 文章参考： 参考文献 细则 维基百科 python python 官网 https://docs.python.org/3/library/index.html Python 作为一门编程语言，应用广泛。曾经有个伟人说过： 吾生也有涯，而知也无涯。以有涯随无涯，殆已！ 就是说这么多东西，等我学完，早挂掉了。莫非是劝我们弃疗？如何用有限的生命解锁 Python 技能呢？ —欲知后事如何，且听下回分解]]></content>
      <categories>
        <category>Python 教程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2.安装 Anaconda]]></title>
    <url>%2Fhomework%2FHello%20World.html</url>
    <content type="text"><![CDATA[12print('世界，您好！')print('您好，世界！') 世界，您好！ 您好，世界！12345678910111213141516171819202122232425262728 _ooOoo_ o8888888o 88" . "88 (| -_- |) O\ = /O ____/`---'\____ .' \\| |// `. / \\||| : |||// \ / _||||| -:- |||||- \ | | \\\ - /// | | | \_| ''\---/'' | | \ .-\__ `-` ___/-. / ___`. .' /--.--\ `. . __ ."" '&lt; `.___\_&lt;|&gt;_/___.' &gt;'"". | | : `- \`.;`\ _ /`;.`/ - ` : | | \ \ `-. \_ __\ /__ _/ .-` / /======`-.____`-.___\_____/___.-`____.-'====== `=---='^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 佛祖保佑 永无BUG 佛曰: 写字楼里写字间，写字间里程序员； 程序人员写程序，又拿程序换酒钱。 酒醒只在网上坐，酒醉还来网下眠； 酒醉酒醒日复日，网上网下年复年。 但愿老死电脑间，不愿鞠躬老板前； 奔驰宝马贵者趣，公交自行程序员。 别人笑我忒疯癫，我笑自己命太贱； 不见满街漂亮妹，哪个归得程序员？]]></content>
      <categories>
        <category>作业</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world.html</url>
    <content type="text"><![CDATA[第一篇博客就像程序员每学习一门新的编程语言的时候，首先输出：Hello World 一样，这篇博客也是我创建自己博客所迈出的第一步。为什么想起写博客在学习工作的过程中，从大家的博客中获益匪浅，也希望自己能够将学到的知识记录下来，分享出去。既可以给自己留下一点复习参考的笔记，又可以服务大家，能够有意或者无意帮助到别人，也算是赠人玫瑰，手留余香的快乐吧。 主要记录些什么作为自己的个人博客，我希望可以记录得随意一些。学习到的新知识，搜索记录的笔记，个人的人生感悟，生活中的一些琐碎，还有杂七杂八的兴趣吧 自己的一方天地生活中零零碎碎的事情，千丝万缕的关系，很难衷心地说出自己的想法，自己的思考。比起朋友圈，这里有更加独立的空间。虽然是公开博客，躲在这互联网的汪洋大海之中，谁也找不到，反而更能够沉下心来记录，感悟。]]></content>
  </entry>
</search>
